# NER (named entity recognition)

POS tagging: recognize lexical unit in a text
NER: info extraction task: identify entity in a text such as company, person and so on  
How do we perform NER or POS?  

- rule base approches
- HMM

Context is essential: resolve ambiguity
Accurate recognition needs 1M words of training data
90% on simple classification, but get worse when dealing for example with product number

# NLP

We start with an example  
How can a pc make sense out of an arab string?  
A pc must know:

- morphology: token, lexicon, stemma and so on
- sintax: how words are related to each others (POS tagging and NER)
- semantic: combined meaning of words (word embeddings)
- pragmatics: what is the meta meaning? meaning has different meaning for anyone, cause anyone has its own background and context
- discourse: handling large chink of text
- inference: making sense of everything (knowledge graphs, ontology)

Ambiguity is one of the main problem of nlp

- word-level ambiguity
- syntactic ambiguity
- anaphora resolution: two persons in a phrase and then refer to one of them with a pronoun
- presupposition

Approches to NLP are mainly statistical

# Language models

Topic models: argomenti trattati nei testi
I topic model generano una distribuzione di probabilità sulla distribuzione di parole
I language models sono distribuzioni di probabilità su parole o gruppi di parole
Probabilistic language model is a probability distribution over word sequences, based on the syntactic level  
given the prob distr we can build a generative model  
Why?

- machine translation
- spelling correction
- speech recognition
- summarization
- question answering

we have some finite vocabulary
we have an infinite set of strings build upon the vocabulary
we a training sample
we want to learn a probability distribution

Goal: compute prob of a sentence or of a sequence of words (joint prob)
Related task: probability of upcoming word (conditional prob)
A model that computes either a joint or a conditional prob is called a language model

We can simplify a general language model with a model that uses only group of tokens or n-grams
conditional probabilities, since they involve product of numbers between 0 and 1, can tend to 0
we can use logarithm and make it as summation
we can simplify the omputation with the k-markov assumption: we consider the previous k words
unigram-model: k = 1 (Naive bayes)
bigram-model: k = 2 (First order markov process)

Suppose some event is not in our example -> 0 probability associated
Smoothing: laplacian smoothing and son on



