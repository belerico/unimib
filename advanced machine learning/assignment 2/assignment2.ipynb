{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/belerico/unimib/blob/master/advanced%20machine%20learning/assignment2%20/assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QBIOBRVd70i",
        "colab_type": "text"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu01qI83ribP",
        "colab_type": "code",
        "outputId": "590c1e28-5bfa-488a-b44f-d6305b39ceb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!pip install wget\n",
        "\n",
        "import wget\n",
        "\n",
        "wget.download('https://github.com/belerico/unimib/raw/master/advanced machine learning/assignment 2/data/x_test.obj', './x_test.obj')\n",
        "wget.download('https://github.com/belerico/unimib/raw/master/advanced machine learning/assignment 2/data/y_train.obj', './y_train.obj')\n",
        "wget.download('https://github.com/belerico/unimib/raw/master/advanced machine learning/assignment 2/data/x_train.obj', './x_train.obj')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=f37c89a9505dc1c5e53ee54431836d5e1b0e12e96937ad77323ebea2666f867e\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./x_train.obj'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhAQNTMEvmbM",
        "colab_type": "code",
        "outputId": "8071e7fd-8d2c-44e1-d0e6-c2f34821af93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import utils\n",
        "\n",
        "with open('./x_train.obj', 'rb') as f:\n",
        "  dataset = pickle.load(f)\n",
        "  dataset = dataset.astype('float32') / 255.\n",
        "  dataset = dataset.reshape(dataset.shape[0], -1)\n",
        "\n",
        "with open('./x_test.obj', 'rb') as f:\n",
        "  x_test = pickle.load(f)\n",
        "  x_test = x_test.astype('float32') / 255.\n",
        "  x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "with open('./y_train.obj', 'rb') as f:\n",
        "  y_dataset = pickle.load(f)\n",
        "  # Label from 0 to 11\n",
        "  y_dataset -= 16\n",
        "  # y_dataset = utils.to_categorical(y_dataset)\n",
        "\n",
        "N_IMG = dataset.shape[0]\n",
        "N_CLASSES = 11\n",
        "IMG_SHAPE = dataset.shape[1]\n",
        "\n",
        "# Split train and val\n",
        "import random\n",
        "\n",
        "indexes = np.arange(N_IMG)\n",
        "# random.seed(42)\n",
        "random.shuffle(indexes)\n",
        "train_val_split = int(N_IMG * 0.8)\n",
        "\n",
        "x_train = dataset[indexes[:train_val_split], :]\n",
        "y_train = y_dataset[indexes[:train_val_split]]\n",
        "\n",
        "x_val = dataset[indexes[train_val_split:], :]\n",
        "y_val = y_dataset[indexes[train_val_split:]]\n",
        "\n",
        "# Standardize to get gaussian\n",
        "\n",
        "mean = np.mean(x_train)\n",
        "std = np.std(x_train)\n",
        "\n",
        "print(f\"Mean {mean} and std {std}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean 0.1727036088705063 and std 0.3314586281776428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIN7SjkDpr5w",
        "colab_type": "text"
      },
      "source": [
        "# Data info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhyxtlpLpuRU",
        "colab_type": "code",
        "outputId": "c5aef485-2b67-4172-98c8-b18fa58af931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "from collections import Counter\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print('train size:', len(dataset))\n",
        "print('test size:', len(x_test))\n",
        "data_distribution = sorted(Counter(y_dataset).items())\n",
        "print('labels distribution:', data_distribution)\n",
        "x = np.arange(len(data_distribution))\n",
        "plt.bar(x, height=[val for _, val in data_distribution])\n",
        "plt.xticks(x, ['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'])\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size: 14000\n",
            "test size: 8800\n",
            "labels distribution: [(0, 1295), (1, 1265), (2, 1346), (3, 1329), (4, 1336), (5, 1297), (6, 1269), (7, 1327), (8, 1322), (9, 1321), (10, 893)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD6CAYAAABNu5eFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IeGz6EQs1Tm",
        "colab_type": "text"
      },
      "source": [
        "## Custom layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsfOS1ubs0Mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class SaltPepperNoise(layers.Layer):\n",
        "  \n",
        "  def __init__(self, ratio=0.5, **kwargs):\n",
        "    super(SaltPepperNoise, self).__init__(**kwargs)\n",
        "    self.ratio = ratio\n",
        "\n",
        "  # NOTE: this is the definition of the call method of custom layer class (i.e. SaltAndPepper)\n",
        "  def call(self, inputs, training=None):\n",
        "    def noised():\n",
        "      shp = K.shape(inputs)[1:] # input shape\n",
        "      mask_select = K.random_binomial(shape=shp, p=self.ratio)\n",
        "      mask_noise = K.random_binomial(shape=shp, p=0.5) # salt and pepper have the same chance\n",
        "      out = inputs * (1-mask_select) + mask_noise * mask_select\n",
        "      return out\n",
        "    \n",
        "    return K.in_train_phase(noised(), inputs, training=training)\n",
        "  \n",
        "  \n",
        "class StandardizeLayer(layers.Layer):\n",
        "  \n",
        "  def __init__(self, mean, std, **kwargs):\n",
        "    super(StandardizeLayer, self).__init__(**kwargs)\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "     return (inputs - self.mean) / self.std\n",
        "    \n",
        "    \n",
        "class DenseTranspose(layers.Layer):\n",
        "  \n",
        "  def __init__(self, dense, activation=None, **kwargs):\n",
        "    self.dense = dense\n",
        "    self.activation = tf.keras.activations.get(activation)\n",
        "    super().__init__(**kwargs)\n",
        "  \n",
        "  def build(self, batch_input_shape):\n",
        "    print(self.dense.weights[0])\n",
        "    self.non_trainable_weights.append(self.dense.weights[0])\n",
        "    self.biases = self.add_weight(name='bias', initializer='zeros',\n",
        "                                 shape=[self.dense.input_shape[-1]])\n",
        "    super().build(batch_input_shape)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    z = inputs @ K.transpose(self.dense.weights[0]) \n",
        "    return self.activation(z + self.biases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JfAZwCnwl5R",
        "colab_type": "text"
      },
      "source": [
        "# Models and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTeBIMf0zoWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "  return K.sqrt(losses.mse(y_true, y_pred))\n",
        "\n",
        "\n",
        "def tanh_crossentropy(y_true, y_pred):\n",
        "  return -0.5 * ((1 - y_true) * K.log(1 - y_pred) + (1 + y_true) * K.log(1 + y_pred))\n",
        "\n",
        "\n",
        "def get_model(\n",
        "    model_type='nn',\n",
        "    tied=False,\n",
        "    divide_by=2,\n",
        "    hidden_first=256,\n",
        "    hidden_last=64,\n",
        "    dropout=False, \n",
        "    dropout_rate=0.3,\n",
        "    batch_norm=False, \n",
        "    noise_type=None,\n",
        "    salt_pepper_noise_ratio=0.3, \n",
        "    gaussian_noise_std=0.5,\n",
        "    standard=False,\n",
        "    decoder_loss='mse',\n",
        "    verbose=True\n",
        "):\n",
        "  \n",
        "  input_layer = layers.Input(shape=(IMG_SHAPE, ))\n",
        "  if model_type == 'autoencoder' or model_type == 'autoencoder_nn':\n",
        "    if noise_type is not None:\n",
        "      # noisy layer\n",
        "      if noise_type == 'gaussian':  # Gaussian\n",
        "        noise_layer = layers.GaussianNoise(gaussian_noise_std)(input_layer)\n",
        "      elif noise_type == 'saltpepper':  # Salt and pepper\n",
        "        noise_layer = SaltPepperNoise(ratio=salt_pepper_noise_ratio)(input_layer)\n",
        "      if standard:\n",
        "        noise_layer = StandardizeLayer(mean, std)(noise_layer)\n",
        "      if isinstance(divide_by, list) and divide_by != []:\n",
        "        dim = int(IMG_SHAPE / divide_by[0])\n",
        "        dims = [dim]\n",
        "        encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(noise_layer)\n",
        "      else:\n",
        "        encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(noise_layer)\n",
        "    elif standard:\n",
        "      encode = StandardizeLayer(mean, std)(input_layer)\n",
        "      if dropout:\n",
        "          encode = layers.Dropout(dropout_rate)(encode) \n",
        "    else:\n",
        "      if dropout:\n",
        "        encode = layers.Dropout(dropout_rate)(input_layer)\n",
        "        if isinstance(divide_by, list) and divide_by != []:\n",
        "          dim = int(IMG_SHAPE / divide_by[0])\n",
        "          dims = [dim]\n",
        "          encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "        else:\n",
        "          encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "      else:\n",
        "        if isinstance(divide_by, list) and divide_by != []:\n",
        "          dim = int(IMG_SHAPE / divide_by[0])\n",
        "          dims = [dim]\n",
        "          encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(input_layer)\n",
        "        else:\n",
        "          encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(input_layer)\n",
        "\n",
        "    # encoder\n",
        "    if isinstance(divide_by, list) and divide_by != []:\n",
        "      # saved_hidden_first = hidden_first\n",
        "      # if noise_type or standard:\n",
        "      #  encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "      i = 1\n",
        "      while i < len(divide_by) and int(dim / divide_by[i]) > hidden_last:\n",
        "        dim = int(dim / divide_by[i])\n",
        "        dims.insert(0, dim)\n",
        "        encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "        if batch_norm:\n",
        "          encode = layers.BatchNormalization()(encode)\n",
        "        i += 1\n",
        "    else:\n",
        "      saved_hidden_first = hidden_first\n",
        "      # if noise_type or standard:\n",
        "      #  encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "      while hidden_first > hidden_last:\n",
        "        hidden_first = int(hidden_first / divide_by)\n",
        "        encode = layers.Dense(hidden_first, activation=\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "        if batch_norm:\n",
        "          encode = layers.BatchNormalization()(encode)\n",
        "    # encode = layers.Dense(hidden_last, activation =\"relu\", kernel_initializer=\"he_normal\", name='encoder')(encode)\n",
        "    encoder = Model(inputs=input_layer, outputs=encode)   \n",
        "    \n",
        "    # decoder\n",
        "    if tied:\n",
        "      i = len(encoder.layers) - 2\n",
        "      end = 1 if not dropout else 2\n",
        "      decode = DenseTranspose(encoder.layers[-1], activation='relu')(encode) \n",
        "      while i > end:\n",
        "        print(encoder.layers[i].name)\n",
        "        if 'dense' in encoder.layers[i].name:\n",
        "          decode = DenseTranspose(encoder.layers[i], activation='relu')(decode) \n",
        "        i -= 1\n",
        "      decode = DenseTranspose(encoder.layers[i], activation='sigmoid', name='decoder')(decode)\n",
        "    else:\n",
        "      if isinstance(divide_by, list) and divide_by != []:\n",
        "        decode = layers.Dense(dims[1], activation='relu', kernel_initializer=\"he_normal\")(encode) \n",
        "        for dim in dims[2:]:\n",
        "          decode = layers.Dense(dim, activation='relu', kernel_initializer=\"he_normal\")(decode) \n",
        "        decode = layers.Dense(IMG_SHAPE, activation='sigmoid', name='decoder')(decode)\n",
        "      else:\n",
        "        hidden_last *= divide_by\n",
        "        decode = layers.Dense(hidden_last, activation='relu', kernel_initializer=\"he_normal\")(encode) \n",
        "        while saved_hidden_first > hidden_last:\n",
        "          hidden_last *= divide_by\n",
        "          decode = layers.Dense(hidden_last, activation='relu', kernel_initializer=\"he_normal\")(decode) \n",
        "        decode = layers.Dense(IMG_SHAPE, activation='sigmoid', name='decoder')(decode)\n",
        "    \n",
        "    outputs = [decode]\n",
        "    if model_type == 'autoencoder_nn':\n",
        "      # classifier\n",
        "      classifier = layers.Dense(N_CLASSES, activation=\"softmax\", name='classifier')(encode)\n",
        "      outputs.insert(0, classifier)\n",
        "    \n",
        "    # model\n",
        "    model = Model(inputs=input_layer, outputs=outputs)\n",
        "    \n",
        "    if len(outputs) > 1:\n",
        "      loss = [\"sparse_categorical_crossentropy\", decoder_loss]\n",
        "      metrics = {'classifier': 'accuracy', 'decoder': 'mse'}\n",
        "    else:\n",
        "      loss = decoder_loss\n",
        "      metrics = {'decoder': 'mse'}\n",
        "      \n",
        "  elif model_type == 'nn':\n",
        "    if isinstance(divide_by, list) and divide_by != []:\n",
        "      dim = int(IMG_SHAPE / divide_by[0])\n",
        "      encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(input_layer)\n",
        "    else:\n",
        "      encode = layers.Dense(hidden_first, activation=\"relu\", kernel_initializer=\"he_normal\")(input_layer)\n",
        "    if dropout:\n",
        "      encode = layers.Dropout(dropout_rate)(encode)\n",
        "    if isinstance(divide_by, list) and divide_by != []:\n",
        "      i = 1\n",
        "      while i < len(divide_by) and int(dim / divide_by[i]) > hidden_last:\n",
        "        dim = int(dim / divide_by[i])\n",
        "        if batch_norm:\n",
        "          encode = layers.BatchNormalization()(encode)\n",
        "        encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "        if dropout:\n",
        "          encode = layers.Dropout(dropout_rate)(encode) \n",
        "        i += 1\n",
        "    else:\n",
        "      while hidden_first > hidden_last * divide_by:\n",
        "        hidden_first = int(hidden_first / divide_by)\n",
        "        if batch_norm:\n",
        "          encode = layers.BatchNormalization()(encode)\n",
        "        encode = layers.Dense(hidden_first, activation=\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "        if dropout:\n",
        "          encode = layers.Dropout(dropout_rate)(encode) \n",
        "    \n",
        "    classifier = layers.Dense(N_CLASSES, activation=\"softmax\", name='classifier')(encode)\n",
        "    # nn model\n",
        "    model = Model(inputs=input_layer, outputs=classifier)\n",
        "    encoder = None\n",
        "    loss = \"sparse_categorical_crossentropy\"\n",
        "    metrics = {'classifier': 'accuracy'}\n",
        "  \n",
        "  model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
        "  if verbose:\n",
        "    model.summary()  \n",
        "  return model, encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SI6WnDDx_xS",
        "colab_type": "text"
      },
      "source": [
        "# Plain neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td3w71yf9f1D",
        "colab_type": "code",
        "outputId": "cfae2d09-b0f9-4f2f-818f-b1cf8a094bb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "monitor = 'val_loss'\n",
        "early_stopping = EarlyStopping(monitor=monitor, patience=10, min_delta=0.001, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=10, verbose=1)\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "model_type = 'nn'\n",
        "if model_type == 'autoencoder_nn':\n",
        "  labels_train = [y_train, x_train]\n",
        "  labels_val = [y_val, x_val]\n",
        "elif model_type == 'nn':\n",
        "  labels_train = y_train\n",
        "  labels_val = y_val\n",
        "elif model_type == 'autoencoder':\n",
        "  labels_train = x_train\n",
        "  labels_val = x_val\n",
        "\n",
        "# define 10-fold cross validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "scores_val = []\n",
        "best_score = 0  # based on accuracy\n",
        "best_model = None\n",
        "best_history = None\n",
        "\n",
        "for idx, (train, val) in enumerate(kfold.split(x_train, labels_train)):\n",
        "  print('FOLD: ', idx + 1)\n",
        "  # create model\n",
        "  model, _ = get_model(\n",
        "      model_type=model_type, \n",
        "      divide_by=[1.5, 2, 2.5, 3], \n",
        "      hidden_first=512, \n",
        "      hidden_last=32, \n",
        "      dropout=True, \n",
        "      batch_norm=False, \n",
        "      standard=False, \n",
        "      verbose=True\n",
        "  )\n",
        "  # Fit the model\n",
        "  history = model.fit(\n",
        "      x_train[train], \n",
        "      labels_train[train], \n",
        "      validation_data=(x_train[val], labels_train[val]), \n",
        "      epochs=300, \n",
        "      batch_size=128, \n",
        "      callbacks=callbacks\n",
        "  )\n",
        "  # evaluate the model\n",
        "  score = model.evaluate(x_val, labels_val, verbose=0)\n",
        "  # save best_model\n",
        "  if score[1] > best_score:  # accuracy\n",
        "    best_score = score[1]\n",
        "    best_model = model\n",
        "    best_history = history\n",
        "  print('Performance on the validation set')\n",
        "  for idx in range(len(model.metrics_names)):\n",
        "    print(\"%s: %.2f\" % (model.metrics_names[idx], score[idx]))\n",
        "  print()\n",
        "  scores_val.append(score)\n",
        "  \n",
        "scores_val = np.array(scores_val)\n",
        "overall_means = np.mean(scores_val, axis=0)\n",
        "overall_stds = np.std(scores_val, axis=0)\n",
        "for idx in range(len(overall_means)):\n",
        "  print('Overall ', model.metrics_names[idx], ': ', '{:.2f}'.format(overall_means[idx]), '+-', '{:.2f}'.format(overall_stds[idx]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD:  1\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 63us/sample - loss: 1.7704 - acc: 0.3866 - val_loss: 0.9021 - val_acc: 0.7402\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 1.0228 - acc: 0.6764 - val_loss: 0.5540 - val_acc: 0.8429\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.7564 - acc: 0.7730 - val_loss: 0.4512 - val_acc: 0.8687\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.5722 - acc: 0.8343 - val_loss: 0.3917 - val_acc: 0.8929\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.4670 - acc: 0.8704 - val_loss: 0.3613 - val_acc: 0.9009\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.3959 - acc: 0.8850 - val_loss: 0.3193 - val_acc: 0.9214\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.3602 - acc: 0.9010 - val_loss: 0.2982 - val_acc: 0.9232\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.3035 - acc: 0.9148 - val_loss: 0.3159 - val_acc: 0.9250\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.2926 - acc: 0.9186 - val_loss: 0.3090 - val_acc: 0.9223\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.2562 - acc: 0.9284 - val_loss: 0.3086 - val_acc: 0.9277\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 38us/sample - loss: 0.2317 - acc: 0.9359 - val_loss: 0.2976 - val_acc: 0.9250\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2216 - acc: 0.9388 - val_loss: 0.3004 - val_acc: 0.9286\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 39us/sample - loss: 0.2013 - acc: 0.9462 - val_loss: 0.3042 - val_acc: 0.9304\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 38us/sample - loss: 0.1861 - acc: 0.9473 - val_loss: 0.2880 - val_acc: 0.9330\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.1918 - acc: 0.9463 - val_loss: 0.2898 - val_acc: 0.9339\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 39us/sample - loss: 0.1582 - acc: 0.9565 - val_loss: 0.3533 - val_acc: 0.9223\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1485 - acc: 0.9601 - val_loss: 0.3230 - val_acc: 0.9339\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.1414 - acc: 0.9584 - val_loss: 0.3168 - val_acc: 0.9357\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 38us/sample - loss: 0.1519 - acc: 0.9573 - val_loss: 0.3360 - val_acc: 0.9321\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 39us/sample - loss: 0.1388 - acc: 0.9604 - val_loss: 0.3334 - val_acc: 0.9312\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 39us/sample - loss: 0.1320 - acc: 0.9642 - val_loss: 0.3576 - val_acc: 0.9312\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1243 - acc: 0.9645 - val_loss: 0.3394 - val_acc: 0.9330\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 39us/sample - loss: 0.1198 - acc: 0.9649 - val_loss: 0.3572 - val_acc: 0.9268\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 49us/sample - loss: 0.1089 - acc: 0.9699 - val_loss: 0.3374 - val_acc: 0.9259\n",
            "Performance on the validation set\n",
            "loss: 0.25\n",
            "acc: 0.93\n",
            "\n",
            "FOLD:  2\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 66us/sample - loss: 1.7623 - acc: 0.3891 - val_loss: 0.9393 - val_acc: 0.7357\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.9906 - acc: 0.6898 - val_loss: 0.5771 - val_acc: 0.8196\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.6792 - acc: 0.7965 - val_loss: 0.4553 - val_acc: 0.8580\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 39us/sample - loss: 0.5471 - acc: 0.8419 - val_loss: 0.4311 - val_acc: 0.8687\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.4545 - acc: 0.8705 - val_loss: 0.3502 - val_acc: 0.9036\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.3834 - acc: 0.8915 - val_loss: 0.3379 - val_acc: 0.8991\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.3422 - acc: 0.9062 - val_loss: 0.3353 - val_acc: 0.8929\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.3119 - acc: 0.9127 - val_loss: 0.3461 - val_acc: 0.9062\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 39us/sample - loss: 0.2816 - acc: 0.9227 - val_loss: 0.3075 - val_acc: 0.9125\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.2647 - acc: 0.9282 - val_loss: 0.2800 - val_acc: 0.9214\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2432 - acc: 0.9329 - val_loss: 0.3094 - val_acc: 0.9241\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2155 - acc: 0.9385 - val_loss: 0.2873 - val_acc: 0.9295\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1890 - acc: 0.9460 - val_loss: 0.2902 - val_acc: 0.9304\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1921 - acc: 0.9481 - val_loss: 0.2982 - val_acc: 0.9304\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.1738 - acc: 0.9539 - val_loss: 0.3096 - val_acc: 0.9339\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.1670 - acc: 0.9537 - val_loss: 0.2983 - val_acc: 0.9357\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1574 - acc: 0.9547 - val_loss: 0.2821 - val_acc: 0.9366\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1466 - acc: 0.9603 - val_loss: 0.2918 - val_acc: 0.9259\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1414 - acc: 0.9594 - val_loss: 0.2924 - val_acc: 0.9411\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 53us/sample - loss: 0.1403 - acc: 0.9618 - val_loss: 0.3099 - val_acc: 0.9321\n",
            "Performance on the validation set\n",
            "loss: 0.24\n",
            "acc: 0.93\n",
            "\n",
            "FOLD:  3\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 1.6739 - acc: 0.4329 - val_loss: 0.8417 - val_acc: 0.7473\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.9633 - acc: 0.7010 - val_loss: 0.5740 - val_acc: 0.8134\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.6778 - acc: 0.7993 - val_loss: 0.4274 - val_acc: 0.8732\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.5340 - acc: 0.8474 - val_loss: 0.3746 - val_acc: 0.8911\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.4413 - acc: 0.8787 - val_loss: 0.3677 - val_acc: 0.8973\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 44us/sample - loss: 0.3930 - acc: 0.8896 - val_loss: 0.3103 - val_acc: 0.9152\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.3308 - acc: 0.9060 - val_loss: 0.2869 - val_acc: 0.9205\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.2937 - acc: 0.9148 - val_loss: 0.3129 - val_acc: 0.9116\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 44us/sample - loss: 0.2724 - acc: 0.9237 - val_loss: 0.2664 - val_acc: 0.9304\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.2578 - acc: 0.9287 - val_loss: 0.2537 - val_acc: 0.9268\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2247 - acc: 0.9392 - val_loss: 0.2868 - val_acc: 0.9241\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 39us/sample - loss: 0.2230 - acc: 0.9367 - val_loss: 0.2839 - val_acc: 0.9304\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1924 - acc: 0.9466 - val_loss: 0.3084 - val_acc: 0.9241\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1827 - acc: 0.9514 - val_loss: 0.2922 - val_acc: 0.9223\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1795 - acc: 0.9508 - val_loss: 0.2816 - val_acc: 0.9357\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1562 - acc: 0.9559 - val_loss: 0.3582 - val_acc: 0.9241\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 39us/sample - loss: 0.1585 - acc: 0.9563 - val_loss: 0.3030 - val_acc: 0.9312\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1457 - acc: 0.9574 - val_loss: 0.3499 - val_acc: 0.9268\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1459 - acc: 0.9597 - val_loss: 0.3335 - val_acc: 0.9312\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 55us/sample - loss: 0.1378 - acc: 0.9620 - val_loss: 0.2959 - val_acc: 0.9321\n",
            "Performance on the validation set\n",
            "loss: 0.24\n",
            "acc: 0.93\n",
            "\n",
            "FOLD:  4\n",
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 1.7673 - acc: 0.3885 - val_loss: 0.8418 - val_acc: 0.7589\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 1.0001 - acc: 0.6788 - val_loss: 0.5539 - val_acc: 0.8321\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.7101 - acc: 0.7815 - val_loss: 0.4195 - val_acc: 0.8705\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.5543 - acc: 0.8347 - val_loss: 0.3573 - val_acc: 0.8920\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.4577 - acc: 0.8679 - val_loss: 0.3211 - val_acc: 0.9080\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.3870 - acc: 0.8911 - val_loss: 0.2969 - val_acc: 0.9152\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.3434 - acc: 0.9069 - val_loss: 0.3004 - val_acc: 0.9152\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.3081 - acc: 0.9122 - val_loss: 0.2688 - val_acc: 0.9214\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.2764 - acc: 0.9224 - val_loss: 0.2710 - val_acc: 0.9250\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.2560 - acc: 0.9272 - val_loss: 0.2828 - val_acc: 0.9268\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2234 - acc: 0.9402 - val_loss: 0.2985 - val_acc: 0.9259\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.1989 - acc: 0.9455 - val_loss: 0.2866 - val_acc: 0.9321\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.1981 - acc: 0.9450 - val_loss: 0.3089 - val_acc: 0.9214\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.1852 - acc: 0.9500 - val_loss: 0.2751 - val_acc: 0.9321\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1667 - acc: 0.9528 - val_loss: 0.2770 - val_acc: 0.9304\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.1698 - acc: 0.9530 - val_loss: 0.2908 - val_acc: 0.9259\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.1554 - acc: 0.9577 - val_loss: 0.2965 - val_acc: 0.9384\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 54us/sample - loss: 0.1485 - acc: 0.9578 - val_loss: 0.2989 - val_acc: 0.9304\n",
            "Performance on the validation set\n",
            "loss: 0.25\n",
            "acc: 0.93\n",
            "\n",
            "FOLD:  5\n",
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 1.8159 - acc: 0.3699 - val_loss: 0.9231 - val_acc: 0.7464\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 1.0187 - acc: 0.6750 - val_loss: 0.5700 - val_acc: 0.8286\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.7167 - acc: 0.7822 - val_loss: 0.4290 - val_acc: 0.8687\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.5586 - acc: 0.8363 - val_loss: 0.3876 - val_acc: 0.8857\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.4616 - acc: 0.8707 - val_loss: 0.3423 - val_acc: 0.9089\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.3908 - acc: 0.8872 - val_loss: 0.3256 - val_acc: 0.9054\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.3557 - acc: 0.8988 - val_loss: 0.3027 - val_acc: 0.9152\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.3122 - acc: 0.9163 - val_loss: 0.2818 - val_acc: 0.9339\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2922 - acc: 0.9187 - val_loss: 0.3323 - val_acc: 0.9152\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2649 - acc: 0.9262 - val_loss: 0.3018 - val_acc: 0.9232\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.2189 - acc: 0.9382 - val_loss: 0.3139 - val_acc: 0.9196\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.2189 - acc: 0.9411 - val_loss: 0.2782 - val_acc: 0.9304\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.2042 - acc: 0.9419 - val_loss: 0.3110 - val_acc: 0.9268\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1836 - acc: 0.9496 - val_loss: 0.2774 - val_acc: 0.9321\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1680 - acc: 0.9510 - val_loss: 0.2944 - val_acc: 0.9420\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1704 - acc: 0.9535 - val_loss: 0.2891 - val_acc: 0.9259\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.1573 - acc: 0.9564 - val_loss: 0.3123 - val_acc: 0.9375\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.1522 - acc: 0.9581 - val_loss: 0.2911 - val_acc: 0.9366\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1448 - acc: 0.9581 - val_loss: 0.2787 - val_acc: 0.9366\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 38us/sample - loss: 0.1514 - acc: 0.9605 - val_loss: 0.3036 - val_acc: 0.9321\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.1418 - acc: 0.9608 - val_loss: 0.2998 - val_acc: 0.9339\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 57us/sample - loss: 0.1265 - acc: 0.9653 - val_loss: 0.2889 - val_acc: 0.9411\n",
            "Performance on the validation set\n",
            "loss: 0.23\n",
            "acc: 0.94\n",
            "\n",
            "FOLD:  6\n",
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 81us/sample - loss: 1.7192 - acc: 0.4083 - val_loss: 0.7807 - val_acc: 0.7723\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.9927 - acc: 0.6822 - val_loss: 0.4988 - val_acc: 0.8527\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.7112 - acc: 0.7863 - val_loss: 0.3774 - val_acc: 0.8875\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.5770 - acc: 0.8321 - val_loss: 0.3332 - val_acc: 0.8955\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.4523 - acc: 0.8704 - val_loss: 0.2956 - val_acc: 0.9054\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.4105 - acc: 0.8865 - val_loss: 0.2616 - val_acc: 0.9196\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.3505 - acc: 0.9014 - val_loss: 0.2304 - val_acc: 0.9214\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 39us/sample - loss: 0.3049 - acc: 0.9131 - val_loss: 0.2610 - val_acc: 0.9250\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2801 - acc: 0.9232 - val_loss: 0.2583 - val_acc: 0.9241\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2540 - acc: 0.9273 - val_loss: 0.2315 - val_acc: 0.9384\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.2205 - acc: 0.9360 - val_loss: 0.2486 - val_acc: 0.9330\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2062 - acc: 0.9422 - val_loss: 0.2556 - val_acc: 0.9286\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1941 - acc: 0.9453 - val_loss: 0.2588 - val_acc: 0.9330\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.2057 - acc: 0.9462 - val_loss: 0.2323 - val_acc: 0.9384\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.1738 - acc: 0.9514 - val_loss: 0.2547 - val_acc: 0.9304\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.1623 - acc: 0.9534 - val_loss: 0.2387 - val_acc: 0.9348\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 61us/sample - loss: 0.1506 - acc: 0.9587 - val_loss: 0.2517 - val_acc: 0.9330\n",
            "Performance on the validation set\n",
            "loss: 0.26\n",
            "acc: 0.93\n",
            "\n",
            "FOLD:  7\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 90us/sample - loss: 1.8074 - acc: 0.3782 - val_loss: 0.9198 - val_acc: 0.7384\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 1.0239 - acc: 0.6818 - val_loss: 0.5589 - val_acc: 0.8482\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.6972 - acc: 0.7943 - val_loss: 0.4373 - val_acc: 0.8714\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.5496 - acc: 0.8388 - val_loss: 0.3757 - val_acc: 0.8920\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.4765 - acc: 0.8658 - val_loss: 0.3392 - val_acc: 0.9054\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.3885 - acc: 0.8936 - val_loss: 0.3196 - val_acc: 0.9080\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.3522 - acc: 0.9044 - val_loss: 0.3123 - val_acc: 0.9161\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.3160 - acc: 0.9142 - val_loss: 0.3245 - val_acc: 0.9107\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2777 - acc: 0.9228 - val_loss: 0.2822 - val_acc: 0.9170\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.2508 - acc: 0.9269 - val_loss: 0.3094 - val_acc: 0.9241\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2267 - acc: 0.9399 - val_loss: 0.3072 - val_acc: 0.9187\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2021 - acc: 0.9435 - val_loss: 0.3053 - val_acc: 0.9223\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2084 - acc: 0.9405 - val_loss: 0.2794 - val_acc: 0.9321\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1939 - acc: 0.9470 - val_loss: 0.2754 - val_acc: 0.9366\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.1744 - acc: 0.9501 - val_loss: 0.2933 - val_acc: 0.9330\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1615 - acc: 0.9551 - val_loss: 0.3061 - val_acc: 0.9268\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.1603 - acc: 0.9573 - val_loss: 0.3185 - val_acc: 0.9259\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 44us/sample - loss: 0.1544 - acc: 0.9573 - val_loss: 0.3244 - val_acc: 0.9277\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1351 - acc: 0.9631 - val_loss: 0.3037 - val_acc: 0.9357\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.1371 - acc: 0.9631 - val_loss: 0.3347 - val_acc: 0.9250\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1244 - acc: 0.9648 - val_loss: 0.3136 - val_acc: 0.9330\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1149 - acc: 0.9670 - val_loss: 0.3211 - val_acc: 0.9375\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1279 - acc: 0.9649 - val_loss: 0.3540 - val_acc: 0.9268\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 1s 64us/sample - loss: 0.1105 - acc: 0.9691 - val_loss: 0.3500 - val_acc: 0.9277\n",
            "Performance on the validation set\n",
            "loss: 0.24\n",
            "acc: 0.94\n",
            "\n",
            "FOLD:  8\n",
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 98us/sample - loss: 1.9610 - acc: 0.3173 - val_loss: 0.9242 - val_acc: 0.7366\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 1.0733 - acc: 0.6634 - val_loss: 0.5284 - val_acc: 0.8393\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.7455 - acc: 0.7704 - val_loss: 0.3807 - val_acc: 0.8821\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.5792 - acc: 0.8315 - val_loss: 0.3081 - val_acc: 0.9080\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.4736 - acc: 0.8696 - val_loss: 0.2929 - val_acc: 0.9170\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.4043 - acc: 0.8876 - val_loss: 0.2485 - val_acc: 0.9241\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 44us/sample - loss: 0.3439 - acc: 0.9057 - val_loss: 0.2680 - val_acc: 0.9330\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.3097 - acc: 0.9154 - val_loss: 0.2535 - val_acc: 0.9321\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.2910 - acc: 0.9206 - val_loss: 0.2337 - val_acc: 0.9330\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.2478 - acc: 0.9308 - val_loss: 0.2189 - val_acc: 0.9357\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.2231 - acc: 0.9380 - val_loss: 0.1972 - val_acc: 0.9420\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.2083 - acc: 0.9413 - val_loss: 0.2270 - val_acc: 0.9429\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 44us/sample - loss: 0.2013 - acc: 0.9425 - val_loss: 0.2121 - val_acc: 0.9429\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.1909 - acc: 0.9455 - val_loss: 0.2133 - val_acc: 0.9429\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1719 - acc: 0.9521 - val_loss: 0.2291 - val_acc: 0.9429\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1657 - acc: 0.9567 - val_loss: 0.2193 - val_acc: 0.9393\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.1480 - acc: 0.9592 - val_loss: 0.2174 - val_acc: 0.9455\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1569 - acc: 0.9547 - val_loss: 0.2129 - val_acc: 0.9429\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.1422 - acc: 0.9623 - val_loss: 0.2034 - val_acc: 0.9500\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.1275 - acc: 0.9662 - val_loss: 0.2420 - val_acc: 0.9402\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 67us/sample - loss: 0.1345 - acc: 0.9636 - val_loss: 0.2289 - val_acc: 0.9402\n",
            "Performance on the validation set\n",
            "loss: 0.23\n",
            "acc: 0.94\n",
            "\n",
            "FOLD:  9\n",
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 98us/sample - loss: 1.7468 - acc: 0.3992 - val_loss: 0.9476 - val_acc: 0.7304\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 1.0026 - acc: 0.6817 - val_loss: 0.6251 - val_acc: 0.8045\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.7063 - acc: 0.7922 - val_loss: 0.4688 - val_acc: 0.8491\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.5337 - acc: 0.8447 - val_loss: 0.4033 - val_acc: 0.8723\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.4297 - acc: 0.8790 - val_loss: 0.3827 - val_acc: 0.8938\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.3892 - acc: 0.8899 - val_loss: 0.3712 - val_acc: 0.8875\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.3245 - acc: 0.9066 - val_loss: 0.3410 - val_acc: 0.8929\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.3146 - acc: 0.9142 - val_loss: 0.3232 - val_acc: 0.9045\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.2711 - acc: 0.9233 - val_loss: 0.2991 - val_acc: 0.9089\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.2352 - acc: 0.9341 - val_loss: 0.3376 - val_acc: 0.9071\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.2241 - acc: 0.9405 - val_loss: 0.3210 - val_acc: 0.9062\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.2076 - acc: 0.9433 - val_loss: 0.2936 - val_acc: 0.9152\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1998 - acc: 0.9468 - val_loss: 0.3320 - val_acc: 0.9080\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1911 - acc: 0.9472 - val_loss: 0.3630 - val_acc: 0.9089\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1708 - acc: 0.9543 - val_loss: 0.3134 - val_acc: 0.9143\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1563 - acc: 0.9561 - val_loss: 0.3363 - val_acc: 0.9125\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1610 - acc: 0.9553 - val_loss: 0.3224 - val_acc: 0.9170\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1512 - acc: 0.9574 - val_loss: 0.3381 - val_acc: 0.9232\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1449 - acc: 0.9607 - val_loss: 0.3117 - val_acc: 0.9143\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 44us/sample - loss: 0.1292 - acc: 0.9658 - val_loss: 0.3213 - val_acc: 0.9250\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1164 - acc: 0.9673 - val_loss: 0.3146 - val_acc: 0.9196\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 66us/sample - loss: 0.1316 - acc: 0.9651 - val_loss: 0.3092 - val_acc: 0.9277\n",
            "Performance on the validation set\n",
            "loss: 0.26\n",
            "acc: 0.93\n",
            "\n",
            "FOLD:  10\n",
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 105us/sample - loss: 1.7386 - acc: 0.4062 - val_loss: 0.8683 - val_acc: 0.7625\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 1.0028 - acc: 0.6846 - val_loss: 0.5504 - val_acc: 0.8313\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.7040 - acc: 0.7919 - val_loss: 0.4312 - val_acc: 0.8643\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.5527 - acc: 0.8433 - val_loss: 0.3501 - val_acc: 0.8946\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.4538 - acc: 0.8748 - val_loss: 0.3304 - val_acc: 0.9143\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.3983 - acc: 0.8887 - val_loss: 0.3182 - val_acc: 0.9089\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.3377 - acc: 0.9088 - val_loss: 0.3036 - val_acc: 0.9107\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 44us/sample - loss: 0.3020 - acc: 0.9196 - val_loss: 0.2966 - val_acc: 0.9179\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.2809 - acc: 0.9239 - val_loss: 0.2850 - val_acc: 0.9205\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2479 - acc: 0.9314 - val_loss: 0.2939 - val_acc: 0.9205\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.2405 - acc: 0.9333 - val_loss: 0.2853 - val_acc: 0.9277\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.2263 - acc: 0.9366 - val_loss: 0.2870 - val_acc: 0.9312\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.2025 - acc: 0.9434 - val_loss: 0.2836 - val_acc: 0.9277\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1909 - acc: 0.9482 - val_loss: 0.2804 - val_acc: 0.9277\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 43us/sample - loss: 0.1651 - acc: 0.9533 - val_loss: 0.3048 - val_acc: 0.9161\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 40us/sample - loss: 0.1658 - acc: 0.9569 - val_loss: 0.2708 - val_acc: 0.9259\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 41us/sample - loss: 0.1615 - acc: 0.9573 - val_loss: 0.2950 - val_acc: 0.9259\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1410 - acc: 0.9624 - val_loss: 0.3038 - val_acc: 0.9304\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1414 - acc: 0.9613 - val_loss: 0.2939 - val_acc: 0.9286\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1233 - acc: 0.9649 - val_loss: 0.3104 - val_acc: 0.9277\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1217 - acc: 0.9664 - val_loss: 0.2866 - val_acc: 0.9339\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 44us/sample - loss: 0.1194 - acc: 0.9672 - val_loss: 0.3287 - val_acc: 0.9241\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 45us/sample - loss: 0.1034 - acc: 0.9703 - val_loss: 0.3069 - val_acc: 0.9375\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1054 - acc: 0.9712 - val_loss: 0.3299 - val_acc: 0.9304\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 42us/sample - loss: 0.1282 - acc: 0.9664 - val_loss: 0.3221 - val_acc: 0.9286\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1095 - acc: 0.9697 - val_loss: 0.3126 - val_acc: 0.9268\n",
            "Performance on the validation set\n",
            "loss: 0.23\n",
            "acc: 0.94\n",
            "\n",
            "Overall  loss :  0.24 +- 0.01\n",
            "Overall  acc :  0.93 +- 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f98FIk3T1MoF",
        "colab_type": "code",
        "outputId": "e85ddd7e-23c6-4e02-f462-d01b587c7b35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "x_plot = list(range(1, len(best_history.history['val_acc']) + 1))\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(x_plot, history.history['loss'])\n",
        "    plt.plot(x_plot, history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.plot(x_plot, history.history['acc'])\n",
        "    plt.plot(x_plot, history.history['val_acc'])\n",
        "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_history(best_history)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIVmCMMYYE5IlCGOMMSH9PxuJgEeuG4MNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FZyEF3wVu_E",
        "colab_type": "code",
        "outputId": "8618872a-eda2-4b0d-c8ec-9976f0e81383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# labels_val_from_categorical = np.argmax(labels_val, axis=1)\n",
        "predictions = np.argmax(best_model.predict(x_val), axis=1)\n",
        "print(classification_report(labels_val, predictions))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.95       273\n",
            "           1       0.96      0.95      0.95       247\n",
            "           2       0.93      0.92      0.93       292\n",
            "           3       0.96      0.98      0.97       251\n",
            "           4       0.97      0.93      0.95       262\n",
            "           5       0.89      0.93      0.91       239\n",
            "           6       0.91      0.88      0.89       261\n",
            "           7       0.95      0.97      0.96       269\n",
            "           8       0.94      0.94      0.94       266\n",
            "           9       0.93      0.91      0.92       279\n",
            "          10       0.96      0.96      0.96       161\n",
            "\n",
            "    accuracy                           0.94      2800\n",
            "   macro avg       0.94      0.94      0.94      2800\n",
            "weighted avg       0.94      0.94      0.94      2800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flw70YN7sJAx",
        "colab_type": "text"
      },
      "source": [
        "# Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "401e4c7b-8d9d-4c63-8be1-8857858165ac",
        "id": "Cn99NR8CsjCe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "monitor = 'val_loss'\n",
        "early_stopping = EarlyStopping(monitor=monitor, patience=10, min_delta=0.001, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=10, verbose=1)\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "model_type = 'autoencoder'\n",
        "if model_type == 'autoencoder_nn':\n",
        "  labels_train = [y_train, x_train]\n",
        "  labels_val = [y_val, x_val]\n",
        "elif model_type == 'nn':\n",
        "  labels_train = y_train\n",
        "  labels_val = y_val\n",
        "elif model_type == 'autoencoder':\n",
        "  labels_train = x_train\n",
        "  labels_val = x_val\n",
        "\n",
        "autoencoder, encoder = get_model(\n",
        "    model_type=model_type, \n",
        "    tied=False,\n",
        "    divide_by=[1.5, 2, 2.5, 3],\n",
        "    hidden_first=512, \n",
        "    hidden_last=32, \n",
        "    noise_type=None, \n",
        "    dropout=False, \n",
        "    batch_norm=False, \n",
        "    standard=False, \n",
        "    verbose=True\n",
        ")\n",
        "history_autoencoder = autoencoder.fit(\n",
        "    x_train, \n",
        "    labels_train,\n",
        "    validation_data=(x_val, labels_val), \n",
        "    epochs=300, \n",
        "    batch_size=128, \n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'dense_68/kernel:0' shape=(104, 34) dtype=float32>\n",
            "dense_67\n",
            "<tf.Variable 'dense_67/kernel:0' shape=(261, 104) dtype=float32>\n",
            "dense_66\n",
            "<tf.Variable 'dense_66/kernel:0' shape=(522, 261) dtype=float32>\n",
            "<tf.Variable 'dense_65/kernel:0' shape=(784, 522) dtype=float32>\n",
            "Model: \"model_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_17 (InputLayer)        [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dense_transpose_17 (DenseTra (None, 104)               3674      \n",
            "_________________________________________________________________\n",
            "dense_transpose_18 (DenseTra (None, 261)               27509     \n",
            "_________________________________________________________________\n",
            "dense_transpose_19 (DenseTra (None, 522)               137025    \n",
            "_________________________________________________________________\n",
            "decoder (DenseTranspose)     (None, 784)               410554    \n",
            "=================================================================\n",
            "Total params: 578,762\n",
            "Trainable params: 578,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 11200 samples, validate on 2800 samples\n",
            "Epoch 1/300\n",
            "11200/11200 [==============================] - 1s 133us/sample - loss: 0.0793 - mean_squared_error: 0.0793 - val_loss: 0.0486 - val_mean_squared_error: 0.0486\n",
            "Epoch 2/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0397 - mean_squared_error: 0.0397 - val_loss: 0.0319 - val_mean_squared_error: 0.0319\n",
            "Epoch 3/300\n",
            "11200/11200 [==============================] - 1s 60us/sample - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0254 - val_mean_squared_error: 0.0254\n",
            "Epoch 4/300\n",
            "11200/11200 [==============================] - 1s 58us/sample - loss: 0.0237 - mean_squared_error: 0.0237 - val_loss: 0.0223 - val_mean_squared_error: 0.0223\n",
            "Epoch 5/300\n",
            "11200/11200 [==============================] - 1s 60us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0199 - val_mean_squared_error: 0.0199\n",
            "Epoch 6/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0184 - val_mean_squared_error: 0.0184\n",
            "Epoch 7/300\n",
            "11200/11200 [==============================] - 1s 60us/sample - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0171 - val_mean_squared_error: 0.0171\n",
            "Epoch 8/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0162 - val_mean_squared_error: 0.0162\n",
            "Epoch 9/300\n",
            "11200/11200 [==============================] - 1s 58us/sample - loss: 0.0152 - mean_squared_error: 0.0152 - val_loss: 0.0153 - val_mean_squared_error: 0.0153\n",
            "Epoch 10/300\n",
            "11200/11200 [==============================] - 1s 58us/sample - loss: 0.0144 - mean_squared_error: 0.0144 - val_loss: 0.0146 - val_mean_squared_error: 0.0146\n",
            "Epoch 11/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0137 - mean_squared_error: 0.0137 - val_loss: 0.0142 - val_mean_squared_error: 0.0142\n",
            "Epoch 12/300\n",
            "11200/11200 [==============================] - 1s 60us/sample - loss: 0.0132 - mean_squared_error: 0.0132 - val_loss: 0.0138 - val_mean_squared_error: 0.0138\n",
            "Epoch 13/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0126 - mean_squared_error: 0.0126 - val_loss: 0.0134 - val_mean_squared_error: 0.0134\n",
            "Epoch 14/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0123 - mean_squared_error: 0.0123 - val_loss: 0.0134 - val_mean_squared_error: 0.0134\n",
            "Epoch 15/300\n",
            "11200/11200 [==============================] - 1s 58us/sample - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0128 - val_mean_squared_error: 0.0128\n",
            "Epoch 16/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0114 - mean_squared_error: 0.0114 - val_loss: 0.0126 - val_mean_squared_error: 0.0126\n",
            "Epoch 17/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0120 - val_mean_squared_error: 0.0120\n",
            "Epoch 18/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 0.0122 - val_mean_squared_error: 0.0122\n",
            "Epoch 19/300\n",
            "11200/11200 [==============================] - 1s 60us/sample - loss: 0.0105 - mean_squared_error: 0.0105 - val_loss: 0.0118 - val_mean_squared_error: 0.0118\n",
            "Epoch 20/300\n",
            "11200/11200 [==============================] - 1s 61us/sample - loss: 0.0103 - mean_squared_error: 0.0103 - val_loss: 0.0115 - val_mean_squared_error: 0.0115\n",
            "Epoch 21/300\n",
            "11200/11200 [==============================] - 1s 58us/sample - loss: 0.0100 - mean_squared_error: 0.0100 - val_loss: 0.0114 - val_mean_squared_error: 0.0114\n",
            "Epoch 22/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0098 - mean_squared_error: 0.0098 - val_loss: 0.0111 - val_mean_squared_error: 0.0111\n",
            "Epoch 23/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0111 - val_mean_squared_error: 0.0111\n",
            "Epoch 24/300\n",
            "11200/11200 [==============================] - 1s 58us/sample - loss: 0.0095 - mean_squared_error: 0.0095 - val_loss: 0.0109 - val_mean_squared_error: 0.0109\n",
            "Epoch 25/300\n",
            "11200/11200 [==============================] - 1s 57us/sample - loss: 0.0094 - mean_squared_error: 0.0094 - val_loss: 0.0110 - val_mean_squared_error: 0.0110\n",
            "Epoch 26/300\n",
            "11200/11200 [==============================] - 1s 58us/sample - loss: 0.0092 - mean_squared_error: 0.0092 - val_loss: 0.0107 - val_mean_squared_error: 0.0107\n",
            "Epoch 27/300\n",
            "11200/11200 [==============================] - 1s 60us/sample - loss: 0.0090 - mean_squared_error: 0.0090 - val_loss: 0.0105 - val_mean_squared_error: 0.0105\n",
            "Epoch 28/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0105 - val_mean_squared_error: 0.0105\n",
            "Epoch 29/300\n",
            "11200/11200 [==============================] - 1s 58us/sample - loss: 0.0087 - mean_squared_error: 0.0087 - val_loss: 0.0103 - val_mean_squared_error: 0.0103\n",
            "Epoch 30/300\n",
            "11200/11200 [==============================] - 1s 60us/sample - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0105 - val_mean_squared_error: 0.0105\n",
            "Epoch 31/300\n",
            "11200/11200 [==============================] - 1s 58us/sample - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
            "Epoch 32/300\n",
            "11200/11200 [==============================] - 1s 58us/sample - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0103 - val_mean_squared_error: 0.0103\n",
            "Epoch 33/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
            "Epoch 34/300\n",
            "11200/11200 [==============================] - 1s 59us/sample - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
            "Epoch 35/300\n",
            "11200/11200 [==============================] - 1s 60us/sample - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0099 - val_mean_squared_error: 0.0099\n",
            "Epoch 36/300\n",
            "11200/11200 [==============================] - 1s 98us/sample - loss: 0.0079 - mean_squared_error: 0.0079 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k6R9UxDW2Uy",
        "colab_type": "code",
        "outputId": "81803e20-4639-4e3a-884e-f975c1ec5b30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# print accuracy\n",
        "x_plot = list(range(1, len(history_autoencoder.history['val_loss']) + 1))\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(x_plot, history.history['loss'])\n",
        "    plt.plot(x_plot, history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "plot_history(history_autoencoder)\n",
        "\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "n=10\n",
        "plt.figure(figsize=(40, 4))\n",
        "for i in range(n):\n",
        "    # display original images\n",
        "    ax = plt.subplot(3, 20, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "    # display encoded images\n",
        "    # ax = plt.subplot(3, 20, i + 1 + 20)\n",
        "    # plt.imshow(encoded_imgs[i].reshape(8,4))\n",
        "    # plt.gray()\n",
        "    # ax.get_xaxis().set_visible(False)\n",
        "    # ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "    # display reconstructed images\n",
        "    ax = plt.subplot(3, 20, 2*20 +i+ 1)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "  \n",
        "plt.show()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAfbTnHwbsGaSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABEYAAADrCAYAAABkdZM2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJPWWEyOSJEmSJKm3/j/pGZ3N4UaevQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 2880x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ0eUWrqBKjh",
        "colab_type": "text"
      },
      "source": [
        "## Train neural net with features learned by the autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WklzvF9rYA-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_encoder_nn(\n",
        "    encoder,\n",
        "    encoder_trainable=False,\n",
        "    divide_by=2,\n",
        "    hidden_first=128,\n",
        "    hidden_last=32,\n",
        "    dropout=True, \n",
        "    dropout_rate=0.3,\n",
        "    batch_norm=False, \n",
        "    standard=False,\n",
        "    verbose=True\n",
        "):\n",
        "  \n",
        "  encoder.trainable = encoder_trainable\n",
        "  encoder_nn = Sequential()\n",
        "  encoder_nn.add(encoder)\n",
        "  \n",
        "  \"\"\" if isinstance(divide_by, list) and divide_by != []:\n",
        "    dim = int(IMG_SHAPE / divide_by[0])\n",
        "    encoder_nn.add(layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "  else:\n",
        "    encoder_nn.add(layers.Dense(hidden_first, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "  if dropout:\n",
        "    encoder_nn.add(layers.Dropout(dropout_rate)) \"\"\"\n",
        "  if isinstance(divide_by, list) and divide_by != []:\n",
        "    i = 0\n",
        "    while i < len(divide_by) and int(dim / divide_by[i]) > hidden_last:\n",
        "      if batch_norm:\n",
        "        encoder_nn.add(layers.BatchNormalization())\n",
        "      encoder_nn.add(layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "      if dropout:\n",
        "        encoder_nn.add(layers.Dropout(dropout_rate))\n",
        "      dim = int(dim / divide_by[i])\n",
        "      i += 1\n",
        "  else:\n",
        "    while hidden_first > hidden_last:\n",
        "      if batch_norm:\n",
        "        encoder_nn.add(layers.BatchNormalization())\n",
        "      encoder_nn.add(layers.Dense(hidden_first, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "      if dropout:\n",
        "        encoder_nn.add(layers.Dropout(dropout_rate))\n",
        "      hidden_first = int(hidden_first / divide_by)\n",
        "  encoder_nn.add(layers.Dense(N_CLASSES, activation=\"softmax\", kernel_initializer=\"he_normal\"))\n",
        "  encoder_nn.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  if verbose:\n",
        "    encoder_nn.summary()\n",
        "  \n",
        "  return encoder_nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymY9NxiBsLDK",
        "colab_type": "code",
        "outputId": "2cab5ad5-f6d1-4c76-d582-84b8b4faa3cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "monitor = 'val_loss'\n",
        "early_stopping = EarlyStopping(monitor=monitor, patience=10, min_delta=0.001, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=10, verbose=1)\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "model_type = 'nn'\n",
        "if model_type == 'autoencoder_nn':\n",
        "  labels_train = [y_train, x_train]\n",
        "  labels_val = [y_val, x_val]\n",
        "elif model_type == 'nn':\n",
        "  labels_train = y_train\n",
        "  labels_val = y_val\n",
        "elif model_type == 'autoencoder':\n",
        "  labels_train = x_train\n",
        "  labels_val = x_val\n",
        "\n",
        "# define 10-fold cross validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "scores_val = []\n",
        "best_score = 0  # based on accuracy\n",
        "best_model = None\n",
        "best_history = None\n",
        "\n",
        "for idx, (train, val) in enumerate(kfold.split(x_train, labels_train)):\n",
        "  print('FOLD: ', idx + 1)\n",
        "  # create model\n",
        "  model = get_encoder_nn(encoder, hidden_first=32, hidden_last=32, dropout=True, encoder_trainable=False)\n",
        "  # Fit the model\n",
        "  history = model.fit(\n",
        "      x_train[train], \n",
        "      labels_train[train], \n",
        "      validation_data=(x_train[val], labels_train[val]), \n",
        "      epochs=300, \n",
        "      batch_size=128, \n",
        "      callbacks=callbacks\n",
        "  )\n",
        "  # evaluate the model\n",
        "  score = model.evaluate(x_val, labels_val, verbose=0)\n",
        "  # save best_model\n",
        "  if score[1] > best_score:  # accuracy\n",
        "    best_score = score[1]\n",
        "    best_model = model\n",
        "    best_history = history\n",
        "  print('Performance on the validation set')\n",
        "  for idx in range(len(model.metrics_names)):\n",
        "    print(\"%s: %.2f\" % (model.metrics_names[idx], score[idx]))\n",
        "  print()\n",
        "  scores_val.append(score)\n",
        "  \n",
        "scores_val = np.array(scores_val)\n",
        "overall_means = np.mean(scores_val, axis=0)\n",
        "overall_stds = np.std(scores_val, axis=0)\n",
        "for idx in range(len(overall_means)):\n",
        "  print('Overall ', model.metrics_names[idx], ': ', '{:.2f}'.format(overall_means[idx]), '+-', '{:.2f}'.format(overall_stds[idx]))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD:  1\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_21 (Model)             (None, 34)                577091    \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 385\n",
            "Non-trainable params: 577,091\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 110us/sample - loss: 9.2294 - acc: 0.0649 - val_loss: 5.6624 - val_acc: 0.0929\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 4.9779 - acc: 0.0859 - val_loss: 4.6450 - val_acc: 0.1009\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 4.1450 - acc: 0.1190 - val_loss: 3.8131 - val_acc: 0.1500\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 3.4011 - acc: 0.1760 - val_loss: 3.1297 - val_acc: 0.2116\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 2.7922 - acc: 0.2517 - val_loss: 2.5795 - val_acc: 0.3027\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 2.3251 - acc: 0.3316 - val_loss: 2.1805 - val_acc: 0.3679\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.9713 - acc: 0.4035 - val_loss: 1.8677 - val_acc: 0.4482\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 1.7085 - acc: 0.4664 - val_loss: 1.6437 - val_acc: 0.5009\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.5121 - acc: 0.5214 - val_loss: 1.4684 - val_acc: 0.5357\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.3624 - acc: 0.5639 - val_loss: 1.3322 - val_acc: 0.5795\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 1.2464 - acc: 0.5955 - val_loss: 1.2337 - val_acc: 0.6089\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 1.1543 - acc: 0.6267 - val_loss: 1.1504 - val_acc: 0.6330\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.0793 - acc: 0.6514 - val_loss: 1.0797 - val_acc: 0.6500\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 1.0211 - acc: 0.6679 - val_loss: 1.0297 - val_acc: 0.6589\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.9752 - acc: 0.6838 - val_loss: 0.9915 - val_acc: 0.6705\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.9372 - acc: 0.6968 - val_loss: 0.9482 - val_acc: 0.6920\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.9027 - acc: 0.7104 - val_loss: 0.9188 - val_acc: 0.6982\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8752 - acc: 0.7182 - val_loss: 0.8836 - val_acc: 0.7188\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8538 - acc: 0.7261 - val_loss: 0.8701 - val_acc: 0.7241\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8358 - acc: 0.7338 - val_loss: 0.8467 - val_acc: 0.7330\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 0.8186 - acc: 0.7409 - val_loss: 0.8284 - val_acc: 0.7446\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.8037 - acc: 0.7469 - val_loss: 0.8144 - val_acc: 0.7527\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7901 - acc: 0.7528 - val_loss: 0.8025 - val_acc: 0.7509\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7774 - acc: 0.7549 - val_loss: 0.7928 - val_acc: 0.7536\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7683 - acc: 0.7599 - val_loss: 0.7807 - val_acc: 0.7598\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7600 - acc: 0.7628 - val_loss: 0.7656 - val_acc: 0.7634\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7517 - acc: 0.7634 - val_loss: 0.7598 - val_acc: 0.7705\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7468 - acc: 0.7677 - val_loss: 0.7516 - val_acc: 0.7696\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7380 - acc: 0.7699 - val_loss: 0.7462 - val_acc: 0.7696\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7331 - acc: 0.7715 - val_loss: 0.7432 - val_acc: 0.7759\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7283 - acc: 0.7711 - val_loss: 0.7377 - val_acc: 0.7759\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7227 - acc: 0.7739 - val_loss: 0.7297 - val_acc: 0.7804\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7195 - acc: 0.7742 - val_loss: 0.7252 - val_acc: 0.7759\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7139 - acc: 0.7786 - val_loss: 0.7236 - val_acc: 0.7848\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7097 - acc: 0.7759 - val_loss: 0.7328 - val_acc: 0.7732\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7092 - acc: 0.7779 - val_loss: 0.7177 - val_acc: 0.7795\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7056 - acc: 0.7777 - val_loss: 0.7098 - val_acc: 0.7875\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7020 - acc: 0.7804 - val_loss: 0.7030 - val_acc: 0.7848\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7009 - acc: 0.7810 - val_loss: 0.7073 - val_acc: 0.7875\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6961 - acc: 0.7831 - val_loss: 0.7016 - val_acc: 0.7884\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6952 - acc: 0.7832 - val_loss: 0.7020 - val_acc: 0.7884\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6946 - acc: 0.7814 - val_loss: 0.6943 - val_acc: 0.7893\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6910 - acc: 0.7838 - val_loss: 0.6954 - val_acc: 0.7929\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6879 - acc: 0.7853 - val_loss: 0.6949 - val_acc: 0.8000\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6883 - acc: 0.7848 - val_loss: 0.6949 - val_acc: 0.7884\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6853 - acc: 0.7869 - val_loss: 0.6932 - val_acc: 0.7920\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6844 - acc: 0.7851 - val_loss: 0.6880 - val_acc: 0.7884\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6836 - acc: 0.7847 - val_loss: 0.6990 - val_acc: 0.7875\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6826 - acc: 0.7854 - val_loss: 0.6917 - val_acc: 0.7911\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6832 - acc: 0.7873 - val_loss: 0.7013 - val_acc: 0.7848\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6806 - acc: 0.7869 - val_loss: 0.6889 - val_acc: 0.7946\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6782 - acc: 0.7873 - val_loss: 0.6849 - val_acc: 0.7991\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6782 - acc: 0.7872 - val_loss: 0.6873 - val_acc: 0.7973\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6763 - acc: 0.7891 - val_loss: 0.6832 - val_acc: 0.7929\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6778 - acc: 0.7862 - val_loss: 0.6795 - val_acc: 0.8027\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 0.6766 - acc: 0.7883 - val_loss: 0.6818 - val_acc: 0.7937\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6748 - acc: 0.7903 - val_loss: 0.6822 - val_acc: 0.8009\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6745 - acc: 0.7902 - val_loss: 0.6760 - val_acc: 0.8000\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 0.6754 - acc: 0.7873 - val_loss: 0.6865 - val_acc: 0.7964\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6739 - acc: 0.7895 - val_loss: 0.6804 - val_acc: 0.7982\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6743 - acc: 0.7889 - val_loss: 0.6778 - val_acc: 0.7964\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 0.6716 - acc: 0.7921 - val_loss: 0.6766 - val_acc: 0.7964\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6730 - acc: 0.7881 - val_loss: 0.6773 - val_acc: 0.8000\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6707 - acc: 0.7910 - val_loss: 0.6755 - val_acc: 0.7964\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 0.6710 - acc: 0.7894 - val_loss: 0.6829 - val_acc: 0.7982\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6708 - acc: 0.7903 - val_loss: 0.6720 - val_acc: 0.8018\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6687 - acc: 0.7900 - val_loss: 0.6728 - val_acc: 0.7991\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6691 - acc: 0.7916 - val_loss: 0.6763 - val_acc: 0.8027\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 0.6690 - acc: 0.7916 - val_loss: 0.6746 - val_acc: 0.8000\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6698 - acc: 0.7921 - val_loss: 0.6775 - val_acc: 0.7902\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6704 - acc: 0.7918 - val_loss: 0.6688 - val_acc: 0.8000\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6673 - acc: 0.7915 - val_loss: 0.6710 - val_acc: 0.7964\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6682 - acc: 0.7899 - val_loss: 0.6701 - val_acc: 0.8036\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6676 - acc: 0.7910 - val_loss: 0.6719 - val_acc: 0.8080\n",
            "Epoch 75/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 0.6679 - acc: 0.7923 - val_loss: 0.6665 - val_acc: 0.8000\n",
            "Epoch 76/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 0.6666 - acc: 0.7918 - val_loss: 0.6765 - val_acc: 0.8036\n",
            "Epoch 77/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6673 - acc: 0.7948 - val_loss: 0.6716 - val_acc: 0.8054\n",
            "Epoch 78/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6671 - acc: 0.7924 - val_loss: 0.6705 - val_acc: 0.8045\n",
            "Epoch 79/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6653 - acc: 0.7911 - val_loss: 0.6717 - val_acc: 0.8045\n",
            "Epoch 80/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6682 - acc: 0.7914 - val_loss: 0.6699 - val_acc: 0.8054\n",
            "Epoch 81/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6689 - acc: 0.7928 - val_loss: 0.6692 - val_acc: 0.8062\n",
            "Epoch 82/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6667 - acc: 0.7930 - val_loss: 0.6686 - val_acc: 0.8071\n",
            "Epoch 83/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6651 - acc: 0.7927 - val_loss: 0.6669 - val_acc: 0.8062\n",
            "Epoch 84/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6659 - acc: 0.7947 - val_loss: 0.6656 - val_acc: 0.8098\n",
            "Epoch 85/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.6653 - acc: 0.7952 - val_loss: 0.6667 - val_acc: 0.8071\n",
            "Performance on the validation set\n",
            "loss: 0.63\n",
            "acc: 0.80\n",
            "\n",
            "FOLD:  2\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_21 (Model)             (None, 34)                577091    \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 385\n",
            "Non-trainable params: 577,091\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 114us/sample - loss: 10.4202 - acc: 0.1039 - val_loss: 6.6328 - val_acc: 0.0946\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 5.6722 - acc: 0.0976 - val_loss: 4.6321 - val_acc: 0.1295\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 4.1245 - acc: 0.1406 - val_loss: 3.7948 - val_acc: 0.1813\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 3.4082 - acc: 0.1928 - val_loss: 3.1554 - val_acc: 0.2348\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 2.8358 - acc: 0.2594 - val_loss: 2.6473 - val_acc: 0.2929\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 2.3905 - acc: 0.3325 - val_loss: 2.2713 - val_acc: 0.3714\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 2.0591 - acc: 0.4045 - val_loss: 1.9831 - val_acc: 0.4187\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.8070 - acc: 0.4591 - val_loss: 1.7681 - val_acc: 0.4777\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.6133 - acc: 0.5073 - val_loss: 1.6061 - val_acc: 0.5134\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.4619 - acc: 0.5492 - val_loss: 1.4801 - val_acc: 0.5455\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.3424 - acc: 0.5869 - val_loss: 1.3715 - val_acc: 0.5714\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.2456 - acc: 0.6173 - val_loss: 1.2851 - val_acc: 0.6036\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.1662 - acc: 0.6351 - val_loss: 1.2198 - val_acc: 0.6161\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.1008 - acc: 0.6544 - val_loss: 1.1528 - val_acc: 0.6402\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.0452 - acc: 0.6720 - val_loss: 1.1059 - val_acc: 0.6429\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 1.0008 - acc: 0.6816 - val_loss: 1.0622 - val_acc: 0.6607\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.9598 - acc: 0.6938 - val_loss: 1.0283 - val_acc: 0.6634\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.9280 - acc: 0.7058 - val_loss: 0.9872 - val_acc: 0.6929\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8985 - acc: 0.7164 - val_loss: 0.9670 - val_acc: 0.6955\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8723 - acc: 0.7253 - val_loss: 0.9453 - val_acc: 0.7018\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8500 - acc: 0.7324 - val_loss: 0.9267 - val_acc: 0.7000\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8304 - acc: 0.7385 - val_loss: 0.9082 - val_acc: 0.7098\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8127 - acc: 0.7446 - val_loss: 0.8912 - val_acc: 0.7223\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7998 - acc: 0.7498 - val_loss: 0.8726 - val_acc: 0.7286\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7858 - acc: 0.7538 - val_loss: 0.8595 - val_acc: 0.7259\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7736 - acc: 0.7586 - val_loss: 0.8494 - val_acc: 0.7330\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7632 - acc: 0.7626 - val_loss: 0.8388 - val_acc: 0.7375\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7545 - acc: 0.7658 - val_loss: 0.8311 - val_acc: 0.7429\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7459 - acc: 0.7680 - val_loss: 0.8257 - val_acc: 0.7464\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7394 - acc: 0.7717 - val_loss: 0.8191 - val_acc: 0.7464\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7327 - acc: 0.7743 - val_loss: 0.8140 - val_acc: 0.7491\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7250 - acc: 0.7777 - val_loss: 0.8059 - val_acc: 0.7500\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7195 - acc: 0.7778 - val_loss: 0.8019 - val_acc: 0.7509\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7163 - acc: 0.7770 - val_loss: 0.7976 - val_acc: 0.7554\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7115 - acc: 0.7801 - val_loss: 0.7947 - val_acc: 0.7643\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7071 - acc: 0.7812 - val_loss: 0.7893 - val_acc: 0.7527\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7034 - acc: 0.7822 - val_loss: 0.7839 - val_acc: 0.7598\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7005 - acc: 0.7845 - val_loss: 0.7824 - val_acc: 0.7536\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6971 - acc: 0.7845 - val_loss: 0.7864 - val_acc: 0.7545\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6928 - acc: 0.7861 - val_loss: 0.7791 - val_acc: 0.7696\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6910 - acc: 0.7873 - val_loss: 0.7869 - val_acc: 0.7509\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6896 - acc: 0.7863 - val_loss: 0.7792 - val_acc: 0.7607\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6868 - acc: 0.7874 - val_loss: 0.7774 - val_acc: 0.7634\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6825 - acc: 0.7903 - val_loss: 0.7752 - val_acc: 0.7723\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6846 - acc: 0.7857 - val_loss: 0.7737 - val_acc: 0.7696\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6797 - acc: 0.7898 - val_loss: 0.7736 - val_acc: 0.7634\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6786 - acc: 0.7914 - val_loss: 0.7760 - val_acc: 0.7723\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6775 - acc: 0.7901 - val_loss: 0.7710 - val_acc: 0.7616\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6765 - acc: 0.7909 - val_loss: 0.7654 - val_acc: 0.7670\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6754 - acc: 0.7907 - val_loss: 0.7667 - val_acc: 0.7616\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6720 - acc: 0.7911 - val_loss: 0.7695 - val_acc: 0.7545\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6717 - acc: 0.7937 - val_loss: 0.7688 - val_acc: 0.7571\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6693 - acc: 0.7923 - val_loss: 0.7617 - val_acc: 0.7679\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6709 - acc: 0.7940 - val_loss: 0.7620 - val_acc: 0.7696\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6695 - acc: 0.7921 - val_loss: 0.7632 - val_acc: 0.7696\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6688 - acc: 0.7944 - val_loss: 0.7615 - val_acc: 0.7679\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6673 - acc: 0.7937 - val_loss: 0.7590 - val_acc: 0.7607\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6686 - acc: 0.7934 - val_loss: 0.7607 - val_acc: 0.7696\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6635 - acc: 0.7959 - val_loss: 0.7661 - val_acc: 0.7652\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6643 - acc: 0.7937 - val_loss: 0.7672 - val_acc: 0.7696\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6640 - acc: 0.7951 - val_loss: 0.7546 - val_acc: 0.7714\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6635 - acc: 0.7963 - val_loss: 0.7590 - val_acc: 0.7741\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6636 - acc: 0.7963 - val_loss: 0.7623 - val_acc: 0.7705\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6618 - acc: 0.7950 - val_loss: 0.7620 - val_acc: 0.7732\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6621 - acc: 0.7966 - val_loss: 0.7626 - val_acc: 0.7741\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6628 - acc: 0.7938 - val_loss: 0.7567 - val_acc: 0.7688\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6613 - acc: 0.7941 - val_loss: 0.7676 - val_acc: 0.7696\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6600 - acc: 0.7974 - val_loss: 0.7596 - val_acc: 0.7759\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6589 - acc: 0.7987 - val_loss: 0.7554 - val_acc: 0.7688\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6588 - acc: 0.7955 - val_loss: 0.7641 - val_acc: 0.7661\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.6596 - acc: 0.7961 - val_loss: 0.7648 - val_acc: 0.7696\n",
            "Performance on the validation set\n",
            "loss: 0.63\n",
            "acc: 0.80\n",
            "\n",
            "FOLD:  3\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_21 (Model)             (None, 34)                577091    \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 385\n",
            "Non-trainable params: 577,091\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 116us/sample - loss: 8.6035 - acc: 0.0923 - val_loss: 5.3050 - val_acc: 0.0875\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 4.5320 - acc: 0.1219 - val_loss: 4.0901 - val_acc: 0.1339\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 3.6856 - acc: 0.1784 - val_loss: 3.3708 - val_acc: 0.2009\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 3.0386 - acc: 0.2550 - val_loss: 2.8141 - val_acc: 0.2875\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 2.5574 - acc: 0.3312 - val_loss: 2.4126 - val_acc: 0.3732\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 2.2020 - acc: 0.3965 - val_loss: 2.1096 - val_acc: 0.4214\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.9302 - acc: 0.4527 - val_loss: 1.8762 - val_acc: 0.4688\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.7232 - acc: 0.5010 - val_loss: 1.7041 - val_acc: 0.4955\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.5600 - acc: 0.5410 - val_loss: 1.5572 - val_acc: 0.5366\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.4281 - acc: 0.5743 - val_loss: 1.4409 - val_acc: 0.5714\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.3219 - acc: 0.6005 - val_loss: 1.3508 - val_acc: 0.5973\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.2359 - acc: 0.6262 - val_loss: 1.2779 - val_acc: 0.6027\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 1.1623 - acc: 0.6427 - val_loss: 1.1944 - val_acc: 0.6393\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.1023 - acc: 0.6581 - val_loss: 1.1433 - val_acc: 0.6446\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.0506 - acc: 0.6737 - val_loss: 1.0986 - val_acc: 0.6509\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.0076 - acc: 0.6864 - val_loss: 1.0498 - val_acc: 0.6732\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.9703 - acc: 0.6965 - val_loss: 1.0159 - val_acc: 0.6804\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.9374 - acc: 0.7084 - val_loss: 0.9825 - val_acc: 0.6955\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.9089 - acc: 0.7163 - val_loss: 0.9571 - val_acc: 0.7009\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8852 - acc: 0.7251 - val_loss: 0.9351 - val_acc: 0.7071\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8624 - acc: 0.7302 - val_loss: 0.9115 - val_acc: 0.7179\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8447 - acc: 0.7389 - val_loss: 0.8932 - val_acc: 0.7196\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8279 - acc: 0.7414 - val_loss: 0.8761 - val_acc: 0.7232\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8119 - acc: 0.7471 - val_loss: 0.8663 - val_acc: 0.7214\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7984 - acc: 0.7499 - val_loss: 0.8369 - val_acc: 0.7330\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7860 - acc: 0.7538 - val_loss: 0.8338 - val_acc: 0.7375\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7745 - acc: 0.7550 - val_loss: 0.8235 - val_acc: 0.7304\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7659 - acc: 0.7613 - val_loss: 0.8063 - val_acc: 0.7437\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7574 - acc: 0.7615 - val_loss: 0.7959 - val_acc: 0.7491\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7493 - acc: 0.7671 - val_loss: 0.7872 - val_acc: 0.7455\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7417 - acc: 0.7669 - val_loss: 0.7847 - val_acc: 0.7482\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7358 - acc: 0.7700 - val_loss: 0.7726 - val_acc: 0.7464\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7290 - acc: 0.7719 - val_loss: 0.7714 - val_acc: 0.7500\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 0.7222 - acc: 0.7745 - val_loss: 0.7641 - val_acc: 0.7536\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7184 - acc: 0.7762 - val_loss: 0.7621 - val_acc: 0.7616\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7143 - acc: 0.7747 - val_loss: 0.7563 - val_acc: 0.7580\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7118 - acc: 0.7740 - val_loss: 0.7611 - val_acc: 0.7616\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7074 - acc: 0.7787 - val_loss: 0.7444 - val_acc: 0.7607\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7027 - acc: 0.7833 - val_loss: 0.7387 - val_acc: 0.7607\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6989 - acc: 0.7818 - val_loss: 0.7396 - val_acc: 0.7607\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6961 - acc: 0.7850 - val_loss: 0.7395 - val_acc: 0.7705\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6934 - acc: 0.7836 - val_loss: 0.7293 - val_acc: 0.7714\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6913 - acc: 0.7865 - val_loss: 0.7369 - val_acc: 0.7670\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6916 - acc: 0.7844 - val_loss: 0.7305 - val_acc: 0.7750\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6871 - acc: 0.7874 - val_loss: 0.7300 - val_acc: 0.7750\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6850 - acc: 0.7862 - val_loss: 0.7224 - val_acc: 0.7679\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6825 - acc: 0.7862 - val_loss: 0.7262 - val_acc: 0.7741\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6802 - acc: 0.7875 - val_loss: 0.7197 - val_acc: 0.7759\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6808 - acc: 0.7883 - val_loss: 0.7225 - val_acc: 0.7777\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6802 - acc: 0.7905 - val_loss: 0.7236 - val_acc: 0.7759\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6785 - acc: 0.7877 - val_loss: 0.7342 - val_acc: 0.7732\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6791 - acc: 0.7914 - val_loss: 0.7176 - val_acc: 0.7804\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6762 - acc: 0.7891 - val_loss: 0.7127 - val_acc: 0.7723\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6747 - acc: 0.7913 - val_loss: 0.7101 - val_acc: 0.7795\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6752 - acc: 0.7902 - val_loss: 0.7098 - val_acc: 0.7857\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6728 - acc: 0.7905 - val_loss: 0.7077 - val_acc: 0.7821\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6702 - acc: 0.7941 - val_loss: 0.7223 - val_acc: 0.7759\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6709 - acc: 0.7940 - val_loss: 0.7173 - val_acc: 0.7732\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6713 - acc: 0.7915 - val_loss: 0.7134 - val_acc: 0.7714\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6718 - acc: 0.7916 - val_loss: 0.7100 - val_acc: 0.7786\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6694 - acc: 0.7926 - val_loss: 0.7135 - val_acc: 0.7768\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6681 - acc: 0.7944 - val_loss: 0.7090 - val_acc: 0.7777\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6678 - acc: 0.7928 - val_loss: 0.7081 - val_acc: 0.7795\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6689 - acc: 0.7930 - val_loss: 0.7156 - val_acc: 0.7741\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6679 - acc: 0.7936 - val_loss: 0.7080 - val_acc: 0.7768\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.6659 - acc: 0.7952 - val_loss: 0.7205 - val_acc: 0.7714\n",
            "Performance on the validation set\n",
            "loss: 0.64\n",
            "acc: 0.80\n",
            "\n",
            "FOLD:  4\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_21 (Model)             (None, 34)                577091    \n",
            "_________________________________________________________________\n",
            "dense_72 (Dense)             (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 385\n",
            "Non-trainable params: 577,091\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 119us/sample - loss: 5.9206 - acc: 0.1617 - val_loss: 4.4136 - val_acc: 0.1991\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 4.1323 - acc: 0.2067 - val_loss: 3.6622 - val_acc: 0.2482\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 3.4235 - acc: 0.2592 - val_loss: 3.0220 - val_acc: 0.3071\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 2.8461 - acc: 0.3187 - val_loss: 2.5162 - val_acc: 0.3759\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 2.3901 - acc: 0.3786 - val_loss: 2.1339 - val_acc: 0.4393\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 2.0408 - acc: 0.4355 - val_loss: 1.8394 - val_acc: 0.4973\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 1.7711 - acc: 0.4894 - val_loss: 1.6209 - val_acc: 0.5589\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.5698 - acc: 0.5326 - val_loss: 1.4491 - val_acc: 0.5991\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.4115 - acc: 0.5756 - val_loss: 1.3249 - val_acc: 0.6286\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.2907 - acc: 0.6044 - val_loss: 1.2340 - val_acc: 0.6411\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.1952 - acc: 0.6311 - val_loss: 1.1438 - val_acc: 0.6768\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.1171 - acc: 0.6563 - val_loss: 1.0763 - val_acc: 0.6830\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.0551 - acc: 0.6750 - val_loss: 1.0250 - val_acc: 0.6955\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.0009 - acc: 0.6907 - val_loss: 0.9911 - val_acc: 0.7063\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.9582 - acc: 0.7047 - val_loss: 0.9460 - val_acc: 0.7250\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.9251 - acc: 0.7131 - val_loss: 0.9128 - val_acc: 0.7277\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8948 - acc: 0.7205 - val_loss: 0.8917 - val_acc: 0.7304\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8686 - acc: 0.7289 - val_loss: 0.8654 - val_acc: 0.7375\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8475 - acc: 0.7386 - val_loss: 0.8492 - val_acc: 0.7491\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8271 - acc: 0.7432 - val_loss: 0.8272 - val_acc: 0.7536\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8115 - acc: 0.7491 - val_loss: 0.8076 - val_acc: 0.7554\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7972 - acc: 0.7534 - val_loss: 0.7984 - val_acc: 0.7616\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7842 - acc: 0.7574 - val_loss: 0.7875 - val_acc: 0.7679\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7729 - acc: 0.7607 - val_loss: 0.7737 - val_acc: 0.7661\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7629 - acc: 0.7650 - val_loss: 0.7624 - val_acc: 0.7741\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7563 - acc: 0.7667 - val_loss: 0.7563 - val_acc: 0.7723\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7486 - acc: 0.7671 - val_loss: 0.7470 - val_acc: 0.7821\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7391 - acc: 0.7720 - val_loss: 0.7409 - val_acc: 0.7795\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7350 - acc: 0.7728 - val_loss: 0.7322 - val_acc: 0.7795\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7277 - acc: 0.7753 - val_loss: 0.7322 - val_acc: 0.7830\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7226 - acc: 0.7768 - val_loss: 0.7380 - val_acc: 0.7768\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7201 - acc: 0.7801 - val_loss: 0.7199 - val_acc: 0.7875\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7154 - acc: 0.7794 - val_loss: 0.7178 - val_acc: 0.7839\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7124 - acc: 0.7803 - val_loss: 0.7135 - val_acc: 0.7902\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7078 - acc: 0.7808 - val_loss: 0.7129 - val_acc: 0.7821\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.7052 - acc: 0.7824 - val_loss: 0.7013 - val_acc: 0.7911\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7006 - acc: 0.7841 - val_loss: 0.7061 - val_acc: 0.7830\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6977 - acc: 0.7839 - val_loss: 0.6986 - val_acc: 0.7929\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6951 - acc: 0.7847 - val_loss: 0.7022 - val_acc: 0.7893\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6948 - acc: 0.7833 - val_loss: 0.7028 - val_acc: 0.7911\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6901 - acc: 0.7893 - val_loss: 0.6941 - val_acc: 0.7902\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6898 - acc: 0.7867 - val_loss: 0.6995 - val_acc: 0.7911\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6878 - acc: 0.7888 - val_loss: 0.6955 - val_acc: 0.7893\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6882 - acc: 0.7901 - val_loss: 0.6881 - val_acc: 0.7982\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6872 - acc: 0.7860 - val_loss: 0.6987 - val_acc: 0.7875\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6857 - acc: 0.7880 - val_loss: 0.6841 - val_acc: 0.7964\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6838 - acc: 0.7880 - val_loss: 0.6868 - val_acc: 0.7893\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6818 - acc: 0.7895 - val_loss: 0.6833 - val_acc: 0.7937\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6799 - acc: 0.7919 - val_loss: 0.6874 - val_acc: 0.7884\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6802 - acc: 0.7877 - val_loss: 0.6765 - val_acc: 0.7991\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6770 - acc: 0.7911 - val_loss: 0.6810 - val_acc: 0.7937\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6762 - acc: 0.7916 - val_loss: 0.6804 - val_acc: 0.7920\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6760 - acc: 0.7919 - val_loss: 0.6798 - val_acc: 0.7982\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6732 - acc: 0.7919 - val_loss: 0.6930 - val_acc: 0.7866\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6752 - acc: 0.7910 - val_loss: 0.6745 - val_acc: 0.7946\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6737 - acc: 0.7936 - val_loss: 0.6795 - val_acc: 0.7929\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6746 - acc: 0.7901 - val_loss: 0.6759 - val_acc: 0.7911\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6705 - acc: 0.7920 - val_loss: 0.6729 - val_acc: 0.7946\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6723 - acc: 0.7914 - val_loss: 0.6717 - val_acc: 0.8000\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6726 - acc: 0.7922 - val_loss: 0.6824 - val_acc: 0.7911\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6712 - acc: 0.7914 - val_loss: 0.6748 - val_acc: 0.7920\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6698 - acc: 0.7934 - val_loss: 0.6760 - val_acc: 0.8000\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6694 - acc: 0.7922 - val_loss: 0.6715 - val_acc: 0.7973\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6710 - acc: 0.7937 - val_loss: 0.6686 - val_acc: 0.8009\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6688 - acc: 0.7947 - val_loss: 0.6763 - val_acc: 0.7920\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6702 - acc: 0.7938 - val_loss: 0.6700 - val_acc: 0.7955\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6693 - acc: 0.7927 - val_loss: 0.6715 - val_acc: 0.7955\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6671 - acc: 0.7962 - val_loss: 0.6764 - val_acc: 0.7920\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6679 - acc: 0.7924 - val_loss: 0.6861 - val_acc: 0.7866\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6661 - acc: 0.7948 - val_loss: 0.6729 - val_acc: 0.7955\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6670 - acc: 0.7932 - val_loss: 0.6730 - val_acc: 0.7937\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6680 - acc: 0.7919 - val_loss: 0.6679 - val_acc: 0.8000\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6669 - acc: 0.7924 - val_loss: 0.6683 - val_acc: 0.7964\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.6660 - acc: 0.7947 - val_loss: 0.6706 - val_acc: 0.7893\n",
            "Performance on the validation set\n",
            "loss: 0.63\n",
            "acc: 0.80\n",
            "\n",
            "FOLD:  5\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_21 (Model)             (None, 34)                577091    \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 385\n",
            "Non-trainable params: 577,091\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 121us/sample - loss: 8.0915 - acc: 0.0969 - val_loss: 5.7730 - val_acc: 0.1223\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 4.9100 - acc: 0.1309 - val_loss: 4.1977 - val_acc: 0.1670\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 3.9675 - acc: 0.1842 - val_loss: 3.5448 - val_acc: 0.2438\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 3.3270 - acc: 0.2514 - val_loss: 2.9964 - val_acc: 0.2911\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 2.8235 - acc: 0.3249 - val_loss: 2.5865 - val_acc: 0.3643\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 2.4328 - acc: 0.3963 - val_loss: 2.2520 - val_acc: 0.4241\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 2.1300 - acc: 0.4555 - val_loss: 2.0048 - val_acc: 0.4696\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.8925 - acc: 0.5034 - val_loss: 1.7880 - val_acc: 0.5107\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.6967 - acc: 0.5449 - val_loss: 1.6110 - val_acc: 0.5446\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 1.5370 - acc: 0.5745 - val_loss: 1.4740 - val_acc: 0.5696\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.4004 - acc: 0.5968 - val_loss: 1.3484 - val_acc: 0.6045\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.2873 - acc: 0.6195 - val_loss: 1.2521 - val_acc: 0.6214\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.1908 - acc: 0.6434 - val_loss: 1.1878 - val_acc: 0.6402\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.1153 - acc: 0.6569 - val_loss: 1.1050 - val_acc: 0.6482\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 1.0477 - acc: 0.6744 - val_loss: 1.0484 - val_acc: 0.6714\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.9909 - acc: 0.6906 - val_loss: 1.0031 - val_acc: 0.6938\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.9470 - acc: 0.7039 - val_loss: 0.9618 - val_acc: 0.7018\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.9104 - acc: 0.7134 - val_loss: 0.9330 - val_acc: 0.7071\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8799 - acc: 0.7225 - val_loss: 0.9048 - val_acc: 0.7143\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8532 - acc: 0.7319 - val_loss: 0.8812 - val_acc: 0.7268\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8326 - acc: 0.7376 - val_loss: 0.8652 - val_acc: 0.7321\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8127 - acc: 0.7466 - val_loss: 0.8460 - val_acc: 0.7366\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7980 - acc: 0.7505 - val_loss: 0.8433 - val_acc: 0.7420\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7851 - acc: 0.7540 - val_loss: 0.8206 - val_acc: 0.7500\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7742 - acc: 0.7571 - val_loss: 0.8104 - val_acc: 0.7482\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7629 - acc: 0.7618 - val_loss: 0.7972 - val_acc: 0.7571\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7544 - acc: 0.7688 - val_loss: 0.7924 - val_acc: 0.7473\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7452 - acc: 0.7692 - val_loss: 0.7887 - val_acc: 0.7616\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7384 - acc: 0.7701 - val_loss: 0.7798 - val_acc: 0.7643\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7313 - acc: 0.7748 - val_loss: 0.7806 - val_acc: 0.7688\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7268 - acc: 0.7744 - val_loss: 0.7759 - val_acc: 0.7625\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7207 - acc: 0.7758 - val_loss: 0.7633 - val_acc: 0.7696\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7165 - acc: 0.7762 - val_loss: 0.7635 - val_acc: 0.7696\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7142 - acc: 0.7804 - val_loss: 0.7557 - val_acc: 0.7705\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7096 - acc: 0.7793 - val_loss: 0.7564 - val_acc: 0.7670\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7076 - acc: 0.7812 - val_loss: 0.7535 - val_acc: 0.7705\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7032 - acc: 0.7820 - val_loss: 0.7557 - val_acc: 0.7643\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7017 - acc: 0.7831 - val_loss: 0.7481 - val_acc: 0.7705\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6969 - acc: 0.7876 - val_loss: 0.7441 - val_acc: 0.7777\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6959 - acc: 0.7839 - val_loss: 0.7434 - val_acc: 0.7830\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6920 - acc: 0.7861 - val_loss: 0.7427 - val_acc: 0.7759\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6900 - acc: 0.7866 - val_loss: 0.7364 - val_acc: 0.7795\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6898 - acc: 0.7844 - val_loss: 0.7379 - val_acc: 0.7821\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6872 - acc: 0.7854 - val_loss: 0.7319 - val_acc: 0.7893\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6857 - acc: 0.7874 - val_loss: 0.7370 - val_acc: 0.7768\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 0.6843 - acc: 0.7884 - val_loss: 0.7377 - val_acc: 0.7821\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6836 - acc: 0.7892 - val_loss: 0.7304 - val_acc: 0.7848\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6808 - acc: 0.7877 - val_loss: 0.7314 - val_acc: 0.7821\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6807 - acc: 0.7890 - val_loss: 0.7315 - val_acc: 0.7839\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6800 - acc: 0.7888 - val_loss: 0.7304 - val_acc: 0.7821\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6777 - acc: 0.7893 - val_loss: 0.7265 - val_acc: 0.7812\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6772 - acc: 0.7890 - val_loss: 0.7381 - val_acc: 0.7768\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6762 - acc: 0.7889 - val_loss: 0.7290 - val_acc: 0.7830\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6746 - acc: 0.7884 - val_loss: 0.7399 - val_acc: 0.7786\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6733 - acc: 0.7925 - val_loss: 0.7280 - val_acc: 0.7893\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6735 - acc: 0.7895 - val_loss: 0.7256 - val_acc: 0.7786\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6710 - acc: 0.7918 - val_loss: 0.7238 - val_acc: 0.7786\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6750 - acc: 0.7896 - val_loss: 0.7235 - val_acc: 0.7848\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6701 - acc: 0.7909 - val_loss: 0.7317 - val_acc: 0.7830\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6675 - acc: 0.7919 - val_loss: 0.7305 - val_acc: 0.7830\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6683 - acc: 0.7934 - val_loss: 0.7273 - val_acc: 0.7830\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6664 - acc: 0.7926 - val_loss: 0.7343 - val_acc: 0.7759\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6683 - acc: 0.7920 - val_loss: 0.7275 - val_acc: 0.7821\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6684 - acc: 0.7897 - val_loss: 0.7231 - val_acc: 0.7750\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6684 - acc: 0.7907 - val_loss: 0.7217 - val_acc: 0.7768\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6654 - acc: 0.7922 - val_loss: 0.7221 - val_acc: 0.7795\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6652 - acc: 0.7920 - val_loss: 0.7212 - val_acc: 0.7830\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6655 - acc: 0.7930 - val_loss: 0.7198 - val_acc: 0.7812\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6663 - acc: 0.7924 - val_loss: 0.7167 - val_acc: 0.7821\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6637 - acc: 0.7915 - val_loss: 0.7184 - val_acc: 0.7893\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6644 - acc: 0.7932 - val_loss: 0.7184 - val_acc: 0.7866\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6633 - acc: 0.7925 - val_loss: 0.7186 - val_acc: 0.7839\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6642 - acc: 0.7939 - val_loss: 0.7176 - val_acc: 0.7857\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6641 - acc: 0.7931 - val_loss: 0.7220 - val_acc: 0.7795\n",
            "Epoch 75/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6625 - acc: 0.7937 - val_loss: 0.7227 - val_acc: 0.7804\n",
            "Epoch 76/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6656 - acc: 0.7932 - val_loss: 0.7200 - val_acc: 0.7750\n",
            "Epoch 77/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6628 - acc: 0.7964 - val_loss: 0.7186 - val_acc: 0.7786\n",
            "Epoch 78/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6619 - acc: 0.7949 - val_loss: 0.7188 - val_acc: 0.7848\n",
            "Epoch 79/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.6624 - acc: 0.7947 - val_loss: 0.7206 - val_acc: 0.7857\n",
            "Performance on the validation set\n",
            "loss: 0.63\n",
            "acc: 0.80\n",
            "\n",
            "FOLD:  6\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_21 (Model)             (None, 34)                577091    \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 385\n",
            "Non-trainable params: 577,091\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 121us/sample - loss: 7.3787 - acc: 0.0625 - val_loss: 5.7094 - val_acc: 0.0714\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 5.3167 - acc: 0.0911 - val_loss: 4.6188 - val_acc: 0.1143\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 4.3008 - acc: 0.1401 - val_loss: 3.6731 - val_acc: 0.1661\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 3.4488 - acc: 0.2012 - val_loss: 2.9190 - val_acc: 0.2509\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 2.7770 - acc: 0.2807 - val_loss: 2.3411 - val_acc: 0.3446\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 2.2653 - acc: 0.3632 - val_loss: 1.9207 - val_acc: 0.4250\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.8906 - acc: 0.4404 - val_loss: 1.6202 - val_acc: 0.5152\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.6273 - acc: 0.5053 - val_loss: 1.4057 - val_acc: 0.5705\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.4340 - acc: 0.5608 - val_loss: 1.2578 - val_acc: 0.6179\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.2981 - acc: 0.5953 - val_loss: 1.1438 - val_acc: 0.6491\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.1930 - acc: 0.6338 - val_loss: 1.0592 - val_acc: 0.6732\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.1090 - acc: 0.6579 - val_loss: 0.9951 - val_acc: 0.6875\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.0435 - acc: 0.6763 - val_loss: 0.9359 - val_acc: 0.6991\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.9919 - acc: 0.6923 - val_loss: 0.8938 - val_acc: 0.7161\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.9468 - acc: 0.7025 - val_loss: 0.8564 - val_acc: 0.7196\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.9128 - acc: 0.7127 - val_loss: 0.8303 - val_acc: 0.7304\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8825 - acc: 0.7223 - val_loss: 0.8064 - val_acc: 0.7348\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8590 - acc: 0.7293 - val_loss: 0.7822 - val_acc: 0.7473\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8346 - acc: 0.7378 - val_loss: 0.7606 - val_acc: 0.7607\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8159 - acc: 0.7429 - val_loss: 0.7455 - val_acc: 0.7607\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8003 - acc: 0.7497 - val_loss: 0.7432 - val_acc: 0.7652\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7888 - acc: 0.7532 - val_loss: 0.7273 - val_acc: 0.7616\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7746 - acc: 0.7547 - val_loss: 0.7138 - val_acc: 0.7723\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7631 - acc: 0.7623 - val_loss: 0.7063 - val_acc: 0.7768\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7554 - acc: 0.7655 - val_loss: 0.6975 - val_acc: 0.7732\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7479 - acc: 0.7659 - val_loss: 0.6930 - val_acc: 0.7759\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7396 - acc: 0.7705 - val_loss: 0.6891 - val_acc: 0.7759\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7338 - acc: 0.7714 - val_loss: 0.6889 - val_acc: 0.7804\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7294 - acc: 0.7726 - val_loss: 0.6860 - val_acc: 0.7768\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7265 - acc: 0.7730 - val_loss: 0.6738 - val_acc: 0.7821\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7178 - acc: 0.7792 - val_loss: 0.6772 - val_acc: 0.7804\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7154 - acc: 0.7773 - val_loss: 0.6682 - val_acc: 0.7759\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7108 - acc: 0.7767 - val_loss: 0.6677 - val_acc: 0.7795\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7088 - acc: 0.7799 - val_loss: 0.6678 - val_acc: 0.7812\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7047 - acc: 0.7807 - val_loss: 0.6706 - val_acc: 0.7786\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 0.7022 - acc: 0.7837 - val_loss: 0.6588 - val_acc: 0.7857\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7010 - acc: 0.7840 - val_loss: 0.6513 - val_acc: 0.7857\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6972 - acc: 0.7842 - val_loss: 0.6543 - val_acc: 0.7830\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6958 - acc: 0.7852 - val_loss: 0.6619 - val_acc: 0.7821\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6930 - acc: 0.7851 - val_loss: 0.6477 - val_acc: 0.7884\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6932 - acc: 0.7846 - val_loss: 0.6524 - val_acc: 0.7884\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6920 - acc: 0.7859 - val_loss: 0.6485 - val_acc: 0.7848\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6886 - acc: 0.7869 - val_loss: 0.6510 - val_acc: 0.7893\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6870 - acc: 0.7877 - val_loss: 0.6482 - val_acc: 0.7902\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6879 - acc: 0.7855 - val_loss: 0.6458 - val_acc: 0.7857\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6842 - acc: 0.7871 - val_loss: 0.6469 - val_acc: 0.7920\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6835 - acc: 0.7875 - val_loss: 0.6429 - val_acc: 0.7911\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6820 - acc: 0.7902 - val_loss: 0.6425 - val_acc: 0.7946\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6836 - acc: 0.7876 - val_loss: 0.6447 - val_acc: 0.7884\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 0.6792 - acc: 0.7901 - val_loss: 0.6449 - val_acc: 0.7884\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6801 - acc: 0.7909 - val_loss: 0.6413 - val_acc: 0.7884\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6784 - acc: 0.7925 - val_loss: 0.6408 - val_acc: 0.7929\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6802 - acc: 0.7884 - val_loss: 0.6389 - val_acc: 0.7929\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6775 - acc: 0.7923 - val_loss: 0.6417 - val_acc: 0.7902\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6756 - acc: 0.7937 - val_loss: 0.6420 - val_acc: 0.7875\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6747 - acc: 0.7932 - val_loss: 0.6361 - val_acc: 0.7920\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6755 - acc: 0.7914 - val_loss: 0.6330 - val_acc: 0.7929\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6748 - acc: 0.7917 - val_loss: 0.6442 - val_acc: 0.7884\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6758 - acc: 0.7922 - val_loss: 0.6360 - val_acc: 0.7929\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6745 - acc: 0.7933 - val_loss: 0.6363 - val_acc: 0.7902\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6731 - acc: 0.7922 - val_loss: 0.6324 - val_acc: 0.7884\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6716 - acc: 0.7942 - val_loss: 0.6392 - val_acc: 0.7866\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6718 - acc: 0.7931 - val_loss: 0.6335 - val_acc: 0.7955\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6744 - acc: 0.7926 - val_loss: 0.6368 - val_acc: 0.7884\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6738 - acc: 0.7921 - val_loss: 0.6307 - val_acc: 0.7937\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6733 - acc: 0.7927 - val_loss: 0.6329 - val_acc: 0.7884\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6701 - acc: 0.7942 - val_loss: 0.6315 - val_acc: 0.7937\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6709 - acc: 0.7944 - val_loss: 0.6416 - val_acc: 0.7830\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6707 - acc: 0.7920 - val_loss: 0.6349 - val_acc: 0.7911\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6702 - acc: 0.7945 - val_loss: 0.6353 - val_acc: 0.7920\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6716 - acc: 0.7925 - val_loss: 0.6322 - val_acc: 0.7929\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6695 - acc: 0.7940 - val_loss: 0.6306 - val_acc: 0.8000\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6732 - acc: 0.7924 - val_loss: 0.6425 - val_acc: 0.7848\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6701 - acc: 0.7957 - val_loss: 0.6307 - val_acc: 0.8000\n",
            "Epoch 75/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.6680 - acc: 0.7945 - val_loss: 0.6299 - val_acc: 0.7902\n",
            "Performance on the validation set\n",
            "loss: 0.63\n",
            "acc: 0.80\n",
            "\n",
            "FOLD:  7\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_21 (Model)             (None, 34)                577091    \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 385\n",
            "Non-trainable params: 577,091\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 124us/sample - loss: 7.3469 - acc: 0.0863 - val_loss: 4.9027 - val_acc: 0.0848\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 4.2029 - acc: 0.1355 - val_loss: 3.8671 - val_acc: 0.1634\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 3.4350 - acc: 0.1965 - val_loss: 3.1605 - val_acc: 0.2250\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 2.8174 - acc: 0.2703 - val_loss: 2.6065 - val_acc: 0.2741\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 2.3486 - acc: 0.3438 - val_loss: 2.1888 - val_acc: 0.3580\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 2.0046 - acc: 0.4131 - val_loss: 1.8810 - val_acc: 0.4402\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.7541 - acc: 0.4759 - val_loss: 1.6621 - val_acc: 0.4955\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.5669 - acc: 0.5265 - val_loss: 1.4894 - val_acc: 0.5250\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 1.4249 - acc: 0.5689 - val_loss: 1.3672 - val_acc: 0.5571\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 1.3116 - acc: 0.5934 - val_loss: 1.2658 - val_acc: 0.5839\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.2262 - acc: 0.6192 - val_loss: 1.1767 - val_acc: 0.6107\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.1480 - acc: 0.6431 - val_loss: 1.1099 - val_acc: 0.6348\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.0863 - acc: 0.6610 - val_loss: 1.0457 - val_acc: 0.6562\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.0366 - acc: 0.6747 - val_loss: 1.0051 - val_acc: 0.6759\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.9890 - acc: 0.6891 - val_loss: 0.9738 - val_acc: 0.6821\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.9535 - acc: 0.7016 - val_loss: 0.9228 - val_acc: 0.7036\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.9208 - acc: 0.7104 - val_loss: 0.9062 - val_acc: 0.7098\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8951 - acc: 0.7186 - val_loss: 0.8672 - val_acc: 0.7188\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8711 - acc: 0.7253 - val_loss: 0.8550 - val_acc: 0.7214\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8520 - acc: 0.7330 - val_loss: 0.8289 - val_acc: 0.7304\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8326 - acc: 0.7371 - val_loss: 0.8126 - val_acc: 0.7357\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8148 - acc: 0.7445 - val_loss: 0.7957 - val_acc: 0.7446\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8030 - acc: 0.7474 - val_loss: 0.7854 - val_acc: 0.7464\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7912 - acc: 0.7516 - val_loss: 0.7770 - val_acc: 0.7571\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7814 - acc: 0.7562 - val_loss: 0.7624 - val_acc: 0.7661\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7711 - acc: 0.7559 - val_loss: 0.7524 - val_acc: 0.7598\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7614 - acc: 0.7557 - val_loss: 0.7464 - val_acc: 0.7723\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7537 - acc: 0.7635 - val_loss: 0.7429 - val_acc: 0.7696\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7480 - acc: 0.7656 - val_loss: 0.7299 - val_acc: 0.7705\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7444 - acc: 0.7653 - val_loss: 0.7214 - val_acc: 0.7705\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7350 - acc: 0.7682 - val_loss: 0.7167 - val_acc: 0.7777\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7325 - acc: 0.7705 - val_loss: 0.7076 - val_acc: 0.7830\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7272 - acc: 0.7711 - val_loss: 0.7073 - val_acc: 0.7804\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7201 - acc: 0.7738 - val_loss: 0.7011 - val_acc: 0.7804\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7176 - acc: 0.7789 - val_loss: 0.6959 - val_acc: 0.7884\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7120 - acc: 0.7764 - val_loss: 0.6895 - val_acc: 0.7875\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7093 - acc: 0.7780 - val_loss: 0.6905 - val_acc: 0.7937\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7091 - acc: 0.7789 - val_loss: 0.6963 - val_acc: 0.7821\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7031 - acc: 0.7811 - val_loss: 0.6866 - val_acc: 0.7884\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7002 - acc: 0.7819 - val_loss: 0.6808 - val_acc: 0.7902\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7006 - acc: 0.7810 - val_loss: 0.6796 - val_acc: 0.7893\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6980 - acc: 0.7821 - val_loss: 0.6862 - val_acc: 0.7839\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6953 - acc: 0.7834 - val_loss: 0.6755 - val_acc: 0.7902\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6921 - acc: 0.7824 - val_loss: 0.6705 - val_acc: 0.8045\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6931 - acc: 0.7839 - val_loss: 0.6722 - val_acc: 0.7875\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6893 - acc: 0.7857 - val_loss: 0.6706 - val_acc: 0.7929\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6892 - acc: 0.7896 - val_loss: 0.6654 - val_acc: 0.7929\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6886 - acc: 0.7857 - val_loss: 0.6704 - val_acc: 0.7902\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6865 - acc: 0.7882 - val_loss: 0.6704 - val_acc: 0.7911\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6835 - acc: 0.7893 - val_loss: 0.6681 - val_acc: 0.7893\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6833 - acc: 0.7857 - val_loss: 0.6637 - val_acc: 0.7955\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6831 - acc: 0.7880 - val_loss: 0.6701 - val_acc: 0.7902\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6830 - acc: 0.7885 - val_loss: 0.6639 - val_acc: 0.7911\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6799 - acc: 0.7875 - val_loss: 0.6544 - val_acc: 0.8071\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6816 - acc: 0.7884 - val_loss: 0.6617 - val_acc: 0.7964\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6802 - acc: 0.7872 - val_loss: 0.6619 - val_acc: 0.7875\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6797 - acc: 0.7892 - val_loss: 0.6597 - val_acc: 0.8062\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6762 - acc: 0.7890 - val_loss: 0.6560 - val_acc: 0.8080\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6777 - acc: 0.7895 - val_loss: 0.6563 - val_acc: 0.7911\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6749 - acc: 0.7920 - val_loss: 0.6567 - val_acc: 0.8009\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6771 - acc: 0.7914 - val_loss: 0.6565 - val_acc: 0.7946\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6738 - acc: 0.7932 - val_loss: 0.6493 - val_acc: 0.8107\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6737 - acc: 0.7924 - val_loss: 0.6523 - val_acc: 0.7982\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6736 - acc: 0.7919 - val_loss: 0.6568 - val_acc: 0.7982\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6726 - acc: 0.7911 - val_loss: 0.6543 - val_acc: 0.7982\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6733 - acc: 0.7912 - val_loss: 0.6586 - val_acc: 0.7884\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6739 - acc: 0.7893 - val_loss: 0.6558 - val_acc: 0.7973\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6718 - acc: 0.7908 - val_loss: 0.6518 - val_acc: 0.7982\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6718 - acc: 0.7938 - val_loss: 0.6611 - val_acc: 0.7937\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6719 - acc: 0.7935 - val_loss: 0.6506 - val_acc: 0.7964\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6720 - acc: 0.7918 - val_loss: 0.6499 - val_acc: 0.8027\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 1s 79us/sample - loss: 0.6698 - acc: 0.7912 - val_loss: 0.6668 - val_acc: 0.7982\n",
            "Performance on the validation set\n",
            "loss: 0.64\n",
            "acc: 0.80\n",
            "\n",
            "FOLD:  8\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_21 (Model)             (None, 34)                577091    \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 385\n",
            "Non-trainable params: 577,091\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 129us/sample - loss: 7.2222 - acc: 0.0850 - val_loss: 4.9921 - val_acc: 0.0920\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 4.4088 - acc: 0.1300 - val_loss: 4.0918 - val_acc: 0.1429\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 3.6071 - acc: 0.1937 - val_loss: 3.3495 - val_acc: 0.2134\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 2.9657 - acc: 0.2698 - val_loss: 2.7626 - val_acc: 0.3098\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 2.4782 - acc: 0.3458 - val_loss: 2.3386 - val_acc: 0.3589\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 2.1197 - acc: 0.4113 - val_loss: 2.0159 - val_acc: 0.4268\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.8535 - acc: 0.4679 - val_loss: 1.7719 - val_acc: 0.4821\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.6496 - acc: 0.5132 - val_loss: 1.5733 - val_acc: 0.5268\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.4908 - acc: 0.5503 - val_loss: 1.4396 - val_acc: 0.5643\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.3688 - acc: 0.5779 - val_loss: 1.3012 - val_acc: 0.6009\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.2685 - acc: 0.6063 - val_loss: 1.2099 - val_acc: 0.6295\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.1867 - acc: 0.6268 - val_loss: 1.1333 - val_acc: 0.6518\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.1219 - acc: 0.6464 - val_loss: 1.0721 - val_acc: 0.6634\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 1.0663 - acc: 0.6641 - val_loss: 1.0101 - val_acc: 0.6696\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.0215 - acc: 0.6788 - val_loss: 0.9659 - val_acc: 0.6893\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.9827 - acc: 0.6908 - val_loss: 0.9281 - val_acc: 0.7054\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.9482 - acc: 0.7033 - val_loss: 0.8978 - val_acc: 0.7125\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.9204 - acc: 0.7095 - val_loss: 0.8667 - val_acc: 0.7223\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8960 - acc: 0.7180 - val_loss: 0.8416 - val_acc: 0.7312\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8733 - acc: 0.7241 - val_loss: 0.8233 - val_acc: 0.7357\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8531 - acc: 0.7309 - val_loss: 0.8080 - val_acc: 0.7348\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8371 - acc: 0.7347 - val_loss: 0.7833 - val_acc: 0.7464\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8223 - acc: 0.7420 - val_loss: 0.7716 - val_acc: 0.7491\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8078 - acc: 0.7476 - val_loss: 0.7581 - val_acc: 0.7607\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7962 - acc: 0.7499 - val_loss: 0.7436 - val_acc: 0.7571\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7867 - acc: 0.7506 - val_loss: 0.7362 - val_acc: 0.7598\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7758 - acc: 0.7567 - val_loss: 0.7222 - val_acc: 0.7714\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7701 - acc: 0.7593 - val_loss: 0.7237 - val_acc: 0.7705\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7608 - acc: 0.7610 - val_loss: 0.7058 - val_acc: 0.7741\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7531 - acc: 0.7634 - val_loss: 0.7024 - val_acc: 0.7795\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7490 - acc: 0.7656 - val_loss: 0.6954 - val_acc: 0.7804\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7414 - acc: 0.7687 - val_loss: 0.6846 - val_acc: 0.7821\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7367 - acc: 0.7715 - val_loss: 0.6807 - val_acc: 0.7804\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7310 - acc: 0.7713 - val_loss: 0.6716 - val_acc: 0.7875\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7271 - acc: 0.7725 - val_loss: 0.6721 - val_acc: 0.7839\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7224 - acc: 0.7743 - val_loss: 0.6742 - val_acc: 0.7893\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7201 - acc: 0.7788 - val_loss: 0.6745 - val_acc: 0.7911\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7161 - acc: 0.7791 - val_loss: 0.6605 - val_acc: 0.7911\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7141 - acc: 0.7775 - val_loss: 0.6623 - val_acc: 0.7911\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7119 - acc: 0.7792 - val_loss: 0.6513 - val_acc: 0.7937\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7081 - acc: 0.7812 - val_loss: 0.6599 - val_acc: 0.7920\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7046 - acc: 0.7836 - val_loss: 0.6578 - val_acc: 0.7955\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7036 - acc: 0.7802 - val_loss: 0.6452 - val_acc: 0.8036\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7022 - acc: 0.7838 - val_loss: 0.6510 - val_acc: 0.8018\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6977 - acc: 0.7853 - val_loss: 0.6441 - val_acc: 0.7946\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6975 - acc: 0.7840 - val_loss: 0.6417 - val_acc: 0.8036\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6959 - acc: 0.7842 - val_loss: 0.6419 - val_acc: 0.7955\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6964 - acc: 0.7840 - val_loss: 0.6418 - val_acc: 0.8009\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6933 - acc: 0.7855 - val_loss: 0.6363 - val_acc: 0.7991\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6894 - acc: 0.7873 - val_loss: 0.6368 - val_acc: 0.8062\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6907 - acc: 0.7856 - val_loss: 0.6309 - val_acc: 0.8062\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6878 - acc: 0.7870 - val_loss: 0.6373 - val_acc: 0.8071\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6887 - acc: 0.7870 - val_loss: 0.6378 - val_acc: 0.8000\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6877 - acc: 0.7870 - val_loss: 0.6366 - val_acc: 0.8054\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6852 - acc: 0.7900 - val_loss: 0.6292 - val_acc: 0.8098\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6849 - acc: 0.7890 - val_loss: 0.6251 - val_acc: 0.8062\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6834 - acc: 0.7879 - val_loss: 0.6294 - val_acc: 0.8062\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6831 - acc: 0.7911 - val_loss: 0.6369 - val_acc: 0.7973\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6845 - acc: 0.7894 - val_loss: 0.6309 - val_acc: 0.7991\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6821 - acc: 0.7907 - val_loss: 0.6272 - val_acc: 0.8018\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6817 - acc: 0.7887 - val_loss: 0.6268 - val_acc: 0.8080\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6819 - acc: 0.7891 - val_loss: 0.6281 - val_acc: 0.8036\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6803 - acc: 0.7905 - val_loss: 0.6229 - val_acc: 0.8152\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6787 - acc: 0.7908 - val_loss: 0.6269 - val_acc: 0.8071\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6789 - acc: 0.7921 - val_loss: 0.6218 - val_acc: 0.8062\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6793 - acc: 0.7913 - val_loss: 0.6340 - val_acc: 0.8018\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6779 - acc: 0.7919 - val_loss: 0.6360 - val_acc: 0.7964\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6778 - acc: 0.7910 - val_loss: 0.6241 - val_acc: 0.8062\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6784 - acc: 0.7905 - val_loss: 0.6262 - val_acc: 0.8080\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6765 - acc: 0.7907 - val_loss: 0.6250 - val_acc: 0.8071\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6765 - acc: 0.7895 - val_loss: 0.6199 - val_acc: 0.8062\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6759 - acc: 0.7893 - val_loss: 0.6270 - val_acc: 0.7991\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6761 - acc: 0.7915 - val_loss: 0.6188 - val_acc: 0.8152\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6751 - acc: 0.7907 - val_loss: 0.6154 - val_acc: 0.8089\n",
            "Epoch 75/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6746 - acc: 0.7937 - val_loss: 0.6244 - val_acc: 0.8071\n",
            "Epoch 76/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6749 - acc: 0.7905 - val_loss: 0.6205 - val_acc: 0.8098\n",
            "Epoch 77/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6739 - acc: 0.7914 - val_loss: 0.6169 - val_acc: 0.8045\n",
            "Epoch 78/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6739 - acc: 0.7922 - val_loss: 0.6140 - val_acc: 0.8098\n",
            "Epoch 79/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6750 - acc: 0.7934 - val_loss: 0.6134 - val_acc: 0.8089\n",
            "Epoch 80/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6731 - acc: 0.7945 - val_loss: 0.6155 - val_acc: 0.8161\n",
            "Epoch 81/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6727 - acc: 0.7921 - val_loss: 0.6174 - val_acc: 0.8062\n",
            "Epoch 82/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6745 - acc: 0.7919 - val_loss: 0.6274 - val_acc: 0.8125\n",
            "Epoch 83/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6737 - acc: 0.7914 - val_loss: 0.6223 - val_acc: 0.8152\n",
            "Epoch 84/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6746 - acc: 0.7936 - val_loss: 0.6195 - val_acc: 0.8089\n",
            "Epoch 85/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6723 - acc: 0.7928 - val_loss: 0.6173 - val_acc: 0.8098\n",
            "Epoch 86/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6755 - acc: 0.7900 - val_loss: 0.6155 - val_acc: 0.8125\n",
            "Epoch 87/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6732 - acc: 0.7920 - val_loss: 0.6203 - val_acc: 0.8054\n",
            "Epoch 88/300\n",
            "10080/10080 [==============================] - 1s 81us/sample - loss: 0.6727 - acc: 0.7933 - val_loss: 0.6188 - val_acc: 0.8170\n",
            "Performance on the validation set\n",
            "loss: 0.63\n",
            "acc: 0.80\n",
            "\n",
            "FOLD:  9\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_21 (Model)             (None, 34)                577091    \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 385\n",
            "Non-trainable params: 577,091\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 127us/sample - loss: 8.3413 - acc: 0.0966 - val_loss: 4.9172 - val_acc: 0.1089\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 4.3989 - acc: 0.1301 - val_loss: 3.8164 - val_acc: 0.1616\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 3.6281 - acc: 0.1853 - val_loss: 3.1917 - val_acc: 0.2330\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 3.0166 - acc: 0.2540 - val_loss: 2.6820 - val_acc: 0.3018\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 2.5251 - acc: 0.3319 - val_loss: 2.2784 - val_acc: 0.3741\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 2.1474 - acc: 0.4031 - val_loss: 1.9794 - val_acc: 0.4357\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.8601 - acc: 0.4654 - val_loss: 1.7621 - val_acc: 0.4964\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.6441 - acc: 0.5180 - val_loss: 1.5914 - val_acc: 0.5384\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.4761 - acc: 0.5593 - val_loss: 1.4469 - val_acc: 0.5750\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.3462 - acc: 0.5894 - val_loss: 1.3460 - val_acc: 0.5911\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.2418 - acc: 0.6162 - val_loss: 1.2660 - val_acc: 0.6098\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.1576 - acc: 0.6384 - val_loss: 1.1930 - val_acc: 0.6277\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.0891 - acc: 0.6607 - val_loss: 1.1445 - val_acc: 0.6411\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.0318 - acc: 0.6754 - val_loss: 1.0906 - val_acc: 0.6634\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.9865 - acc: 0.6909 - val_loss: 1.0443 - val_acc: 0.6786\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.9445 - acc: 0.7020 - val_loss: 1.0059 - val_acc: 0.6804\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.9114 - acc: 0.7124 - val_loss: 0.9760 - val_acc: 0.6875\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8842 - acc: 0.7235 - val_loss: 0.9493 - val_acc: 0.7009\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.8582 - acc: 0.7350 - val_loss: 0.9228 - val_acc: 0.7054\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8388 - acc: 0.7401 - val_loss: 0.9011 - val_acc: 0.7071\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8192 - acc: 0.7415 - val_loss: 0.8859 - val_acc: 0.7170\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8028 - acc: 0.7508 - val_loss: 0.8726 - val_acc: 0.7179\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7890 - acc: 0.7544 - val_loss: 0.8609 - val_acc: 0.7312\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7765 - acc: 0.7590 - val_loss: 0.8464 - val_acc: 0.7339\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7658 - acc: 0.7637 - val_loss: 0.8298 - val_acc: 0.7312\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.7563 - acc: 0.7641 - val_loss: 0.8205 - val_acc: 0.7286\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7477 - acc: 0.7680 - val_loss: 0.8104 - val_acc: 0.7330\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7414 - acc: 0.7713 - val_loss: 0.8128 - val_acc: 0.7429\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7330 - acc: 0.7722 - val_loss: 0.8004 - val_acc: 0.7420\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7257 - acc: 0.7757 - val_loss: 0.7921 - val_acc: 0.7402\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7212 - acc: 0.7752 - val_loss: 0.7920 - val_acc: 0.7473\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7168 - acc: 0.7763 - val_loss: 0.7817 - val_acc: 0.7437\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7115 - acc: 0.7774 - val_loss: 0.7759 - val_acc: 0.7491\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7086 - acc: 0.7798 - val_loss: 0.7680 - val_acc: 0.7455\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7027 - acc: 0.7801 - val_loss: 0.7711 - val_acc: 0.7455\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7023 - acc: 0.7821 - val_loss: 0.7649 - val_acc: 0.7455\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6973 - acc: 0.7842 - val_loss: 0.7571 - val_acc: 0.7473\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6939 - acc: 0.7818 - val_loss: 0.7572 - val_acc: 0.7473\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6927 - acc: 0.7859 - val_loss: 0.7616 - val_acc: 0.7518\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6897 - acc: 0.7853 - val_loss: 0.7567 - val_acc: 0.7545\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6879 - acc: 0.7875 - val_loss: 0.7503 - val_acc: 0.7554\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6837 - acc: 0.7873 - val_loss: 0.7540 - val_acc: 0.7518\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6833 - acc: 0.7866 - val_loss: 0.7461 - val_acc: 0.7491\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6827 - acc: 0.7865 - val_loss: 0.7407 - val_acc: 0.7598\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6788 - acc: 0.7903 - val_loss: 0.7428 - val_acc: 0.7518\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6821 - acc: 0.7853 - val_loss: 0.7386 - val_acc: 0.7580\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6775 - acc: 0.7872 - val_loss: 0.7357 - val_acc: 0.7571\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6760 - acc: 0.7899 - val_loss: 0.7420 - val_acc: 0.7545\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6760 - acc: 0.7885 - val_loss: 0.7400 - val_acc: 0.7589\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6719 - acc: 0.7936 - val_loss: 0.7426 - val_acc: 0.7616\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6737 - acc: 0.7910 - val_loss: 0.7344 - val_acc: 0.7598\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6716 - acc: 0.7921 - val_loss: 0.7348 - val_acc: 0.7545\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6711 - acc: 0.7902 - val_loss: 0.7294 - val_acc: 0.7536\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6705 - acc: 0.7927 - val_loss: 0.7311 - val_acc: 0.7571\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6698 - acc: 0.7927 - val_loss: 0.7319 - val_acc: 0.7616\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6686 - acc: 0.7916 - val_loss: 0.7312 - val_acc: 0.7500\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6696 - acc: 0.7913 - val_loss: 0.7298 - val_acc: 0.7571\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6690 - acc: 0.7927 - val_loss: 0.7289 - val_acc: 0.7563\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6670 - acc: 0.7954 - val_loss: 0.7315 - val_acc: 0.7607\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6666 - acc: 0.7937 - val_loss: 0.7230 - val_acc: 0.7607\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6652 - acc: 0.7939 - val_loss: 0.7344 - val_acc: 0.7589\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6668 - acc: 0.7922 - val_loss: 0.7291 - val_acc: 0.7554\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6651 - acc: 0.7938 - val_loss: 0.7245 - val_acc: 0.7616\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6631 - acc: 0.7938 - val_loss: 0.7250 - val_acc: 0.7670\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6637 - acc: 0.7955 - val_loss: 0.7307 - val_acc: 0.7616\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6640 - acc: 0.7956 - val_loss: 0.7316 - val_acc: 0.7696\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6626 - acc: 0.7970 - val_loss: 0.7241 - val_acc: 0.7598\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6620 - acc: 0.7982 - val_loss: 0.7252 - val_acc: 0.7634\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6621 - acc: 0.7955 - val_loss: 0.7207 - val_acc: 0.7643\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6616 - acc: 0.7973 - val_loss: 0.7196 - val_acc: 0.7643\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6604 - acc: 0.7963 - val_loss: 0.7177 - val_acc: 0.7652\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6597 - acc: 0.7942 - val_loss: 0.7199 - val_acc: 0.7661\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6623 - acc: 0.7951 - val_loss: 0.7243 - val_acc: 0.7580\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6616 - acc: 0.7972 - val_loss: 0.7203 - val_acc: 0.7634\n",
            "Epoch 75/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6590 - acc: 0.7997 - val_loss: 0.7242 - val_acc: 0.7661\n",
            "Epoch 76/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6604 - acc: 0.7958 - val_loss: 0.7278 - val_acc: 0.7616\n",
            "Epoch 77/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6596 - acc: 0.7967 - val_loss: 0.7186 - val_acc: 0.7670\n",
            "Epoch 78/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6609 - acc: 0.7957 - val_loss: 0.7193 - val_acc: 0.7679\n",
            "Epoch 79/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6605 - acc: 0.7961 - val_loss: 0.7340 - val_acc: 0.7580\n",
            "Epoch 80/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6609 - acc: 0.7953 - val_loss: 0.7214 - val_acc: 0.7661\n",
            "Epoch 81/300\n",
            "10080/10080 [==============================] - 1s 84us/sample - loss: 0.6576 - acc: 0.7968 - val_loss: 0.7187 - val_acc: 0.7643\n",
            "Performance on the validation set\n",
            "loss: 0.62\n",
            "acc: 0.80\n",
            "\n",
            "FOLD:  10\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_21 (Model)             (None, 34)                577091    \n",
            "_________________________________________________________________\n",
            "dense_78 (Dense)             (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 385\n",
            "Non-trainable params: 577,091\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 134us/sample - loss: 7.5142 - acc: 0.1394 - val_loss: 4.8968 - val_acc: 0.1795\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 4.2612 - acc: 0.1934 - val_loss: 3.7803 - val_acc: 0.2277\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 3.4932 - acc: 0.2416 - val_loss: 3.1756 - val_acc: 0.2777\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 2.9292 - acc: 0.3006 - val_loss: 2.6517 - val_acc: 0.3482\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 2.4693 - acc: 0.3691 - val_loss: 2.2483 - val_acc: 0.4232\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 2.1124 - acc: 0.4320 - val_loss: 1.9378 - val_acc: 0.4714\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 1.8360 - acc: 0.4899 - val_loss: 1.7059 - val_acc: 0.5063\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 1.6220 - acc: 0.5353 - val_loss: 1.5146 - val_acc: 0.5527\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.4543 - acc: 0.5678 - val_loss: 1.3636 - val_acc: 0.5839\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.3204 - acc: 0.6015 - val_loss: 1.2372 - val_acc: 0.6152\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.2121 - acc: 0.6275 - val_loss: 1.1428 - val_acc: 0.6473\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 1.1240 - acc: 0.6499 - val_loss: 1.0680 - val_acc: 0.6679\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.0523 - acc: 0.6718 - val_loss: 1.0044 - val_acc: 0.6875\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.9961 - acc: 0.6868 - val_loss: 0.9617 - val_acc: 0.7125\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.9486 - acc: 0.7038 - val_loss: 0.9114 - val_acc: 0.7223\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.9105 - acc: 0.7136 - val_loss: 0.8802 - val_acc: 0.7304\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8780 - acc: 0.7242 - val_loss: 0.8569 - val_acc: 0.7411\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8531 - acc: 0.7355 - val_loss: 0.8351 - val_acc: 0.7446\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.8319 - acc: 0.7408 - val_loss: 0.8200 - val_acc: 0.7518\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8133 - acc: 0.7479 - val_loss: 0.8064 - val_acc: 0.7491\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7954 - acc: 0.7525 - val_loss: 0.7908 - val_acc: 0.7589\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7844 - acc: 0.7570 - val_loss: 0.7791 - val_acc: 0.7652\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7691 - acc: 0.7615 - val_loss: 0.7666 - val_acc: 0.7607\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7606 - acc: 0.7615 - val_loss: 0.7617 - val_acc: 0.7625\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7533 - acc: 0.7636 - val_loss: 0.7558 - val_acc: 0.7625\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7441 - acc: 0.7689 - val_loss: 0.7536 - val_acc: 0.7661\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7377 - acc: 0.7705 - val_loss: 0.7454 - val_acc: 0.7705\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7318 - acc: 0.7717 - val_loss: 0.7484 - val_acc: 0.7741\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7271 - acc: 0.7751 - val_loss: 0.7437 - val_acc: 0.7714\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7221 - acc: 0.7749 - val_loss: 0.7325 - val_acc: 0.7705\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7169 - acc: 0.7759 - val_loss: 0.7379 - val_acc: 0.7696\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7136 - acc: 0.7762 - val_loss: 0.7266 - val_acc: 0.7750\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7090 - acc: 0.7778 - val_loss: 0.7239 - val_acc: 0.7768\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.7063 - acc: 0.7792 - val_loss: 0.7234 - val_acc: 0.7714\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7023 - acc: 0.7809 - val_loss: 0.7233 - val_acc: 0.7741\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7007 - acc: 0.7818 - val_loss: 0.7223 - val_acc: 0.7750\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.7017 - acc: 0.7789 - val_loss: 0.7246 - val_acc: 0.7732\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6952 - acc: 0.7823 - val_loss: 0.7215 - val_acc: 0.7714\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6921 - acc: 0.7823 - val_loss: 0.7127 - val_acc: 0.7750\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6906 - acc: 0.7836 - val_loss: 0.7197 - val_acc: 0.7768\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6911 - acc: 0.7841 - val_loss: 0.7136 - val_acc: 0.7732\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6872 - acc: 0.7861 - val_loss: 0.7107 - val_acc: 0.7741\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6852 - acc: 0.7863 - val_loss: 0.7082 - val_acc: 0.7768\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6860 - acc: 0.7865 - val_loss: 0.7076 - val_acc: 0.7741\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6855 - acc: 0.7850 - val_loss: 0.7148 - val_acc: 0.7821\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6824 - acc: 0.7892 - val_loss: 0.7068 - val_acc: 0.7750\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6787 - acc: 0.7858 - val_loss: 0.7115 - val_acc: 0.7750\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6790 - acc: 0.7899 - val_loss: 0.7126 - val_acc: 0.7759\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6787 - acc: 0.7879 - val_loss: 0.7119 - val_acc: 0.7812\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6775 - acc: 0.7904 - val_loss: 0.7045 - val_acc: 0.7768\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6752 - acc: 0.7904 - val_loss: 0.7081 - val_acc: 0.7777\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6778 - acc: 0.7885 - val_loss: 0.7064 - val_acc: 0.7821\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6740 - acc: 0.7920 - val_loss: 0.7139 - val_acc: 0.7795\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6735 - acc: 0.7912 - val_loss: 0.7039 - val_acc: 0.7812\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6732 - acc: 0.7918 - val_loss: 0.7086 - val_acc: 0.7777\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6727 - acc: 0.7889 - val_loss: 0.7093 - val_acc: 0.7830\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6729 - acc: 0.7900 - val_loss: 0.7031 - val_acc: 0.7839\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6712 - acc: 0.7909 - val_loss: 0.6987 - val_acc: 0.7777\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6729 - acc: 0.7941 - val_loss: 0.7057 - val_acc: 0.7777\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6710 - acc: 0.7904 - val_loss: 0.7093 - val_acc: 0.7812\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 28us/sample - loss: 0.6715 - acc: 0.7903 - val_loss: 0.7072 - val_acc: 0.7786\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6698 - acc: 0.7894 - val_loss: 0.7087 - val_acc: 0.7786\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6703 - acc: 0.7927 - val_loss: 0.7080 - val_acc: 0.7750\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 29us/sample - loss: 0.6694 - acc: 0.7926 - val_loss: 0.7034 - val_acc: 0.7839\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6675 - acc: 0.7940 - val_loss: 0.7049 - val_acc: 0.7830\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6668 - acc: 0.7940 - val_loss: 0.6980 - val_acc: 0.7795\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 30us/sample - loss: 0.6664 - acc: 0.7928 - val_loss: 0.7035 - val_acc: 0.7920\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 1s 84us/sample - loss: 0.6664 - acc: 0.7930 - val_loss: 0.7128 - val_acc: 0.7804\n",
            "Performance on the validation set\n",
            "loss: 0.63\n",
            "acc: 0.80\n",
            "\n",
            "Overall  loss :  0.63 +- 0.00\n",
            "Overall  acc :  0.80 +- 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLM2b-ozNCki",
        "colab_type": "code",
        "outputId": "89c569c7-3f12-4d6d-c89a-e4f353ea2a47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "x_plot = list(range(1, len(best_history.history['val_acc']) + 1))\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(x_plot, history.history['loss'])\n",
        "    plt.plot(x_plot, history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.plot(x_plot, history.history['acc'])\n",
        "    plt.plot(x_plot, history.history['val_acc'])\n",
        "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_history(best_history)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxvg5SwTGGOPnLBEYY4yf+/989sI+nTneRAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7fd-_loairL",
        "colab_type": "code",
        "outputId": "f77923df-249b-4768-fe1a-841691ea5742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(labels_val, best_model.predict_classes(x_val)))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       273\n",
            "           1       0.82      0.75      0.78       247\n",
            "           2       0.77      0.77      0.77       292\n",
            "           3       0.84      0.89      0.87       251\n",
            "           4       0.65      0.72      0.68       262\n",
            "           5       0.80      0.81      0.81       239\n",
            "           6       0.78      0.77      0.78       261\n",
            "           7       0.90      0.88      0.89       269\n",
            "           8       0.87      0.82      0.84       266\n",
            "           9       0.78      0.77      0.77       279\n",
            "          10       0.82      0.83      0.82       161\n",
            "\n",
            "    accuracy                           0.80      2800\n",
            "   macro avg       0.81      0.81      0.81      2800\n",
            "weighted avg       0.81      0.80      0.80      2800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxbMs2I6dcr0",
        "colab_type": "text"
      },
      "source": [
        "# Autoencoder tied with the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWd6qMK5dlEY",
        "colab_type": "code",
        "outputId": "66559678-bfcc-4b05-9d14-a1e94de90d34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "monitor = 'val_loss'\n",
        "early_stopping = EarlyStopping(monitor=monitor, patience=10, min_delta=0.001, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=10, verbose=1)\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "model_type = 'autoencoder_nn'\n",
        "if model_type == 'autoencoder_nn':\n",
        "  labels_train = [y_train, x_train]\n",
        "  labels_val = [y_val, x_val]\n",
        "elif model_type == 'nn':\n",
        "  labels_train = y_train\n",
        "  labels_val = y_val\n",
        "elif model_type == 'autoencoder':\n",
        "  labels_train = x_train\n",
        "  labels_val = x_val\n",
        "\n",
        "# define 10-fold cross validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "scores_val = []\n",
        "best_score = 0  # based on accuracy\n",
        "best_model = None\n",
        "best_history = None\n",
        "\n",
        "for idx, (train, val) in enumerate(kfold.split(x_train, labels_train[0])):\n",
        "  print('FOLD: ', idx + 1)\n",
        "  # create model\n",
        "  autoencoder_nn, _ = get_model(\n",
        "      model_type=model_type, \n",
        "      divide_by=[1.5, 2, 2.5, 3], \n",
        "      hidden_first=512, \n",
        "      hidden_last=32, \n",
        "      dropout=True, \n",
        "      batch_norm=False, \n",
        "      standard=False, \n",
        "      verbose=True\n",
        "  )\n",
        "  # Fit the model\n",
        "  history = autoencoder_nn.fit(\n",
        "      x_train[train], \n",
        "      [labels_train[0][train], labels_train[1][train]], \n",
        "      validation_data=(x_train[val], [labels_train[0][val], labels_train[1][val]]), \n",
        "      epochs=300, \n",
        "      batch_size=128, \n",
        "      callbacks=callbacks\n",
        "  )\n",
        "  # evaluate the model\n",
        "  score = autoencoder_nn.evaluate(x_val, labels_val, verbose=0)\n",
        "  # save best_model\n",
        "  if score[3] > best_score:  # classifier accuracy\n",
        "    best_score = score[3]\n",
        "    best_model = autoencoder_nn\n",
        "    best_history = history\n",
        "  print('Performance on the validation set')\n",
        "  for idx in range(len(autoencoder_nn.metrics_names)):\n",
        "    print(\"%s: %.2f\" % (autoencoder_nn.metrics_names[idx], score[idx]))\n",
        "  print()\n",
        "  scores_val.append(score)\n",
        "  \n",
        "scores_val = np.array(scores_val)\n",
        "overall_means = np.mean(scores_val, axis=0)\n",
        "overall_stds = np.std(scores_val, axis=0)\n",
        "for idx in range(len(overall_means)):\n",
        "  print('Overall ', autoencoder_nn.metrics_names[idx], ': ', '{:.2f}'.format(overall_means[idx]), '+-', '{:.2f}'.format(overall_stds[idx]))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD:  1\n",
            "Model: \"model_37\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_25 (InputLayer)           [(None, 784)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 784)          0           input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_136 (Dense)               (None, 522)          409770      dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_137 (Dense)               (None, 261)          136503      dense_136[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_138 (Dense)               (None, 104)          27248       dense_137[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_139 (Dense)               (None, 34)           3570        dense_138[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_140 (Dense)               (None, 104)          3640        dense_139[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_141 (Dense)               (None, 261)          27405       dense_140[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_142 (Dense)               (None, 522)          136764      dense_141[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           385         dense_139[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          410032      dense_142[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,317\n",
            "Trainable params: 1,155,317\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 261us/sample - loss: 1.1139 - classifier_loss: 1.0266 - decoder_loss: 0.0863 - classifier_acc: 0.6730 - decoder_mean_squared_error: 0.0863 - val_loss: 0.6081 - val_classifier_loss: 0.5483 - val_decoder_loss: 0.0622 - val_classifier_acc: 0.8411 - val_decoder_mean_squared_error: 0.0621\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.5445 - classifier_loss: 0.4864 - decoder_loss: 0.0578 - classifier_acc: 0.8487 - decoder_mean_squared_error: 0.0579 - val_loss: 0.4399 - val_classifier_loss: 0.3847 - val_decoder_loss: 0.0524 - val_classifier_acc: 0.8830 - val_decoder_mean_squared_error: 0.0523\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 71us/sample - loss: 0.3941 - classifier_loss: 0.3423 - decoder_loss: 0.0516 - classifier_acc: 0.8942 - decoder_mean_squared_error: 0.0516 - val_loss: 0.3685 - val_classifier_loss: 0.3259 - val_decoder_loss: 0.0458 - val_classifier_acc: 0.9062 - val_decoder_mean_squared_error: 0.0458\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.3231 - classifier_loss: 0.2752 - decoder_loss: 0.0480 - classifier_acc: 0.9177 - decoder_mean_squared_error: 0.0480 - val_loss: 0.3515 - val_classifier_loss: 0.3089 - val_decoder_loss: 0.0461 - val_classifier_acc: 0.9098 - val_decoder_mean_squared_error: 0.0461\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.2698 - classifier_loss: 0.2243 - decoder_loss: 0.0454 - classifier_acc: 0.9290 - decoder_mean_squared_error: 0.0454 - val_loss: 0.3265 - val_classifier_loss: 0.2903 - val_decoder_loss: 0.0420 - val_classifier_acc: 0.9205 - val_decoder_mean_squared_error: 0.0419\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.2325 - classifier_loss: 0.1893 - decoder_loss: 0.0433 - classifier_acc: 0.9397 - decoder_mean_squared_error: 0.0433 - val_loss: 0.3024 - val_classifier_loss: 0.2632 - val_decoder_loss: 0.0408 - val_classifier_acc: 0.9214 - val_decoder_mean_squared_error: 0.0407\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 71us/sample - loss: 0.2121 - classifier_loss: 0.1700 - decoder_loss: 0.0425 - classifier_acc: 0.9457 - decoder_mean_squared_error: 0.0425 - val_loss: 0.2958 - val_classifier_loss: 0.2533 - val_decoder_loss: 0.0412 - val_classifier_acc: 0.9259 - val_decoder_mean_squared_error: 0.0413\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.1996 - classifier_loss: 0.1578 - decoder_loss: 0.0417 - classifier_acc: 0.9489 - decoder_mean_squared_error: 0.0417 - val_loss: 0.3040 - val_classifier_loss: 0.2639 - val_decoder_loss: 0.0393 - val_classifier_acc: 0.9277 - val_decoder_mean_squared_error: 0.0393\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.1695 - classifier_loss: 0.1297 - decoder_loss: 0.0398 - classifier_acc: 0.9581 - decoder_mean_squared_error: 0.0398 - val_loss: 0.2999 - val_classifier_loss: 0.2576 - val_decoder_loss: 0.0373 - val_classifier_acc: 0.9295 - val_decoder_mean_squared_error: 0.0374\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1626 - classifier_loss: 0.1235 - decoder_loss: 0.0392 - classifier_acc: 0.9587 - decoder_mean_squared_error: 0.0392 - val_loss: 0.3182 - val_classifier_loss: 0.2813 - val_decoder_loss: 0.0380 - val_classifier_acc: 0.9187 - val_decoder_mean_squared_error: 0.0379\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1499 - classifier_loss: 0.1124 - decoder_loss: 0.0378 - classifier_acc: 0.9633 - decoder_mean_squared_error: 0.0378 - val_loss: 0.2902 - val_classifier_loss: 0.2496 - val_decoder_loss: 0.0359 - val_classifier_acc: 0.9411 - val_decoder_mean_squared_error: 0.0359\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1363 - classifier_loss: 0.0992 - decoder_loss: 0.0373 - classifier_acc: 0.9667 - decoder_mean_squared_error: 0.0373 - val_loss: 0.2841 - val_classifier_loss: 0.2510 - val_decoder_loss: 0.0357 - val_classifier_acc: 0.9366 - val_decoder_mean_squared_error: 0.0357\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1301 - classifier_loss: 0.0938 - decoder_loss: 0.0363 - classifier_acc: 0.9681 - decoder_mean_squared_error: 0.0362 - val_loss: 0.2920 - val_classifier_loss: 0.2603 - val_decoder_loss: 0.0348 - val_classifier_acc: 0.9304 - val_decoder_mean_squared_error: 0.0348\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1219 - classifier_loss: 0.0861 - decoder_loss: 0.0357 - classifier_acc: 0.9719 - decoder_mean_squared_error: 0.0357 - val_loss: 0.2862 - val_classifier_loss: 0.2522 - val_decoder_loss: 0.0346 - val_classifier_acc: 0.9402 - val_decoder_mean_squared_error: 0.0345\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.1164 - classifier_loss: 0.0809 - decoder_loss: 0.0353 - classifier_acc: 0.9709 - decoder_mean_squared_error: 0.0353 - val_loss: 0.2914 - val_classifier_loss: 0.2586 - val_decoder_loss: 0.0330 - val_classifier_acc: 0.9339 - val_decoder_mean_squared_error: 0.0329\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 71us/sample - loss: 0.1013 - classifier_loss: 0.0673 - decoder_loss: 0.0340 - classifier_acc: 0.9766 - decoder_mean_squared_error: 0.0340 - val_loss: 0.3016 - val_classifier_loss: 0.2701 - val_decoder_loss: 0.0330 - val_classifier_acc: 0.9393 - val_decoder_mean_squared_error: 0.0330\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 71us/sample - loss: 0.1015 - classifier_loss: 0.0679 - decoder_loss: 0.0335 - classifier_acc: 0.9773 - decoder_mean_squared_error: 0.0335 - val_loss: 0.3233 - val_classifier_loss: 0.2897 - val_decoder_loss: 0.0330 - val_classifier_acc: 0.9330 - val_decoder_mean_squared_error: 0.0329\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 70us/sample - loss: 0.0925 - classifier_loss: 0.0602 - decoder_loss: 0.0327 - classifier_acc: 0.9805 - decoder_mean_squared_error: 0.0327 - val_loss: 0.3025 - val_classifier_loss: 0.2723 - val_decoder_loss: 0.0308 - val_classifier_acc: 0.9330 - val_decoder_mean_squared_error: 0.0308\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.0876 - classifier_loss: 0.0555 - decoder_loss: 0.0322 - classifier_acc: 0.9812 - decoder_mean_squared_error: 0.0322 - val_loss: 0.3052 - val_classifier_loss: 0.2824 - val_decoder_loss: 0.0316 - val_classifier_acc: 0.9429 - val_decoder_mean_squared_error: 0.0315\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.0875 - classifier_loss: 0.0555 - decoder_loss: 0.0319 - classifier_acc: 0.9791 - decoder_mean_squared_error: 0.0319 - val_loss: 0.3015 - val_classifier_loss: 0.2762 - val_decoder_loss: 0.0303 - val_classifier_acc: 0.9402 - val_decoder_mean_squared_error: 0.0302\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.0855 - classifier_loss: 0.0539 - decoder_loss: 0.0318 - classifier_acc: 0.9811 - decoder_mean_squared_error: 0.0318 - val_loss: 0.3190 - val_classifier_loss: 0.2894 - val_decoder_loss: 0.0308 - val_classifier_acc: 0.9348 - val_decoder_mean_squared_error: 0.0309\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 2s 174us/sample - loss: 0.1000 - classifier_loss: 0.0673 - decoder_loss: 0.0326 - classifier_acc: 0.9762 - decoder_mean_squared_error: 0.0326 - val_loss: 0.3044 - val_classifier_loss: 0.2749 - val_decoder_loss: 0.0309 - val_classifier_acc: 0.9384 - val_decoder_mean_squared_error: 0.0309\n",
            "Performance on the validation set\n",
            "loss: 0.25\n",
            "classifier_loss: 0.22\n",
            "decoder_loss: 0.04\n",
            "classifier_acc: 0.94\n",
            "decoder_mean_squared_error: 0.04\n",
            "\n",
            "FOLD:  2\n",
            "Model: \"model_39\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           [(None, 784)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 784)          0           input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_143 (Dense)               (None, 522)          409770      dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_144 (Dense)               (None, 261)          136503      dense_143[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_145 (Dense)               (None, 104)          27248       dense_144[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_146 (Dense)               (None, 34)           3570        dense_145[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_147 (Dense)               (None, 104)          3640        dense_146[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_148 (Dense)               (None, 261)          27405       dense_147[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_149 (Dense)               (None, 522)          136764      dense_148[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           385         dense_146[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          410032      dense_149[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,317\n",
            "Trainable params: 1,155,317\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 277us/sample - loss: 1.1504 - classifier_loss: 1.0594 - decoder_loss: 0.0901 - classifier_acc: 0.6576 - decoder_mean_squared_error: 0.0902 - val_loss: 0.6514 - val_classifier_loss: 0.5889 - val_decoder_loss: 0.0655 - val_classifier_acc: 0.8196 - val_decoder_mean_squared_error: 0.0654\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.5489 - classifier_loss: 0.4895 - decoder_loss: 0.0593 - classifier_acc: 0.8501 - decoder_mean_squared_error: 0.0593 - val_loss: 0.4595 - val_classifier_loss: 0.4068 - val_decoder_loss: 0.0513 - val_classifier_acc: 0.8723 - val_decoder_mean_squared_error: 0.0513\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.3999 - classifier_loss: 0.3492 - decoder_loss: 0.0506 - classifier_acc: 0.8932 - decoder_mean_squared_error: 0.0506 - val_loss: 0.3527 - val_classifier_loss: 0.3041 - val_decoder_loss: 0.0468 - val_classifier_acc: 0.9071 - val_decoder_mean_squared_error: 0.0468\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.3224 - classifier_loss: 0.2757 - decoder_loss: 0.0468 - classifier_acc: 0.9152 - decoder_mean_squared_error: 0.0468 - val_loss: 0.3285 - val_classifier_loss: 0.2846 - val_decoder_loss: 0.0426 - val_classifier_acc: 0.9187 - val_decoder_mean_squared_error: 0.0426\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.2779 - classifier_loss: 0.2338 - decoder_loss: 0.0442 - classifier_acc: 0.9269 - decoder_mean_squared_error: 0.0442 - val_loss: 0.3066 - val_classifier_loss: 0.2601 - val_decoder_loss: 0.0433 - val_classifier_acc: 0.9295 - val_decoder_mean_squared_error: 0.0433\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.2373 - classifier_loss: 0.1942 - decoder_loss: 0.0428 - classifier_acc: 0.9377 - decoder_mean_squared_error: 0.0428 - val_loss: 0.2872 - val_classifier_loss: 0.2505 - val_decoder_loss: 0.0399 - val_classifier_acc: 0.9286 - val_decoder_mean_squared_error: 0.0398\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 71us/sample - loss: 0.2214 - classifier_loss: 0.1802 - decoder_loss: 0.0409 - classifier_acc: 0.9400 - decoder_mean_squared_error: 0.0409 - val_loss: 0.2658 - val_classifier_loss: 0.2275 - val_decoder_loss: 0.0393 - val_classifier_acc: 0.9312 - val_decoder_mean_squared_error: 0.0392\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1884 - classifier_loss: 0.1484 - decoder_loss: 0.0397 - classifier_acc: 0.9542 - decoder_mean_squared_error: 0.0397 - val_loss: 0.2774 - val_classifier_loss: 0.2410 - val_decoder_loss: 0.0366 - val_classifier_acc: 0.9366 - val_decoder_mean_squared_error: 0.0367\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.1678 - classifier_loss: 0.1298 - decoder_loss: 0.0381 - classifier_acc: 0.9571 - decoder_mean_squared_error: 0.0381 - val_loss: 0.2833 - val_classifier_loss: 0.2426 - val_decoder_loss: 0.0366 - val_classifier_acc: 0.9330 - val_decoder_mean_squared_error: 0.0366\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1628 - classifier_loss: 0.1250 - decoder_loss: 0.0377 - classifier_acc: 0.9569 - decoder_mean_squared_error: 0.0377 - val_loss: 0.2915 - val_classifier_loss: 0.2610 - val_decoder_loss: 0.0369 - val_classifier_acc: 0.9339 - val_decoder_mean_squared_error: 0.0369\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1479 - classifier_loss: 0.1109 - decoder_loss: 0.0369 - classifier_acc: 0.9624 - decoder_mean_squared_error: 0.0369 - val_loss: 0.2571 - val_classifier_loss: 0.2225 - val_decoder_loss: 0.0346 - val_classifier_acc: 0.9420 - val_decoder_mean_squared_error: 0.0346\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.1406 - classifier_loss: 0.1044 - decoder_loss: 0.0362 - classifier_acc: 0.9673 - decoder_mean_squared_error: 0.0362 - val_loss: 0.2436 - val_classifier_loss: 0.2049 - val_decoder_loss: 0.0352 - val_classifier_acc: 0.9464 - val_decoder_mean_squared_error: 0.0352\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1196 - classifier_loss: 0.0849 - decoder_loss: 0.0348 - classifier_acc: 0.9716 - decoder_mean_squared_error: 0.0348 - val_loss: 0.2657 - val_classifier_loss: 0.2285 - val_decoder_loss: 0.0332 - val_classifier_acc: 0.9357 - val_decoder_mean_squared_error: 0.0332\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1274 - classifier_loss: 0.0924 - decoder_loss: 0.0350 - classifier_acc: 0.9682 - decoder_mean_squared_error: 0.0350 - val_loss: 0.3099 - val_classifier_loss: 0.2768 - val_decoder_loss: 0.0349 - val_classifier_acc: 0.9268 - val_decoder_mean_squared_error: 0.0349\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 70us/sample - loss: 0.1228 - classifier_loss: 0.0884 - decoder_loss: 0.0343 - classifier_acc: 0.9688 - decoder_mean_squared_error: 0.0343 - val_loss: 0.2676 - val_classifier_loss: 0.2334 - val_decoder_loss: 0.0327 - val_classifier_acc: 0.9446 - val_decoder_mean_squared_error: 0.0327\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1109 - classifier_loss: 0.0775 - decoder_loss: 0.0333 - classifier_acc: 0.9720 - decoder_mean_squared_error: 0.0333 - val_loss: 0.2606 - val_classifier_loss: 0.2253 - val_decoder_loss: 0.0327 - val_classifier_acc: 0.9393 - val_decoder_mean_squared_error: 0.0327\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1124 - classifier_loss: 0.0794 - decoder_loss: 0.0329 - classifier_acc: 0.9722 - decoder_mean_squared_error: 0.0329 - val_loss: 0.2534 - val_classifier_loss: 0.2196 - val_decoder_loss: 0.0326 - val_classifier_acc: 0.9438 - val_decoder_mean_squared_error: 0.0326\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1043 - classifier_loss: 0.0710 - decoder_loss: 0.0332 - classifier_acc: 0.9754 - decoder_mean_squared_error: 0.0332 - val_loss: 0.2588 - val_classifier_loss: 0.2243 - val_decoder_loss: 0.0324 - val_classifier_acc: 0.9473 - val_decoder_mean_squared_error: 0.0324\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.0938 - classifier_loss: 0.0618 - decoder_loss: 0.0321 - classifier_acc: 0.9796 - decoder_mean_squared_error: 0.0321 - val_loss: 0.2879 - val_classifier_loss: 0.2535 - val_decoder_loss: 0.0312 - val_classifier_acc: 0.9438 - val_decoder_mean_squared_error: 0.0311\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.0855 - classifier_loss: 0.0540 - decoder_loss: 0.0314 - classifier_acc: 0.9811 - decoder_mean_squared_error: 0.0314 - val_loss: 0.2628 - val_classifier_loss: 0.2366 - val_decoder_loss: 0.0304 - val_classifier_acc: 0.9491 - val_decoder_mean_squared_error: 0.0303\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.0983 - classifier_loss: 0.0669 - decoder_loss: 0.0313 - classifier_acc: 0.9757 - decoder_mean_squared_error: 0.0313 - val_loss: 0.2540 - val_classifier_loss: 0.2235 - val_decoder_loss: 0.0314 - val_classifier_acc: 0.9438 - val_decoder_mean_squared_error: 0.0314\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 2s 180us/sample - loss: 0.0894 - classifier_loss: 0.0581 - decoder_loss: 0.0311 - classifier_acc: 0.9785 - decoder_mean_squared_error: 0.0311 - val_loss: 0.2532 - val_classifier_loss: 0.2277 - val_decoder_loss: 0.0305 - val_classifier_acc: 0.9411 - val_decoder_mean_squared_error: 0.0305\n",
            "Performance on the validation set\n",
            "loss: 0.24\n",
            "classifier_loss: 0.20\n",
            "decoder_loss: 0.03\n",
            "classifier_acc: 0.94\n",
            "decoder_mean_squared_error: 0.03\n",
            "\n",
            "FOLD:  3\n",
            "Model: \"model_41\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_27 (InputLayer)           [(None, 784)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 784)          0           input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_150 (Dense)               (None, 522)          409770      dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_151 (Dense)               (None, 261)          136503      dense_150[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_152 (Dense)               (None, 104)          27248       dense_151[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_153 (Dense)               (None, 34)           3570        dense_152[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_154 (Dense)               (None, 104)          3640        dense_153[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_155 (Dense)               (None, 261)          27405       dense_154[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_156 (Dense)               (None, 522)          136764      dense_155[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           385         dense_153[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          410032      dense_156[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,317\n",
            "Trainable params: 1,155,317\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 278us/sample - loss: 1.0592 - classifier_loss: 0.9710 - decoder_loss: 0.0866 - classifier_acc: 0.6974 - decoder_mean_squared_error: 0.0867 - val_loss: 0.5653 - val_classifier_loss: 0.4997 - val_decoder_loss: 0.0638 - val_classifier_acc: 0.8455 - val_decoder_mean_squared_error: 0.0638\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.5319 - classifier_loss: 0.4738 - decoder_loss: 0.0582 - classifier_acc: 0.8496 - decoder_mean_squared_error: 0.0582 - val_loss: 0.4009 - val_classifier_loss: 0.3499 - val_decoder_loss: 0.0519 - val_classifier_acc: 0.8964 - val_decoder_mean_squared_error: 0.0519\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.4018 - classifier_loss: 0.3496 - decoder_loss: 0.0518 - classifier_acc: 0.8914 - decoder_mean_squared_error: 0.0518 - val_loss: 0.3409 - val_classifier_loss: 0.2931 - val_decoder_loss: 0.0478 - val_classifier_acc: 0.9161 - val_decoder_mean_squared_error: 0.0478\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.3179 - classifier_loss: 0.2709 - decoder_loss: 0.0472 - classifier_acc: 0.9144 - decoder_mean_squared_error: 0.0472 - val_loss: 0.3045 - val_classifier_loss: 0.2593 - val_decoder_loss: 0.0437 - val_classifier_acc: 0.9286 - val_decoder_mean_squared_error: 0.0437\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 71us/sample - loss: 0.2780 - classifier_loss: 0.2339 - decoder_loss: 0.0443 - classifier_acc: 0.9262 - decoder_mean_squared_error: 0.0443 - val_loss: 0.2920 - val_classifier_loss: 0.2457 - val_decoder_loss: 0.0443 - val_classifier_acc: 0.9295 - val_decoder_mean_squared_error: 0.0443\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.2427 - classifier_loss: 0.1998 - decoder_loss: 0.0428 - classifier_acc: 0.9373 - decoder_mean_squared_error: 0.0428 - val_loss: 0.2789 - val_classifier_loss: 0.2381 - val_decoder_loss: 0.0411 - val_classifier_acc: 0.9241 - val_decoder_mean_squared_error: 0.0411\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.2165 - classifier_loss: 0.1746 - decoder_loss: 0.0419 - classifier_acc: 0.9436 - decoder_mean_squared_error: 0.0419 - val_loss: 0.2612 - val_classifier_loss: 0.2238 - val_decoder_loss: 0.0395 - val_classifier_acc: 0.9357 - val_decoder_mean_squared_error: 0.0395\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 71us/sample - loss: 0.1808 - classifier_loss: 0.1415 - decoder_loss: 0.0393 - classifier_acc: 0.9518 - decoder_mean_squared_error: 0.0393 - val_loss: 0.2678 - val_classifier_loss: 0.2292 - val_decoder_loss: 0.0374 - val_classifier_acc: 0.9375 - val_decoder_mean_squared_error: 0.0374\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1715 - classifier_loss: 0.1332 - decoder_loss: 0.0384 - classifier_acc: 0.9547 - decoder_mean_squared_error: 0.0384 - val_loss: 0.2815 - val_classifier_loss: 0.2430 - val_decoder_loss: 0.0365 - val_classifier_acc: 0.9393 - val_decoder_mean_squared_error: 0.0365\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1457 - classifier_loss: 0.1087 - decoder_loss: 0.0369 - classifier_acc: 0.9628 - decoder_mean_squared_error: 0.0369 - val_loss: 0.2462 - val_classifier_loss: 0.2135 - val_decoder_loss: 0.0347 - val_classifier_acc: 0.9366 - val_decoder_mean_squared_error: 0.0348\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1483 - classifier_loss: 0.1120 - decoder_loss: 0.0365 - classifier_acc: 0.9629 - decoder_mean_squared_error: 0.0365 - val_loss: 0.2606 - val_classifier_loss: 0.2240 - val_decoder_loss: 0.0353 - val_classifier_acc: 0.9393 - val_decoder_mean_squared_error: 0.0353\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.1363 - classifier_loss: 0.1005 - decoder_loss: 0.0358 - classifier_acc: 0.9652 - decoder_mean_squared_error: 0.0358 - val_loss: 0.2486 - val_classifier_loss: 0.2114 - val_decoder_loss: 0.0347 - val_classifier_acc: 0.9464 - val_decoder_mean_squared_error: 0.0347\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1212 - classifier_loss: 0.0867 - decoder_loss: 0.0346 - classifier_acc: 0.9719 - decoder_mean_squared_error: 0.0346 - val_loss: 0.2596 - val_classifier_loss: 0.2253 - val_decoder_loss: 0.0338 - val_classifier_acc: 0.9384 - val_decoder_mean_squared_error: 0.0338\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1269 - classifier_loss: 0.0920 - decoder_loss: 0.0347 - classifier_acc: 0.9689 - decoder_mean_squared_error: 0.0347 - val_loss: 0.2388 - val_classifier_loss: 0.2027 - val_decoder_loss: 0.0341 - val_classifier_acc: 0.9482 - val_decoder_mean_squared_error: 0.0342\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1108 - classifier_loss: 0.0770 - decoder_loss: 0.0337 - classifier_acc: 0.9729 - decoder_mean_squared_error: 0.0337 - val_loss: 0.2661 - val_classifier_loss: 0.2390 - val_decoder_loss: 0.0326 - val_classifier_acc: 0.9357 - val_decoder_mean_squared_error: 0.0327\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1122 - classifier_loss: 0.0786 - decoder_loss: 0.0336 - classifier_acc: 0.9733 - decoder_mean_squared_error: 0.0336 - val_loss: 0.2602 - val_classifier_loss: 0.2286 - val_decoder_loss: 0.0338 - val_classifier_acc: 0.9393 - val_decoder_mean_squared_error: 0.0338\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1038 - classifier_loss: 0.0711 - decoder_loss: 0.0331 - classifier_acc: 0.9743 - decoder_mean_squared_error: 0.0331 - val_loss: 0.2577 - val_classifier_loss: 0.2266 - val_decoder_loss: 0.0319 - val_classifier_acc: 0.9366 - val_decoder_mean_squared_error: 0.0319\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.0973 - classifier_loss: 0.0648 - decoder_loss: 0.0324 - classifier_acc: 0.9770 - decoder_mean_squared_error: 0.0323 - val_loss: 0.2939 - val_classifier_loss: 0.2668 - val_decoder_loss: 0.0327 - val_classifier_acc: 0.9375 - val_decoder_mean_squared_error: 0.0326\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.0957 - classifier_loss: 0.0638 - decoder_loss: 0.0319 - classifier_acc: 0.9769 - decoder_mean_squared_error: 0.0319 - val_loss: 0.3234 - val_classifier_loss: 0.2940 - val_decoder_loss: 0.0316 - val_classifier_acc: 0.9348 - val_decoder_mean_squared_error: 0.0316\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.0968 - classifier_loss: 0.0653 - decoder_loss: 0.0316 - classifier_acc: 0.9794 - decoder_mean_squared_error: 0.0316 - val_loss: 0.2651 - val_classifier_loss: 0.2355 - val_decoder_loss: 0.0301 - val_classifier_acc: 0.9348 - val_decoder_mean_squared_error: 0.0302\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.0833 - classifier_loss: 0.0525 - decoder_loss: 0.0308 - classifier_acc: 0.9827 - decoder_mean_squared_error: 0.0308 - val_loss: 0.3052 - val_classifier_loss: 0.2829 - val_decoder_loss: 0.0302 - val_classifier_acc: 0.9295 - val_decoder_mean_squared_error: 0.0301\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 70us/sample - loss: 0.0866 - classifier_loss: 0.0557 - decoder_loss: 0.0309 - classifier_acc: 0.9800 - decoder_mean_squared_error: 0.0309 - val_loss: 0.2764 - val_classifier_loss: 0.2456 - val_decoder_loss: 0.0318 - val_classifier_acc: 0.9384 - val_decoder_mean_squared_error: 0.0318\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.0786 - classifier_loss: 0.0486 - decoder_loss: 0.0301 - classifier_acc: 0.9846 - decoder_mean_squared_error: 0.0301 - val_loss: 0.2706 - val_classifier_loss: 0.2521 - val_decoder_loss: 0.0306 - val_classifier_acc: 0.9420 - val_decoder_mean_squared_error: 0.0305\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 2s 188us/sample - loss: 0.0864 - classifier_loss: 0.0565 - decoder_loss: 0.0301 - classifier_acc: 0.9798 - decoder_mean_squared_error: 0.0301 - val_loss: 0.2785 - val_classifier_loss: 0.2495 - val_decoder_loss: 0.0305 - val_classifier_acc: 0.9357 - val_decoder_mean_squared_error: 0.0305\n",
            "Performance on the validation set\n",
            "loss: 0.23\n",
            "classifier_loss: 0.20\n",
            "decoder_loss: 0.03\n",
            "classifier_acc: 0.95\n",
            "decoder_mean_squared_error: 0.03\n",
            "\n",
            "FOLD:  4\n",
            "Model: \"model_43\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_28 (InputLayer)           [(None, 784)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 784)          0           input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_157 (Dense)               (None, 522)          409770      dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_158 (Dense)               (None, 261)          136503      dense_157[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_159 (Dense)               (None, 104)          27248       dense_158[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_160 (Dense)               (None, 34)           3570        dense_159[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_161 (Dense)               (None, 104)          3640        dense_160[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_162 (Dense)               (None, 261)          27405       dense_161[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_163 (Dense)               (None, 522)          136764      dense_162[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           385         dense_160[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          410032      dense_163[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,317\n",
            "Trainable params: 1,155,317\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 300us/sample - loss: 1.1331 - classifier_loss: 1.0443 - decoder_loss: 0.0872 - classifier_acc: 0.6578 - decoder_mean_squared_error: 0.0872 - val_loss: 0.5713 - val_classifier_loss: 0.5106 - val_decoder_loss: 0.0636 - val_classifier_acc: 0.8562 - val_decoder_mean_squared_error: 0.0636\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.5611 - classifier_loss: 0.5025 - decoder_loss: 0.0581 - classifier_acc: 0.8441 - decoder_mean_squared_error: 0.0581 - val_loss: 0.4351 - val_classifier_loss: 0.3784 - val_decoder_loss: 0.0535 - val_classifier_acc: 0.8902 - val_decoder_mean_squared_error: 0.0535\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.4028 - classifier_loss: 0.3506 - decoder_loss: 0.0517 - classifier_acc: 0.8894 - decoder_mean_squared_error: 0.0517 - val_loss: 0.3389 - val_classifier_loss: 0.2867 - val_decoder_loss: 0.0488 - val_classifier_acc: 0.9107 - val_decoder_mean_squared_error: 0.0489\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.3207 - classifier_loss: 0.2728 - decoder_loss: 0.0480 - classifier_acc: 0.9153 - decoder_mean_squared_error: 0.0480 - val_loss: 0.3446 - val_classifier_loss: 0.2942 - val_decoder_loss: 0.0468 - val_classifier_acc: 0.9134 - val_decoder_mean_squared_error: 0.0469\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.2774 - classifier_loss: 0.2329 - decoder_loss: 0.0453 - classifier_acc: 0.9257 - decoder_mean_squared_error: 0.0453 - val_loss: 0.2944 - val_classifier_loss: 0.2493 - val_decoder_loss: 0.0429 - val_classifier_acc: 0.9277 - val_decoder_mean_squared_error: 0.0429\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.2435 - classifier_loss: 0.2006 - decoder_loss: 0.0434 - classifier_acc: 0.9360 - decoder_mean_squared_error: 0.0434 - val_loss: 0.2970 - val_classifier_loss: 0.2557 - val_decoder_loss: 0.0429 - val_classifier_acc: 0.9268 - val_decoder_mean_squared_error: 0.0428\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.2113 - classifier_loss: 0.1690 - decoder_loss: 0.0421 - classifier_acc: 0.9458 - decoder_mean_squared_error: 0.0421 - val_loss: 0.2708 - val_classifier_loss: 0.2298 - val_decoder_loss: 0.0405 - val_classifier_acc: 0.9286 - val_decoder_mean_squared_error: 0.0404\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1851 - classifier_loss: 0.1451 - decoder_loss: 0.0400 - classifier_acc: 0.9513 - decoder_mean_squared_error: 0.0400 - val_loss: 0.3030 - val_classifier_loss: 0.2625 - val_decoder_loss: 0.0388 - val_classifier_acc: 0.9321 - val_decoder_mean_squared_error: 0.0390\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1695 - classifier_loss: 0.1300 - decoder_loss: 0.0394 - classifier_acc: 0.9577 - decoder_mean_squared_error: 0.0394 - val_loss: 0.2464 - val_classifier_loss: 0.2075 - val_decoder_loss: 0.0385 - val_classifier_acc: 0.9375 - val_decoder_mean_squared_error: 0.0385\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.1699 - classifier_loss: 0.1309 - decoder_loss: 0.0390 - classifier_acc: 0.9543 - decoder_mean_squared_error: 0.0390 - val_loss: 0.2800 - val_classifier_loss: 0.2422 - val_decoder_loss: 0.0378 - val_classifier_acc: 0.9348 - val_decoder_mean_squared_error: 0.0377\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1459 - classifier_loss: 0.1087 - decoder_loss: 0.0371 - classifier_acc: 0.9633 - decoder_mean_squared_error: 0.0371 - val_loss: 0.2953 - val_classifier_loss: 0.2557 - val_decoder_loss: 0.0385 - val_classifier_acc: 0.9187 - val_decoder_mean_squared_error: 0.0384\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1361 - classifier_loss: 0.0995 - decoder_loss: 0.0366 - classifier_acc: 0.9658 - decoder_mean_squared_error: 0.0366 - val_loss: 0.2635 - val_classifier_loss: 0.2245 - val_decoder_loss: 0.0355 - val_classifier_acc: 0.9339 - val_decoder_mean_squared_error: 0.0356\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1333 - classifier_loss: 0.0979 - decoder_loss: 0.0356 - classifier_acc: 0.9661 - decoder_mean_squared_error: 0.0356 - val_loss: 0.2740 - val_classifier_loss: 0.2413 - val_decoder_loss: 0.0364 - val_classifier_acc: 0.9375 - val_decoder_mean_squared_error: 0.0363\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1156 - classifier_loss: 0.0808 - decoder_loss: 0.0347 - classifier_acc: 0.9727 - decoder_mean_squared_error: 0.0347 - val_loss: 0.2906 - val_classifier_loss: 0.2583 - val_decoder_loss: 0.0351 - val_classifier_acc: 0.9330 - val_decoder_mean_squared_error: 0.0351\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1165 - classifier_loss: 0.0820 - decoder_loss: 0.0346 - classifier_acc: 0.9713 - decoder_mean_squared_error: 0.0346 - val_loss: 0.2813 - val_classifier_loss: 0.2532 - val_decoder_loss: 0.0351 - val_classifier_acc: 0.9402 - val_decoder_mean_squared_error: 0.0350\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.1041 - classifier_loss: 0.0700 - decoder_loss: 0.0341 - classifier_acc: 0.9737 - decoder_mean_squared_error: 0.0341 - val_loss: 0.2994 - val_classifier_loss: 0.2596 - val_decoder_loss: 0.0347 - val_classifier_acc: 0.9357 - val_decoder_mean_squared_error: 0.0347\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1009 - classifier_loss: 0.0672 - decoder_loss: 0.0337 - classifier_acc: 0.9773 - decoder_mean_squared_error: 0.0337 - val_loss: 0.2838 - val_classifier_loss: 0.2477 - val_decoder_loss: 0.0338 - val_classifier_acc: 0.9366 - val_decoder_mean_squared_error: 0.0337\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1010 - classifier_loss: 0.0679 - decoder_loss: 0.0333 - classifier_acc: 0.9761 - decoder_mean_squared_error: 0.0333 - val_loss: 0.2954 - val_classifier_loss: 0.2644 - val_decoder_loss: 0.0328 - val_classifier_acc: 0.9366 - val_decoder_mean_squared_error: 0.0328\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 2s 192us/sample - loss: 0.0908 - classifier_loss: 0.0584 - decoder_loss: 0.0323 - classifier_acc: 0.9783 - decoder_mean_squared_error: 0.0323 - val_loss: 0.2907 - val_classifier_loss: 0.2566 - val_decoder_loss: 0.0329 - val_classifier_acc: 0.9348 - val_decoder_mean_squared_error: 0.0329\n",
            "Performance on the validation set\n",
            "loss: 0.24\n",
            "classifier_loss: 0.20\n",
            "decoder_loss: 0.04\n",
            "classifier_acc: 0.94\n",
            "decoder_mean_squared_error: 0.04\n",
            "\n",
            "FOLD:  5\n",
            "Model: \"model_45\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_29 (InputLayer)           [(None, 784)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 784)          0           input_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_164 (Dense)               (None, 522)          409770      dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_165 (Dense)               (None, 261)          136503      dense_164[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_166 (Dense)               (None, 104)          27248       dense_165[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_167 (Dense)               (None, 34)           3570        dense_166[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_168 (Dense)               (None, 104)          3640        dense_167[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_169 (Dense)               (None, 261)          27405       dense_168[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_170 (Dense)               (None, 522)          136764      dense_169[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           385         dense_167[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          410032      dense_170[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,317\n",
            "Trainable params: 1,155,317\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 309us/sample - loss: 1.1386 - classifier_loss: 1.0513 - decoder_loss: 0.0861 - classifier_acc: 0.6712 - decoder_mean_squared_error: 0.0861 - val_loss: 0.6276 - val_classifier_loss: 0.5656 - val_decoder_loss: 0.0639 - val_classifier_acc: 0.8357 - val_decoder_mean_squared_error: 0.0639\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.5673 - classifier_loss: 0.5086 - decoder_loss: 0.0583 - classifier_acc: 0.8474 - decoder_mean_squared_error: 0.0583 - val_loss: 0.4789 - val_classifier_loss: 0.4267 - val_decoder_loss: 0.0543 - val_classifier_acc: 0.8848 - val_decoder_mean_squared_error: 0.0543\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.4168 - classifier_loss: 0.3664 - decoder_loss: 0.0508 - classifier_acc: 0.8907 - decoder_mean_squared_error: 0.0508 - val_loss: 0.3951 - val_classifier_loss: 0.3463 - val_decoder_loss: 0.0475 - val_classifier_acc: 0.9071 - val_decoder_mean_squared_error: 0.0475\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.3483 - classifier_loss: 0.3005 - decoder_loss: 0.0475 - classifier_acc: 0.9068 - decoder_mean_squared_error: 0.0475 - val_loss: 0.3355 - val_classifier_loss: 0.2882 - val_decoder_loss: 0.0453 - val_classifier_acc: 0.9205 - val_decoder_mean_squared_error: 0.0453\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.2840 - classifier_loss: 0.2392 - decoder_loss: 0.0449 - classifier_acc: 0.9231 - decoder_mean_squared_error: 0.0449 - val_loss: 0.3179 - val_classifier_loss: 0.2745 - val_decoder_loss: 0.0426 - val_classifier_acc: 0.9205 - val_decoder_mean_squared_error: 0.0426\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.2574 - classifier_loss: 0.2149 - decoder_loss: 0.0431 - classifier_acc: 0.9332 - decoder_mean_squared_error: 0.0431 - val_loss: 0.3284 - val_classifier_loss: 0.2847 - val_decoder_loss: 0.0407 - val_classifier_acc: 0.9205 - val_decoder_mean_squared_error: 0.0407\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.2311 - classifier_loss: 0.1887 - decoder_loss: 0.0421 - classifier_acc: 0.9391 - decoder_mean_squared_error: 0.0421 - val_loss: 0.2843 - val_classifier_loss: 0.2452 - val_decoder_loss: 0.0386 - val_classifier_acc: 0.9339 - val_decoder_mean_squared_error: 0.0387\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1892 - classifier_loss: 0.1500 - decoder_loss: 0.0392 - classifier_acc: 0.9501 - decoder_mean_squared_error: 0.0392 - val_loss: 0.2856 - val_classifier_loss: 0.2466 - val_decoder_loss: 0.0382 - val_classifier_acc: 0.9366 - val_decoder_mean_squared_error: 0.0382\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1875 - classifier_loss: 0.1482 - decoder_loss: 0.0391 - classifier_acc: 0.9514 - decoder_mean_squared_error: 0.0391 - val_loss: 0.2984 - val_classifier_loss: 0.2663 - val_decoder_loss: 0.0374 - val_classifier_acc: 0.9304 - val_decoder_mean_squared_error: 0.0374\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1669 - classifier_loss: 0.1289 - decoder_loss: 0.0379 - classifier_acc: 0.9576 - decoder_mean_squared_error: 0.0379 - val_loss: 0.2788 - val_classifier_loss: 0.2394 - val_decoder_loss: 0.0381 - val_classifier_acc: 0.9375 - val_decoder_mean_squared_error: 0.0380\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1391 - classifier_loss: 0.1027 - decoder_loss: 0.0366 - classifier_acc: 0.9637 - decoder_mean_squared_error: 0.0366 - val_loss: 0.2945 - val_classifier_loss: 0.2570 - val_decoder_loss: 0.0350 - val_classifier_acc: 0.9321 - val_decoder_mean_squared_error: 0.0351\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.1484 - classifier_loss: 0.1120 - decoder_loss: 0.0364 - classifier_acc: 0.9623 - decoder_mean_squared_error: 0.0364 - val_loss: 0.2948 - val_classifier_loss: 0.2546 - val_decoder_loss: 0.0350 - val_classifier_acc: 0.9357 - val_decoder_mean_squared_error: 0.0350\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1341 - classifier_loss: 0.0985 - decoder_loss: 0.0355 - classifier_acc: 0.9670 - decoder_mean_squared_error: 0.0355 - val_loss: 0.3069 - val_classifier_loss: 0.2764 - val_decoder_loss: 0.0345 - val_classifier_acc: 0.9321 - val_decoder_mean_squared_error: 0.0344\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1187 - classifier_loss: 0.0841 - decoder_loss: 0.0348 - classifier_acc: 0.9707 - decoder_mean_squared_error: 0.0348 - val_loss: 0.2790 - val_classifier_loss: 0.2523 - val_decoder_loss: 0.0333 - val_classifier_acc: 0.9429 - val_decoder_mean_squared_error: 0.0332\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1188 - classifier_loss: 0.0852 - decoder_loss: 0.0339 - classifier_acc: 0.9684 - decoder_mean_squared_error: 0.0339 - val_loss: 0.2829 - val_classifier_loss: 0.2578 - val_decoder_loss: 0.0329 - val_classifier_acc: 0.9366 - val_decoder_mean_squared_error: 0.0330\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1058 - classifier_loss: 0.0720 - decoder_loss: 0.0337 - classifier_acc: 0.9753 - decoder_mean_squared_error: 0.0337 - val_loss: 0.2751 - val_classifier_loss: 0.2372 - val_decoder_loss: 0.0325 - val_classifier_acc: 0.9438 - val_decoder_mean_squared_error: 0.0325\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 72us/sample - loss: 0.1024 - classifier_loss: 0.0699 - decoder_loss: 0.0327 - classifier_acc: 0.9759 - decoder_mean_squared_error: 0.0327 - val_loss: 0.2881 - val_classifier_loss: 0.2546 - val_decoder_loss: 0.0319 - val_classifier_acc: 0.9455 - val_decoder_mean_squared_error: 0.0319\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.0996 - classifier_loss: 0.0673 - decoder_loss: 0.0323 - classifier_acc: 0.9766 - decoder_mean_squared_error: 0.0323 - val_loss: 0.2862 - val_classifier_loss: 0.2556 - val_decoder_loss: 0.0321 - val_classifier_acc: 0.9402 - val_decoder_mean_squared_error: 0.0321\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.0955 - classifier_loss: 0.0639 - decoder_loss: 0.0317 - classifier_acc: 0.9770 - decoder_mean_squared_error: 0.0317 - val_loss: 0.3130 - val_classifier_loss: 0.2912 - val_decoder_loss: 0.0318 - val_classifier_acc: 0.9384 - val_decoder_mean_squared_error: 0.0318\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.0973 - classifier_loss: 0.0651 - decoder_loss: 0.0321 - classifier_acc: 0.9759 - decoder_mean_squared_error: 0.0321 - val_loss: 0.2841 - val_classifier_loss: 0.2547 - val_decoder_loss: 0.0318 - val_classifier_acc: 0.9446 - val_decoder_mean_squared_error: 0.0317\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.0972 - classifier_loss: 0.0654 - decoder_loss: 0.0318 - classifier_acc: 0.9769 - decoder_mean_squared_error: 0.0318 - val_loss: 0.3165 - val_classifier_loss: 0.2837 - val_decoder_loss: 0.0318 - val_classifier_acc: 0.9330 - val_decoder_mean_squared_error: 0.0318\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.0840 - classifier_loss: 0.0532 - decoder_loss: 0.0308 - classifier_acc: 0.9805 - decoder_mean_squared_error: 0.0308 - val_loss: 0.2950 - val_classifier_loss: 0.2730 - val_decoder_loss: 0.0309 - val_classifier_acc: 0.9429 - val_decoder_mean_squared_error: 0.0308\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.0838 - classifier_loss: 0.0530 - decoder_loss: 0.0308 - classifier_acc: 0.9806 - decoder_mean_squared_error: 0.0307 - val_loss: 0.2929 - val_classifier_loss: 0.2759 - val_decoder_loss: 0.0309 - val_classifier_acc: 0.9411 - val_decoder_mean_squared_error: 0.0308\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.0806 - classifier_loss: 0.0505 - decoder_loss: 0.0301 - classifier_acc: 0.9815 - decoder_mean_squared_error: 0.0301 - val_loss: 0.3138 - val_classifier_loss: 0.2820 - val_decoder_loss: 0.0295 - val_classifier_acc: 0.9402 - val_decoder_mean_squared_error: 0.0294\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.0836 - classifier_loss: 0.0536 - decoder_loss: 0.0301 - classifier_acc: 0.9803 - decoder_mean_squared_error: 0.0301 - val_loss: 0.3136 - val_classifier_loss: 0.2834 - val_decoder_loss: 0.0311 - val_classifier_acc: 0.9375 - val_decoder_mean_squared_error: 0.0310\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 2s 197us/sample - loss: 0.0731 - classifier_loss: 0.0434 - decoder_loss: 0.0297 - classifier_acc: 0.9851 - decoder_mean_squared_error: 0.0297 - val_loss: 0.3237 - val_classifier_loss: 0.2924 - val_decoder_loss: 0.0305 - val_classifier_acc: 0.9411 - val_decoder_mean_squared_error: 0.0304\n",
            "Performance on the validation set\n",
            "loss: 0.23\n",
            "classifier_loss: 0.20\n",
            "decoder_loss: 0.03\n",
            "classifier_acc: 0.95\n",
            "decoder_mean_squared_error: 0.03\n",
            "\n",
            "FOLD:  6\n",
            "Model: \"model_47\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_30 (InputLayer)           [(None, 784)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_52 (Dropout)            (None, 784)          0           input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_171 (Dense)               (None, 522)          409770      dropout_52[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_172 (Dense)               (None, 261)          136503      dense_171[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_173 (Dense)               (None, 104)          27248       dense_172[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_174 (Dense)               (None, 34)           3570        dense_173[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_175 (Dense)               (None, 104)          3640        dense_174[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_176 (Dense)               (None, 261)          27405       dense_175[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_177 (Dense)               (None, 522)          136764      dense_176[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           385         dense_174[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          410032      dense_177[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,317\n",
            "Trainable params: 1,155,317\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 323us/sample - loss: 1.1360 - classifier_loss: 1.0472 - decoder_loss: 0.0876 - classifier_acc: 0.6655 - decoder_mean_squared_error: 0.0877 - val_loss: 0.6165 - val_classifier_loss: 0.5493 - val_decoder_loss: 0.0656 - val_classifier_acc: 0.8339 - val_decoder_mean_squared_error: 0.0657\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.5433 - classifier_loss: 0.4847 - decoder_loss: 0.0582 - classifier_acc: 0.8491 - decoder_mean_squared_error: 0.0582 - val_loss: 0.3872 - val_classifier_loss: 0.3370 - val_decoder_loss: 0.0508 - val_classifier_acc: 0.9009 - val_decoder_mean_squared_error: 0.0507\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.4103 - classifier_loss: 0.3587 - decoder_loss: 0.0511 - classifier_acc: 0.8881 - decoder_mean_squared_error: 0.0511 - val_loss: 0.3382 - val_classifier_loss: 0.2913 - val_decoder_loss: 0.0484 - val_classifier_acc: 0.9098 - val_decoder_mean_squared_error: 0.0484\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.3210 - classifier_loss: 0.2727 - decoder_loss: 0.0480 - classifier_acc: 0.9132 - decoder_mean_squared_error: 0.0480 - val_loss: 0.3056 - val_classifier_loss: 0.2643 - val_decoder_loss: 0.0441 - val_classifier_acc: 0.9179 - val_decoder_mean_squared_error: 0.0441\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.2825 - classifier_loss: 0.2368 - decoder_loss: 0.0459 - classifier_acc: 0.9260 - decoder_mean_squared_error: 0.0459 - val_loss: 0.2692 - val_classifier_loss: 0.2252 - val_decoder_loss: 0.0417 - val_classifier_acc: 0.9321 - val_decoder_mean_squared_error: 0.0417\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.2401 - classifier_loss: 0.1970 - decoder_loss: 0.0428 - classifier_acc: 0.9399 - decoder_mean_squared_error: 0.0428 - val_loss: 0.2624 - val_classifier_loss: 0.2222 - val_decoder_loss: 0.0393 - val_classifier_acc: 0.9339 - val_decoder_mean_squared_error: 0.0394\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.2082 - classifier_loss: 0.1669 - decoder_loss: 0.0413 - classifier_acc: 0.9483 - decoder_mean_squared_error: 0.0413 - val_loss: 0.2215 - val_classifier_loss: 0.1863 - val_decoder_loss: 0.0380 - val_classifier_acc: 0.9429 - val_decoder_mean_squared_error: 0.0380\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1980 - classifier_loss: 0.1574 - decoder_loss: 0.0404 - classifier_acc: 0.9502 - decoder_mean_squared_error: 0.0404 - val_loss: 0.2448 - val_classifier_loss: 0.2094 - val_decoder_loss: 0.0382 - val_classifier_acc: 0.9339 - val_decoder_mean_squared_error: 0.0382\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.1749 - classifier_loss: 0.1362 - decoder_loss: 0.0394 - classifier_acc: 0.9534 - decoder_mean_squared_error: 0.0394 - val_loss: 0.2413 - val_classifier_loss: 0.2079 - val_decoder_loss: 0.0370 - val_classifier_acc: 0.9366 - val_decoder_mean_squared_error: 0.0371\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1580 - classifier_loss: 0.1198 - decoder_loss: 0.0382 - classifier_acc: 0.9593 - decoder_mean_squared_error: 0.0382 - val_loss: 0.2332 - val_classifier_loss: 0.1955 - val_decoder_loss: 0.0357 - val_classifier_acc: 0.9411 - val_decoder_mean_squared_error: 0.0357\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1479 - classifier_loss: 0.1103 - decoder_loss: 0.0377 - classifier_acc: 0.9631 - decoder_mean_squared_error: 0.0377 - val_loss: 0.2264 - val_classifier_loss: 0.1907 - val_decoder_loss: 0.0356 - val_classifier_acc: 0.9438 - val_decoder_mean_squared_error: 0.0357\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1357 - classifier_loss: 0.0994 - decoder_loss: 0.0363 - classifier_acc: 0.9659 - decoder_mean_squared_error: 0.0363 - val_loss: 0.2221 - val_classifier_loss: 0.1868 - val_decoder_loss: 0.0347 - val_classifier_acc: 0.9455 - val_decoder_mean_squared_error: 0.0347\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1157 - classifier_loss: 0.0803 - decoder_loss: 0.0352 - classifier_acc: 0.9730 - decoder_mean_squared_error: 0.0352 - val_loss: 0.2406 - val_classifier_loss: 0.2116 - val_decoder_loss: 0.0330 - val_classifier_acc: 0.9420 - val_decoder_mean_squared_error: 0.0332\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.1280 - classifier_loss: 0.0923 - decoder_loss: 0.0358 - classifier_acc: 0.9685 - decoder_mean_squared_error: 0.0358 - val_loss: 0.2605 - val_classifier_loss: 0.2218 - val_decoder_loss: 0.0340 - val_classifier_acc: 0.9339 - val_decoder_mean_squared_error: 0.0340\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1093 - classifier_loss: 0.0747 - decoder_loss: 0.0347 - classifier_acc: 0.9737 - decoder_mean_squared_error: 0.0347 - val_loss: 0.2162 - val_classifier_loss: 0.1848 - val_decoder_loss: 0.0323 - val_classifier_acc: 0.9429 - val_decoder_mean_squared_error: 0.0324\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.1049 - classifier_loss: 0.0711 - decoder_loss: 0.0338 - classifier_acc: 0.9745 - decoder_mean_squared_error: 0.0338 - val_loss: 0.2403 - val_classifier_loss: 0.2058 - val_decoder_loss: 0.0330 - val_classifier_acc: 0.9357 - val_decoder_mean_squared_error: 0.0330\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1060 - classifier_loss: 0.0724 - decoder_loss: 0.0335 - classifier_acc: 0.9740 - decoder_mean_squared_error: 0.0335 - val_loss: 0.2469 - val_classifier_loss: 0.2131 - val_decoder_loss: 0.0321 - val_classifier_acc: 0.9429 - val_decoder_mean_squared_error: 0.0320\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.0970 - classifier_loss: 0.0642 - decoder_loss: 0.0328 - classifier_acc: 0.9784 - decoder_mean_squared_error: 0.0327 - val_loss: 0.2304 - val_classifier_loss: 0.1996 - val_decoder_loss: 0.0322 - val_classifier_acc: 0.9464 - val_decoder_mean_squared_error: 0.0323\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.0881 - classifier_loss: 0.0560 - decoder_loss: 0.0321 - classifier_acc: 0.9812 - decoder_mean_squared_error: 0.0322 - val_loss: 0.2345 - val_classifier_loss: 0.1993 - val_decoder_loss: 0.0312 - val_classifier_acc: 0.9464 - val_decoder_mean_squared_error: 0.0311\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 80us/sample - loss: 0.0922 - classifier_loss: 0.0597 - decoder_loss: 0.0324 - classifier_acc: 0.9785 - decoder_mean_squared_error: 0.0324 - val_loss: 0.2445 - val_classifier_loss: 0.2110 - val_decoder_loss: 0.0325 - val_classifier_acc: 0.9429 - val_decoder_mean_squared_error: 0.0325\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 80us/sample - loss: 0.0921 - classifier_loss: 0.0604 - decoder_loss: 0.0318 - classifier_acc: 0.9783 - decoder_mean_squared_error: 0.0318 - val_loss: 0.2588 - val_classifier_loss: 0.2267 - val_decoder_loss: 0.0321 - val_classifier_acc: 0.9339 - val_decoder_mean_squared_error: 0.0321\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.0854 - classifier_loss: 0.0542 - decoder_loss: 0.0315 - classifier_acc: 0.9806 - decoder_mean_squared_error: 0.0315 - val_loss: 0.2375 - val_classifier_loss: 0.2108 - val_decoder_loss: 0.0307 - val_classifier_acc: 0.9411 - val_decoder_mean_squared_error: 0.0307\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 73us/sample - loss: 0.0853 - classifier_loss: 0.0540 - decoder_loss: 0.0313 - classifier_acc: 0.9811 - decoder_mean_squared_error: 0.0313 - val_loss: 0.2825 - val_classifier_loss: 0.2494 - val_decoder_loss: 0.0321 - val_classifier_acc: 0.9393 - val_decoder_mean_squared_error: 0.0320\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.0906 - classifier_loss: 0.0592 - decoder_loss: 0.0313 - classifier_acc: 0.9796 - decoder_mean_squared_error: 0.0313 - val_loss: 0.2338 - val_classifier_loss: 0.2099 - val_decoder_loss: 0.0299 - val_classifier_acc: 0.9473 - val_decoder_mean_squared_error: 0.0299\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 2s 204us/sample - loss: 0.0742 - classifier_loss: 0.0438 - decoder_loss: 0.0303 - classifier_acc: 0.9852 - decoder_mean_squared_error: 0.0303 - val_loss: 0.2493 - val_classifier_loss: 0.2188 - val_decoder_loss: 0.0295 - val_classifier_acc: 0.9438 - val_decoder_mean_squared_error: 0.0294\n",
            "Performance on the validation set\n",
            "loss: 0.24\n",
            "classifier_loss: 0.20\n",
            "decoder_loss: 0.03\n",
            "classifier_acc: 0.95\n",
            "decoder_mean_squared_error: 0.03\n",
            "\n",
            "FOLD:  7\n",
            "Model: \"model_49\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_31 (InputLayer)           [(None, 784)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_53 (Dropout)            (None, 784)          0           input_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_178 (Dense)               (None, 522)          409770      dropout_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_179 (Dense)               (None, 261)          136503      dense_178[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_180 (Dense)               (None, 104)          27248       dense_179[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_181 (Dense)               (None, 34)           3570        dense_180[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_182 (Dense)               (None, 104)          3640        dense_181[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_183 (Dense)               (None, 261)          27405       dense_182[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_184 (Dense)               (None, 522)          136764      dense_183[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           385         dense_181[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          410032      dense_184[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,317\n",
            "Trainable params: 1,155,317\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 341us/sample - loss: 1.1367 - classifier_loss: 1.0511 - decoder_loss: 0.0839 - classifier_acc: 0.6575 - decoder_mean_squared_error: 0.0840 - val_loss: 0.5800 - val_classifier_loss: 0.5143 - val_decoder_loss: 0.0629 - val_classifier_acc: 0.8446 - val_decoder_mean_squared_error: 0.0629\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.5380 - classifier_loss: 0.4802 - decoder_loss: 0.0575 - classifier_acc: 0.8497 - decoder_mean_squared_error: 0.0575 - val_loss: 0.4058 - val_classifier_loss: 0.3519 - val_decoder_loss: 0.0530 - val_classifier_acc: 0.8884 - val_decoder_mean_squared_error: 0.0530\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.4052 - classifier_loss: 0.3535 - decoder_loss: 0.0515 - classifier_acc: 0.8917 - decoder_mean_squared_error: 0.0516 - val_loss: 0.3757 - val_classifier_loss: 0.3244 - val_decoder_loss: 0.0484 - val_classifier_acc: 0.8991 - val_decoder_mean_squared_error: 0.0484\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.3206 - classifier_loss: 0.2732 - decoder_loss: 0.0472 - classifier_acc: 0.9134 - decoder_mean_squared_error: 0.0472 - val_loss: 0.3079 - val_classifier_loss: 0.2646 - val_decoder_loss: 0.0463 - val_classifier_acc: 0.9143 - val_decoder_mean_squared_error: 0.0463\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.2773 - classifier_loss: 0.2316 - decoder_loss: 0.0458 - classifier_acc: 0.9275 - decoder_mean_squared_error: 0.0458 - val_loss: 0.3109 - val_classifier_loss: 0.2640 - val_decoder_loss: 0.0443 - val_classifier_acc: 0.9179 - val_decoder_mean_squared_error: 0.0443\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.2383 - classifier_loss: 0.1945 - decoder_loss: 0.0437 - classifier_acc: 0.9399 - decoder_mean_squared_error: 0.0437 - val_loss: 0.2979 - val_classifier_loss: 0.2567 - val_decoder_loss: 0.0426 - val_classifier_acc: 0.9107 - val_decoder_mean_squared_error: 0.0426\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.2190 - classifier_loss: 0.1767 - decoder_loss: 0.0423 - classifier_acc: 0.9435 - decoder_mean_squared_error: 0.0423 - val_loss: 0.2724 - val_classifier_loss: 0.2327 - val_decoder_loss: 0.0401 - val_classifier_acc: 0.9321 - val_decoder_mean_squared_error: 0.0400\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1864 - classifier_loss: 0.1461 - decoder_loss: 0.0403 - classifier_acc: 0.9536 - decoder_mean_squared_error: 0.0404 - val_loss: 0.2771 - val_classifier_loss: 0.2387 - val_decoder_loss: 0.0386 - val_classifier_acc: 0.9232 - val_decoder_mean_squared_error: 0.0386\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1750 - classifier_loss: 0.1355 - decoder_loss: 0.0396 - classifier_acc: 0.9561 - decoder_mean_squared_error: 0.0396 - val_loss: 0.2824 - val_classifier_loss: 0.2405 - val_decoder_loss: 0.0397 - val_classifier_acc: 0.9286 - val_decoder_mean_squared_error: 0.0397\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1620 - classifier_loss: 0.1229 - decoder_loss: 0.0391 - classifier_acc: 0.9628 - decoder_mean_squared_error: 0.0391 - val_loss: 0.2460 - val_classifier_loss: 0.2064 - val_decoder_loss: 0.0370 - val_classifier_acc: 0.9402 - val_decoder_mean_squared_error: 0.0370\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1474 - classifier_loss: 0.1103 - decoder_loss: 0.0372 - classifier_acc: 0.9628 - decoder_mean_squared_error: 0.0372 - val_loss: 0.2645 - val_classifier_loss: 0.2265 - val_decoder_loss: 0.0354 - val_classifier_acc: 0.9348 - val_decoder_mean_squared_error: 0.0355\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1363 - classifier_loss: 0.0997 - decoder_loss: 0.0364 - classifier_acc: 0.9665 - decoder_mean_squared_error: 0.0364 - val_loss: 0.2493 - val_classifier_loss: 0.2148 - val_decoder_loss: 0.0351 - val_classifier_acc: 0.9393 - val_decoder_mean_squared_error: 0.0351\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1263 - classifier_loss: 0.0904 - decoder_loss: 0.0357 - classifier_acc: 0.9697 - decoder_mean_squared_error: 0.0356 - val_loss: 0.3010 - val_classifier_loss: 0.2617 - val_decoder_loss: 0.0354 - val_classifier_acc: 0.9250 - val_decoder_mean_squared_error: 0.0355\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1211 - classifier_loss: 0.0865 - decoder_loss: 0.0350 - classifier_acc: 0.9693 - decoder_mean_squared_error: 0.0350 - val_loss: 0.2692 - val_classifier_loss: 0.2307 - val_decoder_loss: 0.0350 - val_classifier_acc: 0.9402 - val_decoder_mean_squared_error: 0.0350\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1133 - classifier_loss: 0.0791 - decoder_loss: 0.0342 - classifier_acc: 0.9719 - decoder_mean_squared_error: 0.0342 - val_loss: 0.2956 - val_classifier_loss: 0.2635 - val_decoder_loss: 0.0332 - val_classifier_acc: 0.9375 - val_decoder_mean_squared_error: 0.0333\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1014 - classifier_loss: 0.0676 - decoder_loss: 0.0337 - classifier_acc: 0.9770 - decoder_mean_squared_error: 0.0337 - val_loss: 0.3233 - val_classifier_loss: 0.2877 - val_decoder_loss: 0.0335 - val_classifier_acc: 0.9312 - val_decoder_mean_squared_error: 0.0336\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.0980 - classifier_loss: 0.0649 - decoder_loss: 0.0331 - classifier_acc: 0.9773 - decoder_mean_squared_error: 0.0331 - val_loss: 0.3045 - val_classifier_loss: 0.2718 - val_decoder_loss: 0.0326 - val_classifier_acc: 0.9348 - val_decoder_mean_squared_error: 0.0327\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.0862 - classifier_loss: 0.0541 - decoder_loss: 0.0322 - classifier_acc: 0.9804 - decoder_mean_squared_error: 0.0322 - val_loss: 0.3171 - val_classifier_loss: 0.2825 - val_decoder_loss: 0.0318 - val_classifier_acc: 0.9295 - val_decoder_mean_squared_error: 0.0318\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.0944 - classifier_loss: 0.0617 - decoder_loss: 0.0327 - classifier_acc: 0.9788 - decoder_mean_squared_error: 0.0327 - val_loss: 0.3064 - val_classifier_loss: 0.2755 - val_decoder_loss: 0.0329 - val_classifier_acc: 0.9384 - val_decoder_mean_squared_error: 0.0330\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 2s 214us/sample - loss: 0.0931 - classifier_loss: 0.0607 - decoder_loss: 0.0324 - classifier_acc: 0.9781 - decoder_mean_squared_error: 0.0324 - val_loss: 0.3182 - val_classifier_loss: 0.2863 - val_decoder_loss: 0.0319 - val_classifier_acc: 0.9304 - val_decoder_mean_squared_error: 0.0319\n",
            "Performance on the validation set\n",
            "loss: 0.24\n",
            "classifier_loss: 0.20\n",
            "decoder_loss: 0.04\n",
            "classifier_acc: 0.94\n",
            "decoder_mean_squared_error: 0.04\n",
            "\n",
            "FOLD:  8\n",
            "Model: \"model_51\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_32 (InputLayer)           [(None, 784)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 784)          0           input_32[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_185 (Dense)               (None, 522)          409770      dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_186 (Dense)               (None, 261)          136503      dense_185[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_187 (Dense)               (None, 104)          27248       dense_186[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_188 (Dense)               (None, 34)           3570        dense_187[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_189 (Dense)               (None, 104)          3640        dense_188[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_190 (Dense)               (None, 261)          27405       dense_189[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_191 (Dense)               (None, 522)          136764      dense_190[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           385         dense_188[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          410032      dense_191[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,317\n",
            "Trainable params: 1,155,317\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 344us/sample - loss: 1.1381 - classifier_loss: 1.0500 - decoder_loss: 0.0870 - classifier_acc: 0.6565 - decoder_mean_squared_error: 0.0870 - val_loss: 0.5805 - val_classifier_loss: 0.5163 - val_decoder_loss: 0.0652 - val_classifier_acc: 0.8393 - val_decoder_mean_squared_error: 0.0652\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.5720 - classifier_loss: 0.5130 - decoder_loss: 0.0589 - classifier_acc: 0.8417 - decoder_mean_squared_error: 0.0589 - val_loss: 0.3907 - val_classifier_loss: 0.3351 - val_decoder_loss: 0.0554 - val_classifier_acc: 0.9054 - val_decoder_mean_squared_error: 0.0553\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.4168 - classifier_loss: 0.3647 - decoder_loss: 0.0516 - classifier_acc: 0.8872 - decoder_mean_squared_error: 0.0516 - val_loss: 0.3013 - val_classifier_loss: 0.2526 - val_decoder_loss: 0.0487 - val_classifier_acc: 0.9259 - val_decoder_mean_squared_error: 0.0488\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.3346 - classifier_loss: 0.2878 - decoder_loss: 0.0472 - classifier_acc: 0.9118 - decoder_mean_squared_error: 0.0472 - val_loss: 0.2954 - val_classifier_loss: 0.2514 - val_decoder_loss: 0.0444 - val_classifier_acc: 0.9187 - val_decoder_mean_squared_error: 0.0445\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.2755 - classifier_loss: 0.2309 - decoder_loss: 0.0449 - classifier_acc: 0.9296 - decoder_mean_squared_error: 0.0449 - val_loss: 0.2498 - val_classifier_loss: 0.2058 - val_decoder_loss: 0.0432 - val_classifier_acc: 0.9402 - val_decoder_mean_squared_error: 0.0432\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.2426 - classifier_loss: 0.1991 - decoder_loss: 0.0433 - classifier_acc: 0.9381 - decoder_mean_squared_error: 0.0433 - val_loss: 0.2403 - val_classifier_loss: 0.2009 - val_decoder_loss: 0.0405 - val_classifier_acc: 0.9402 - val_decoder_mean_squared_error: 0.0406\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.2074 - classifier_loss: 0.1660 - decoder_loss: 0.0416 - classifier_acc: 0.9474 - decoder_mean_squared_error: 0.0416 - val_loss: 0.2556 - val_classifier_loss: 0.2134 - val_decoder_loss: 0.0397 - val_classifier_acc: 0.9321 - val_decoder_mean_squared_error: 0.0397\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1925 - classifier_loss: 0.1520 - decoder_loss: 0.0407 - classifier_acc: 0.9518 - decoder_mean_squared_error: 0.0407 - val_loss: 0.2254 - val_classifier_loss: 0.1898 - val_decoder_loss: 0.0374 - val_classifier_acc: 0.9455 - val_decoder_mean_squared_error: 0.0375\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1691 - classifier_loss: 0.1298 - decoder_loss: 0.0392 - classifier_acc: 0.9579 - decoder_mean_squared_error: 0.0392 - val_loss: 0.2211 - val_classifier_loss: 0.1800 - val_decoder_loss: 0.0379 - val_classifier_acc: 0.9420 - val_decoder_mean_squared_error: 0.0380\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1643 - classifier_loss: 0.1255 - decoder_loss: 0.0389 - classifier_acc: 0.9585 - decoder_mean_squared_error: 0.0389 - val_loss: 0.2438 - val_classifier_loss: 0.2068 - val_decoder_loss: 0.0361 - val_classifier_acc: 0.9393 - val_decoder_mean_squared_error: 0.0362\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1475 - classifier_loss: 0.1097 - decoder_loss: 0.0376 - classifier_acc: 0.9662 - decoder_mean_squared_error: 0.0376 - val_loss: 0.2233 - val_classifier_loss: 0.1909 - val_decoder_loss: 0.0352 - val_classifier_acc: 0.9411 - val_decoder_mean_squared_error: 0.0353\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1383 - classifier_loss: 0.1017 - decoder_loss: 0.0365 - classifier_acc: 0.9667 - decoder_mean_squared_error: 0.0365 - val_loss: 0.2570 - val_classifier_loss: 0.2244 - val_decoder_loss: 0.0357 - val_classifier_acc: 0.9366 - val_decoder_mean_squared_error: 0.0357\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1231 - classifier_loss: 0.0871 - decoder_loss: 0.0358 - classifier_acc: 0.9706 - decoder_mean_squared_error: 0.0358 - val_loss: 0.2686 - val_classifier_loss: 0.2339 - val_decoder_loss: 0.0344 - val_classifier_acc: 0.9357 - val_decoder_mean_squared_error: 0.0344\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1181 - classifier_loss: 0.0832 - decoder_loss: 0.0352 - classifier_acc: 0.9723 - decoder_mean_squared_error: 0.0352 - val_loss: 0.2360 - val_classifier_loss: 0.2020 - val_decoder_loss: 0.0350 - val_classifier_acc: 0.9357 - val_decoder_mean_squared_error: 0.0349\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1112 - classifier_loss: 0.0763 - decoder_loss: 0.0349 - classifier_acc: 0.9729 - decoder_mean_squared_error: 0.0349 - val_loss: 0.2563 - val_classifier_loss: 0.2271 - val_decoder_loss: 0.0327 - val_classifier_acc: 0.9446 - val_decoder_mean_squared_error: 0.0326\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1084 - classifier_loss: 0.0744 - decoder_loss: 0.0342 - classifier_acc: 0.9733 - decoder_mean_squared_error: 0.0342 - val_loss: 0.2364 - val_classifier_loss: 0.2026 - val_decoder_loss: 0.0332 - val_classifier_acc: 0.9393 - val_decoder_mean_squared_error: 0.0331\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1059 - classifier_loss: 0.0715 - decoder_loss: 0.0343 - classifier_acc: 0.9757 - decoder_mean_squared_error: 0.0343 - val_loss: 0.2287 - val_classifier_loss: 0.1980 - val_decoder_loss: 0.0327 - val_classifier_acc: 0.9420 - val_decoder_mean_squared_error: 0.0327\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.0919 - classifier_loss: 0.0590 - decoder_loss: 0.0328 - classifier_acc: 0.9803 - decoder_mean_squared_error: 0.0328 - val_loss: 0.2324 - val_classifier_loss: 0.1984 - val_decoder_loss: 0.0318 - val_classifier_acc: 0.9429 - val_decoder_mean_squared_error: 0.0319\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 2s 216us/sample - loss: 0.0974 - classifier_loss: 0.0646 - decoder_loss: 0.0327 - classifier_acc: 0.9778 - decoder_mean_squared_error: 0.0327 - val_loss: 0.2416 - val_classifier_loss: 0.2072 - val_decoder_loss: 0.0323 - val_classifier_acc: 0.9455 - val_decoder_mean_squared_error: 0.0322\n",
            "Performance on the validation set\n",
            "loss: 0.24\n",
            "classifier_loss: 0.20\n",
            "decoder_loss: 0.04\n",
            "classifier_acc: 0.94\n",
            "decoder_mean_squared_error: 0.04\n",
            "\n",
            "FOLD:  9\n",
            "Model: \"model_53\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_33 (InputLayer)           [(None, 784)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 784)          0           input_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_192 (Dense)               (None, 522)          409770      dropout_55[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_193 (Dense)               (None, 261)          136503      dense_192[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_194 (Dense)               (None, 104)          27248       dense_193[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_195 (Dense)               (None, 34)           3570        dense_194[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_196 (Dense)               (None, 104)          3640        dense_195[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_197 (Dense)               (None, 261)          27405       dense_196[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_198 (Dense)               (None, 522)          136764      dense_197[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           385         dense_195[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          410032      dense_198[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,317\n",
            "Trainable params: 1,155,317\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 4s 360us/sample - loss: 1.1592 - classifier_loss: 1.0724 - decoder_loss: 0.0861 - classifier_acc: 0.6564 - decoder_mean_squared_error: 0.0862 - val_loss: 0.6823 - val_classifier_loss: 0.6143 - val_decoder_loss: 0.0626 - val_classifier_acc: 0.8125 - val_decoder_mean_squared_error: 0.0627\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.5532 - classifier_loss: 0.4970 - decoder_loss: 0.0567 - classifier_acc: 0.8453 - decoder_mean_squared_error: 0.0567 - val_loss: 0.4742 - val_classifier_loss: 0.4194 - val_decoder_loss: 0.0516 - val_classifier_acc: 0.8589 - val_decoder_mean_squared_error: 0.0517\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 79us/sample - loss: 0.4075 - classifier_loss: 0.3576 - decoder_loss: 0.0502 - classifier_acc: 0.8916 - decoder_mean_squared_error: 0.0502 - val_loss: 0.3937 - val_classifier_loss: 0.3495 - val_decoder_loss: 0.0469 - val_classifier_acc: 0.8920 - val_decoder_mean_squared_error: 0.0468\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.3351 - classifier_loss: 0.2874 - decoder_loss: 0.0475 - classifier_acc: 0.9116 - decoder_mean_squared_error: 0.0475 - val_loss: 0.3635 - val_classifier_loss: 0.3207 - val_decoder_loss: 0.0441 - val_classifier_acc: 0.8973 - val_decoder_mean_squared_error: 0.0441\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.2830 - classifier_loss: 0.2388 - decoder_loss: 0.0445 - classifier_acc: 0.9283 - decoder_mean_squared_error: 0.0445 - val_loss: 0.3215 - val_classifier_loss: 0.2792 - val_decoder_loss: 0.0412 - val_classifier_acc: 0.9080 - val_decoder_mean_squared_error: 0.0412\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.2440 - classifier_loss: 0.2013 - decoder_loss: 0.0427 - classifier_acc: 0.9378 - decoder_mean_squared_error: 0.0427 - val_loss: 0.3157 - val_classifier_loss: 0.2747 - val_decoder_loss: 0.0417 - val_classifier_acc: 0.9089 - val_decoder_mean_squared_error: 0.0417\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.2214 - classifier_loss: 0.1804 - decoder_loss: 0.0413 - classifier_acc: 0.9442 - decoder_mean_squared_error: 0.0413 - val_loss: 0.3231 - val_classifier_loss: 0.2844 - val_decoder_loss: 0.0408 - val_classifier_acc: 0.9062 - val_decoder_mean_squared_error: 0.0408\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1895 - classifier_loss: 0.1498 - decoder_loss: 0.0399 - classifier_acc: 0.9529 - decoder_mean_squared_error: 0.0399 - val_loss: 0.2996 - val_classifier_loss: 0.2622 - val_decoder_loss: 0.0386 - val_classifier_acc: 0.9187 - val_decoder_mean_squared_error: 0.0386\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1657 - classifier_loss: 0.1272 - decoder_loss: 0.0386 - classifier_acc: 0.9602 - decoder_mean_squared_error: 0.0386 - val_loss: 0.3336 - val_classifier_loss: 0.2960 - val_decoder_loss: 0.0388 - val_classifier_acc: 0.9134 - val_decoder_mean_squared_error: 0.0387\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1659 - classifier_loss: 0.1275 - decoder_loss: 0.0384 - classifier_acc: 0.9577 - decoder_mean_squared_error: 0.0384 - val_loss: 0.2800 - val_classifier_loss: 0.2395 - val_decoder_loss: 0.0370 - val_classifier_acc: 0.9259 - val_decoder_mean_squared_error: 0.0370\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1443 - classifier_loss: 0.1067 - decoder_loss: 0.0375 - classifier_acc: 0.9630 - decoder_mean_squared_error: 0.0375 - val_loss: 0.3033 - val_classifier_loss: 0.2687 - val_decoder_loss: 0.0368 - val_classifier_acc: 0.9187 - val_decoder_mean_squared_error: 0.0368\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1369 - classifier_loss: 0.1007 - decoder_loss: 0.0362 - classifier_acc: 0.9666 - decoder_mean_squared_error: 0.0362 - val_loss: 0.3072 - val_classifier_loss: 0.2747 - val_decoder_loss: 0.0360 - val_classifier_acc: 0.9214 - val_decoder_mean_squared_error: 0.0361\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1330 - classifier_loss: 0.0969 - decoder_loss: 0.0359 - classifier_acc: 0.9671 - decoder_mean_squared_error: 0.0359 - val_loss: 0.3294 - val_classifier_loss: 0.2997 - val_decoder_loss: 0.0354 - val_classifier_acc: 0.9143 - val_decoder_mean_squared_error: 0.0355\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 79us/sample - loss: 0.1210 - classifier_loss: 0.0859 - decoder_loss: 0.0352 - classifier_acc: 0.9723 - decoder_mean_squared_error: 0.0352 - val_loss: 0.3031 - val_classifier_loss: 0.2711 - val_decoder_loss: 0.0343 - val_classifier_acc: 0.9179 - val_decoder_mean_squared_error: 0.0342\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1116 - classifier_loss: 0.0777 - decoder_loss: 0.0339 - classifier_acc: 0.9742 - decoder_mean_squared_error: 0.0339 - val_loss: 0.2874 - val_classifier_loss: 0.2522 - val_decoder_loss: 0.0342 - val_classifier_acc: 0.9241 - val_decoder_mean_squared_error: 0.0342\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1084 - classifier_loss: 0.0746 - decoder_loss: 0.0338 - classifier_acc: 0.9743 - decoder_mean_squared_error: 0.0338 - val_loss: 0.3220 - val_classifier_loss: 0.2895 - val_decoder_loss: 0.0325 - val_classifier_acc: 0.9232 - val_decoder_mean_squared_error: 0.0325\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.1060 - classifier_loss: 0.0725 - decoder_loss: 0.0335 - classifier_acc: 0.9763 - decoder_mean_squared_error: 0.0335 - val_loss: 0.2866 - val_classifier_loss: 0.2525 - val_decoder_loss: 0.0333 - val_classifier_acc: 0.9277 - val_decoder_mean_squared_error: 0.0333\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.0957 - classifier_loss: 0.0632 - decoder_loss: 0.0324 - classifier_acc: 0.9792 - decoder_mean_squared_error: 0.0324 - val_loss: 0.3423 - val_classifier_loss: 0.3091 - val_decoder_loss: 0.0320 - val_classifier_acc: 0.9170 - val_decoder_mean_squared_error: 0.0319\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 74us/sample - loss: 0.0975 - classifier_loss: 0.0655 - decoder_loss: 0.0319 - classifier_acc: 0.9781 - decoder_mean_squared_error: 0.0320 - val_loss: 0.2964 - val_classifier_loss: 0.2621 - val_decoder_loss: 0.0320 - val_classifier_acc: 0.9330 - val_decoder_mean_squared_error: 0.0320\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 2s 224us/sample - loss: 0.0913 - classifier_loss: 0.0599 - decoder_loss: 0.0316 - classifier_acc: 0.9777 - decoder_mean_squared_error: 0.0316 - val_loss: 0.3079 - val_classifier_loss: 0.2753 - val_decoder_loss: 0.0313 - val_classifier_acc: 0.9259 - val_decoder_mean_squared_error: 0.0313\n",
            "Performance on the validation set\n",
            "loss: 0.24\n",
            "classifier_loss: 0.20\n",
            "decoder_loss: 0.04\n",
            "classifier_acc: 0.94\n",
            "decoder_mean_squared_error: 0.04\n",
            "\n",
            "FOLD:  10\n",
            "Model: \"model_55\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_34 (InputLayer)           [(None, 784)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 784)          0           input_34[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_199 (Dense)               (None, 522)          409770      dropout_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_200 (Dense)               (None, 261)          136503      dense_199[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_201 (Dense)               (None, 104)          27248       dense_200[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_202 (Dense)               (None, 34)           3570        dense_201[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_203 (Dense)               (None, 104)          3640        dense_202[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_204 (Dense)               (None, 261)          27405       dense_203[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_205 (Dense)               (None, 522)          136764      dense_204[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           385         dense_202[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          410032      dense_205[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,317\n",
            "Trainable params: 1,155,317\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 4s 365us/sample - loss: 1.0986 - classifier_loss: 1.0107 - decoder_loss: 0.0866 - classifier_acc: 0.6828 - decoder_mean_squared_error: 0.0867 - val_loss: 0.6417 - val_classifier_loss: 0.5765 - val_decoder_loss: 0.0631 - val_classifier_acc: 0.8268 - val_decoder_mean_squared_error: 0.0632\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.5426 - classifier_loss: 0.4847 - decoder_loss: 0.0576 - classifier_acc: 0.8487 - decoder_mean_squared_error: 0.0577 - val_loss: 0.4417 - val_classifier_loss: 0.3904 - val_decoder_loss: 0.0514 - val_classifier_acc: 0.8804 - val_decoder_mean_squared_error: 0.0513\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.4074 - classifier_loss: 0.3565 - decoder_loss: 0.0507 - classifier_acc: 0.8915 - decoder_mean_squared_error: 0.0507 - val_loss: 0.3945 - val_classifier_loss: 0.3456 - val_decoder_loss: 0.0482 - val_classifier_acc: 0.8982 - val_decoder_mean_squared_error: 0.0482\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 79us/sample - loss: 0.3308 - classifier_loss: 0.2840 - decoder_loss: 0.0467 - classifier_acc: 0.9085 - decoder_mean_squared_error: 0.0467 - val_loss: 0.3299 - val_classifier_loss: 0.2901 - val_decoder_loss: 0.0438 - val_classifier_acc: 0.9152 - val_decoder_mean_squared_error: 0.0438\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 79us/sample - loss: 0.2706 - classifier_loss: 0.2266 - decoder_loss: 0.0438 - classifier_acc: 0.9287 - decoder_mean_squared_error: 0.0438 - val_loss: 0.3176 - val_classifier_loss: 0.2758 - val_decoder_loss: 0.0439 - val_classifier_acc: 0.9134 - val_decoder_mean_squared_error: 0.0439\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.2400 - classifier_loss: 0.1980 - decoder_loss: 0.0423 - classifier_acc: 0.9368 - decoder_mean_squared_error: 0.0423 - val_loss: 0.2886 - val_classifier_loss: 0.2455 - val_decoder_loss: 0.0411 - val_classifier_acc: 0.9161 - val_decoder_mean_squared_error: 0.0410\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.2040 - classifier_loss: 0.1633 - decoder_loss: 0.0407 - classifier_acc: 0.9470 - decoder_mean_squared_error: 0.0407 - val_loss: 0.2878 - val_classifier_loss: 0.2505 - val_decoder_loss: 0.0380 - val_classifier_acc: 0.9286 - val_decoder_mean_squared_error: 0.0381\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.1973 - classifier_loss: 0.1579 - decoder_loss: 0.0395 - classifier_acc: 0.9487 - decoder_mean_squared_error: 0.0395 - val_loss: 0.2920 - val_classifier_loss: 0.2555 - val_decoder_loss: 0.0370 - val_classifier_acc: 0.9259 - val_decoder_mean_squared_error: 0.0369\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.1746 - classifier_loss: 0.1367 - decoder_loss: 0.0376 - classifier_acc: 0.9563 - decoder_mean_squared_error: 0.0376 - val_loss: 0.3091 - val_classifier_loss: 0.2765 - val_decoder_loss: 0.0366 - val_classifier_acc: 0.9223 - val_decoder_mean_squared_error: 0.0366\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 79us/sample - loss: 0.1528 - classifier_loss: 0.1164 - decoder_loss: 0.0364 - classifier_acc: 0.9618 - decoder_mean_squared_error: 0.0364 - val_loss: 0.2723 - val_classifier_loss: 0.2403 - val_decoder_loss: 0.0349 - val_classifier_acc: 0.9321 - val_decoder_mean_squared_error: 0.0348\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1400 - classifier_loss: 0.1043 - decoder_loss: 0.0357 - classifier_acc: 0.9637 - decoder_mean_squared_error: 0.0357 - val_loss: 0.2926 - val_classifier_loss: 0.2562 - val_decoder_loss: 0.0347 - val_classifier_acc: 0.9268 - val_decoder_mean_squared_error: 0.0346\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1426 - classifier_loss: 0.1064 - decoder_loss: 0.0363 - classifier_acc: 0.9633 - decoder_mean_squared_error: 0.0363 - val_loss: 0.2695 - val_classifier_loss: 0.2354 - val_decoder_loss: 0.0358 - val_classifier_acc: 0.9250 - val_decoder_mean_squared_error: 0.0359\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1221 - classifier_loss: 0.0875 - decoder_loss: 0.0345 - classifier_acc: 0.9698 - decoder_mean_squared_error: 0.0345 - val_loss: 0.2920 - val_classifier_loss: 0.2605 - val_decoder_loss: 0.0343 - val_classifier_acc: 0.9295 - val_decoder_mean_squared_error: 0.0343\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1191 - classifier_loss: 0.0848 - decoder_loss: 0.0342 - classifier_acc: 0.9705 - decoder_mean_squared_error: 0.0342 - val_loss: 0.2958 - val_classifier_loss: 0.2634 - val_decoder_loss: 0.0330 - val_classifier_acc: 0.9277 - val_decoder_mean_squared_error: 0.0330\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 76us/sample - loss: 0.1055 - classifier_loss: 0.0723 - decoder_loss: 0.0332 - classifier_acc: 0.9737 - decoder_mean_squared_error: 0.0332 - val_loss: 0.2924 - val_classifier_loss: 0.2615 - val_decoder_loss: 0.0320 - val_classifier_acc: 0.9259 - val_decoder_mean_squared_error: 0.0321\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.1056 - classifier_loss: 0.0728 - decoder_loss: 0.0328 - classifier_acc: 0.9733 - decoder_mean_squared_error: 0.0328 - val_loss: 0.2813 - val_classifier_loss: 0.2482 - val_decoder_loss: 0.0321 - val_classifier_acc: 0.9312 - val_decoder_mean_squared_error: 0.0320\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 75us/sample - loss: 0.1068 - classifier_loss: 0.0744 - decoder_loss: 0.0323 - classifier_acc: 0.9750 - decoder_mean_squared_error: 0.0324 - val_loss: 0.2902 - val_classifier_loss: 0.2556 - val_decoder_loss: 0.0318 - val_classifier_acc: 0.9312 - val_decoder_mean_squared_error: 0.0317\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 77us/sample - loss: 0.0955 - classifier_loss: 0.0642 - decoder_loss: 0.0315 - classifier_acc: 0.9777 - decoder_mean_squared_error: 0.0315 - val_loss: 0.2960 - val_classifier_loss: 0.2684 - val_decoder_loss: 0.0315 - val_classifier_acc: 0.9312 - val_decoder_mean_squared_error: 0.0315\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.0961 - classifier_loss: 0.0643 - decoder_loss: 0.0318 - classifier_acc: 0.9754 - decoder_mean_squared_error: 0.0318 - val_loss: 0.2759 - val_classifier_loss: 0.2443 - val_decoder_loss: 0.0310 - val_classifier_acc: 0.9330 - val_decoder_mean_squared_error: 0.0311\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 80us/sample - loss: 0.0949 - classifier_loss: 0.0639 - decoder_loss: 0.0312 - classifier_acc: 0.9778 - decoder_mean_squared_error: 0.0312 - val_loss: 0.2773 - val_classifier_loss: 0.2467 - val_decoder_loss: 0.0313 - val_classifier_acc: 0.9286 - val_decoder_mean_squared_error: 0.0313\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 78us/sample - loss: 0.0940 - classifier_loss: 0.0630 - decoder_loss: 0.0310 - classifier_acc: 0.9790 - decoder_mean_squared_error: 0.0310 - val_loss: 0.3141 - val_classifier_loss: 0.2876 - val_decoder_loss: 0.0303 - val_classifier_acc: 0.9259 - val_decoder_mean_squared_error: 0.0303\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 2s 230us/sample - loss: 0.0795 - classifier_loss: 0.0495 - decoder_loss: 0.0301 - classifier_acc: 0.9832 - decoder_mean_squared_error: 0.0301 - val_loss: 0.2919 - val_classifier_loss: 0.2614 - val_decoder_loss: 0.0301 - val_classifier_acc: 0.9304 - val_decoder_mean_squared_error: 0.0301\n",
            "Performance on the validation set\n",
            "loss: 0.24\n",
            "classifier_loss: 0.20\n",
            "decoder_loss: 0.03\n",
            "classifier_acc: 0.94\n",
            "decoder_mean_squared_error: 0.03\n",
            "\n",
            "Overall  loss :  0.24 +- 0.01\n",
            "Overall  classifier_loss :  0.20 +- 0.01\n",
            "Overall  decoder_loss :  0.03 +- 0.00\n",
            "Overall  classifier_acc :  0.94 +- 0.00\n",
            "Overall  decoder_mean_squared_error :  0.03 +- 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEPuhED3n88F",
        "colab_type": "text"
      },
      "source": [
        "## Plot decoder reconstructed images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL6SfF95oLXm",
        "colab_type": "code",
        "outputId": "c8109d49-0665-4f0b-c1e8-3b0191cda76b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "_, decoded_imgs = autoencoder_nn.predict(x_test)\n",
        "\n",
        "n=10\n",
        "plt.figure(figsize=(40, 4))\n",
        "for i in range(n):\n",
        "    # display original images\n",
        "    ax = plt.subplot(3, 20, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "    # display reconstructed images\n",
        "    ax = plt.subplot(3, 20, 2*20 +i+ 1)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "  \n",
        "plt.show()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABEYAAADrCAYAAABkdZM2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 2880x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzcSXKmYoNCl",
        "colab_type": "text"
      },
      "source": [
        "## Plot performances plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlkzqJw-d2FL",
        "colab_type": "code",
        "outputId": "310c7a0d-ed97-447c-b00b-1241479699b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "x_plot = list(range(1, len(best_history.history['val_classifier_acc']) + 1))\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Decoder loss')\n",
        "    plt.plot(x_plot, history.history['decoder_loss'])\n",
        "    plt.plot(x_plot, history.history['val_decoder_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Classifier loss')\n",
        "    plt.plot(x_plot, history.history['classifier_loss'])\n",
        "    plt.plot(x_plot, history.history['val_classifier_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Decoder MSE')\n",
        "    plt.plot(x_plot, history.history['decoder_mean_squared_error'])\n",
        "    plt.plot(x_plot, history.history['val_decoder_mean_squared_error'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Classifier accuracy')\n",
        "    plt.plot(x_plot, history.history['classifier_acc'])\n",
        "    plt.plot(x_plot, history.history['val_classifier_acc'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(best_history)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAEYCAYAAABBfQDEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAEYCAYAAABBfQDEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlWiArJRSSimlVCUaICullFJKKVXJ/wNQ5Y9INM52qwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J29MjpFPktgI",
        "colab_type": "code",
        "outputId": "866892f7-035a-433a-c464-fa44b3f29da5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "labels, _ = autoencoder_nn.predict(x_val)\n",
        "print(classification_report(labels_val[0], np.argmax(labels, axis=1)))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.96       273\n",
            "           1       0.93      0.94      0.93       247\n",
            "           2       0.94      0.93      0.93       292\n",
            "           3       0.95      0.98      0.97       251\n",
            "           4       0.96      0.94      0.95       262\n",
            "           5       0.90      0.91      0.91       239\n",
            "           6       0.90      0.89      0.89       261\n",
            "           7       0.96      0.97      0.96       269\n",
            "           8       0.94      0.95      0.95       266\n",
            "           9       0.92      0.92      0.92       279\n",
            "          10       0.94      0.95      0.94       161\n",
            "\n",
            "    accuracy                           0.94      2800\n",
            "   macro avg       0.94      0.94      0.94      2800\n",
            "weighted avg       0.94      0.94      0.94      2800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBJHphr-w3UA",
        "colab_type": "text"
      },
      "source": [
        "# Tied autoencoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWaO4R0TzPzt",
        "colab_type": "code",
        "outputId": "24d21f34-868e-4f0e-fbbb-9527f80e0491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "monitor = 'val_loss'\n",
        "early_stopping = EarlyStopping(monitor=monitor, patience=10, min_delta=0.001, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=10, verbose=1)\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "model_type = 'autoencoder'\n",
        "if model_type == 'autoencoder_nn':\n",
        "  labels_train = [y_train, x_train]\n",
        "  labels_val = [y_val, x_val]\n",
        "elif model_type == 'nn':\n",
        "  labels_train = y_train\n",
        "  labels_val = y_val\n",
        "elif model_type == 'autoencoder':\n",
        "  labels_train = x_train\n",
        "  labels_val = x_val\n",
        "\n",
        "autoencoder, encoder = get_model(\n",
        "    model_type=model_type, \n",
        "    tied=True,\n",
        "    divide_by=[1.5, 2, 2.5, 3],\n",
        "    hidden_first=512, \n",
        "    hidden_last=32, \n",
        "    noise_type=None, \n",
        "    dropout=False, \n",
        "    batch_norm=False, \n",
        "    standard=False, \n",
        "    verbose=True\n",
        ")\n",
        "history_autoencoder = autoencoder.fit(\n",
        "    x_train, \n",
        "    labels_train,\n",
        "    validation_data=(x_val, labels_val), \n",
        "    epochs=300, \n",
        "    batch_size=128, \n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'dense_221/kernel:0' shape=(104, 34) dtype=float32>\n",
            "dense_220\n",
            "<tf.Variable 'dense_220/kernel:0' shape=(261, 104) dtype=float32>\n",
            "dense_219\n",
            "<tf.Variable 'dense_219/kernel:0' shape=(522, 261) dtype=float32>\n",
            "<tf.Variable 'dense_218/kernel:0' shape=(784, 522) dtype=float32>\n",
            "Model: \"model_63\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_38 (InputLayer)        [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_218 (Dense)            (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dense_219 (Dense)            (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dense_220 (Dense)            (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dense_221 (Dense)            (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dense_transpose_29 (DenseTra (None, 104)               3674      \n",
            "_________________________________________________________________\n",
            "dense_transpose_30 (DenseTra (None, 261)               27509     \n",
            "_________________________________________________________________\n",
            "dense_transpose_31 (DenseTra (None, 522)               137025    \n",
            "_________________________________________________________________\n",
            "decoder (DenseTranspose)     (None, 784)               410554    \n",
            "=================================================================\n",
            "Total params: 578,762\n",
            "Trainable params: 578,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 11200 samples, validate on 2800 samples\n",
            "Epoch 1/300\n",
            "11200/11200 [==============================] - 4s 357us/sample - loss: 0.0742 - mean_squared_error: 0.0742 - val_loss: 0.0431 - val_mean_squared_error: 0.0431\n",
            "Epoch 2/300\n",
            "11200/11200 [==============================] - 1s 69us/sample - loss: 0.0346 - mean_squared_error: 0.0346 - val_loss: 0.0284 - val_mean_squared_error: 0.0284\n",
            "Epoch 3/300\n",
            "11200/11200 [==============================] - 1s 70us/sample - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0226 - val_mean_squared_error: 0.0226\n",
            "Epoch 4/300\n",
            "11200/11200 [==============================] - 1s 70us/sample - loss: 0.0210 - mean_squared_error: 0.0210 - val_loss: 0.0202 - val_mean_squared_error: 0.0202\n",
            "Epoch 5/300\n",
            "11200/11200 [==============================] - 1s 70us/sample - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0182 - val_mean_squared_error: 0.0182\n",
            "Epoch 6/300\n",
            "11200/11200 [==============================] - 1s 70us/sample - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0168 - val_mean_squared_error: 0.0168\n",
            "Epoch 7/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.0158 - val_mean_squared_error: 0.0158\n",
            "Epoch 8/300\n",
            "11200/11200 [==============================] - 1s 71us/sample - loss: 0.0148 - mean_squared_error: 0.0148 - val_loss: 0.0152 - val_mean_squared_error: 0.0152\n",
            "Epoch 9/300\n",
            "11200/11200 [==============================] - 1s 71us/sample - loss: 0.0140 - mean_squared_error: 0.0140 - val_loss: 0.0144 - val_mean_squared_error: 0.0144\n",
            "Epoch 10/300\n",
            "11200/11200 [==============================] - 1s 69us/sample - loss: 0.0133 - mean_squared_error: 0.0133 - val_loss: 0.0137 - val_mean_squared_error: 0.0137\n",
            "Epoch 11/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0129 - mean_squared_error: 0.0129 - val_loss: 0.0134 - val_mean_squared_error: 0.0134\n",
            "Epoch 12/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0122 - mean_squared_error: 0.0122 - val_loss: 0.0129 - val_mean_squared_error: 0.0129\n",
            "Epoch 13/300\n",
            "11200/11200 [==============================] - 1s 70us/sample - loss: 0.0118 - mean_squared_error: 0.0118 - val_loss: 0.0124 - val_mean_squared_error: 0.0124\n",
            "Epoch 14/300\n",
            "11200/11200 [==============================] - 1s 70us/sample - loss: 0.0113 - mean_squared_error: 0.0113 - val_loss: 0.0121 - val_mean_squared_error: 0.0121\n",
            "Epoch 15/300\n",
            "11200/11200 [==============================] - 1s 74us/sample - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0121 - val_mean_squared_error: 0.0121\n",
            "Epoch 16/300\n",
            "11200/11200 [==============================] - 1s 69us/sample - loss: 0.0107 - mean_squared_error: 0.0107 - val_loss: 0.0115 - val_mean_squared_error: 0.0115\n",
            "Epoch 17/300\n",
            "11200/11200 [==============================] - 1s 69us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - val_loss: 0.0113 - val_mean_squared_error: 0.0113\n",
            "Epoch 18/300\n",
            "11200/11200 [==============================] - 1s 70us/sample - loss: 0.0102 - mean_squared_error: 0.0102 - val_loss: 0.0112 - val_mean_squared_error: 0.0112\n",
            "Epoch 19/300\n",
            "11200/11200 [==============================] - 1s 69us/sample - loss: 0.0099 - mean_squared_error: 0.0099 - val_loss: 0.0112 - val_mean_squared_error: 0.0112\n",
            "Epoch 20/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0107 - val_mean_squared_error: 0.0107\n",
            "Epoch 21/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0095 - mean_squared_error: 0.0095 - val_loss: 0.0107 - val_mean_squared_error: 0.0107\n",
            "Epoch 22/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0093 - mean_squared_error: 0.0093 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
            "Epoch 23/300\n",
            "11200/11200 [==============================] - 1s 69us/sample - loss: 0.0090 - mean_squared_error: 0.0090 - val_loss: 0.0107 - val_mean_squared_error: 0.0107\n",
            "Epoch 24/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0090 - mean_squared_error: 0.0090 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
            "Epoch 25/300\n",
            "11200/11200 [==============================] - 1s 70us/sample - loss: 0.0087 - mean_squared_error: 0.0087 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
            "Epoch 26/300\n",
            "11200/11200 [==============================] - 1s 70us/sample - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
            "Epoch 27/300\n",
            "11200/11200 [==============================] - 1s 70us/sample - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0099 - val_mean_squared_error: 0.0099\n",
            "Epoch 28/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
            "Epoch 29/300\n",
            "11200/11200 [==============================] - 1s 71us/sample - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0099 - val_mean_squared_error: 0.0099\n",
            "Epoch 30/300\n",
            "11200/11200 [==============================] - 2s 217us/sample - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0098 - val_mean_squared_error: 0.0098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfVizLNjzT2r",
        "colab_type": "code",
        "outputId": "f6b9563e-87e7-4722-f92e-9485ca45cfe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# print accuracy\n",
        "x_plot = list(range(1, len(history_autoencoder.history['val_loss']) + 1))\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(x_plot, history.history['loss'])\n",
        "    plt.plot(x_plot, history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "plot_history(history_autoencoder)\n",
        "\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "n=10\n",
        "plt.figure(figsize=(40, 4))\n",
        "for i in range(n):\n",
        "    # display original images\n",
        "    ax = plt.subplot(3, 20, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "    # display reconstructed images\n",
        "    ax = plt.subplot(3, 20, 2*20 +i+ 1)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "  \n",
        "plt.show()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCGEObYdV1QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABEYAAADrCAYAAABkdZM2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 2880x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9pEvVhE-MuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}