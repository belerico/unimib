{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/belerico/text/blob/master/advanced%20machine%20learning/assignment%202/assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QBIOBRVd70i",
        "colab_type": "text"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu01qI83ribP",
        "colab_type": "code",
        "outputId": "590c1e28-5bfa-488a-b44f-d6305b39ceb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!pip install wget\n",
        "\n",
        "import wget\n",
        "\n",
        "wget.download('https://github.com/belerico/unimib/raw/master/advanced machine learning/assignment 2/data/x_test.obj', './x_test.obj')\n",
        "wget.download('https://github.com/belerico/unimib/raw/master/advanced machine learning/assignment 2/data/y_train.obj', './y_train.obj')\n",
        "wget.download('https://github.com/belerico/unimib/raw/master/advanced machine learning/assignment 2/data/x_train.obj', './x_train.obj')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=f37c89a9505dc1c5e53ee54431836d5e1b0e12e96937ad77323ebea2666f867e\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./x_train.obj'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhAQNTMEvmbM",
        "colab_type": "code",
        "outputId": "8414d87d-a469-4d2e-ce63-e66f3b8aec3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import utils\n",
        "\n",
        "with open('./x_train.obj', 'rb') as f:\n",
        "  dataset = pickle.load(f)\n",
        "  dataset = dataset.astype('float32') / 255.\n",
        "  dataset = dataset.reshape(dataset.shape[0], -1)\n",
        "\n",
        "with open('./x_test.obj', 'rb') as f:\n",
        "  x_test = pickle.load(f)\n",
        "  x_test = x_test.astype('float32') / 255.\n",
        "  x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "with open('./y_train.obj', 'rb') as f:\n",
        "  y_dataset = pickle.load(f)\n",
        "  # Label from 0 to 11\n",
        "  y_dataset -= 16\n",
        "  # y_dataset = utils.to_categorical(y_dataset)\n",
        "\n",
        "N_IMG = dataset.shape[0]\n",
        "N_CLASSES = 11\n",
        "IMG_SHAPE = dataset.shape[1]\n",
        "\n",
        "# Split train and val\n",
        "import random\n",
        "\n",
        "indexes = np.arange(N_IMG)\n",
        "# random.seed(42)\n",
        "random.shuffle(indexes)\n",
        "train_val_split = int(N_IMG * 0.8)\n",
        "\n",
        "x_train = dataset[indexes[:train_val_split], :]\n",
        "y_train = y_dataset[indexes[:train_val_split]]\n",
        "\n",
        "x_val = dataset[indexes[train_val_split:], :]\n",
        "y_val = y_dataset[indexes[train_val_split:]]\n",
        "\n",
        "# Standardize to get gaussian\n",
        "\n",
        "mean = np.mean(x_train)\n",
        "std = np.std(x_train)\n",
        "\n",
        "print(f\"Mean {mean} and std {std}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean 0.17235195636749268 and std 0.33112195134162903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIN7SjkDpr5w",
        "colab_type": "text"
      },
      "source": [
        "# Data info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhyxtlpLpuRU",
        "colab_type": "code",
        "outputId": "5f572704-1fe6-4fc1-f66b-c072915666a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "from collections import Counter\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print('train size:', len(dataset))\n",
        "print('test size:', len(x_test))\n",
        "data_distribution = sorted(Counter(y_dataset).items())\n",
        "print('labels distribution:', data_distribution)\n",
        "x = np.arange(len(data_distribution))\n",
        "plt.bar(x, height=[val for _, val in data_distribution])\n",
        "plt.xticks(x, ['p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size: 14000\n",
            "test size: 8800\n",
            "labels distribution: [(0, 1295), (1, 1265), (2, 1346), (3, 1329), (4, 1336), (5, 1297), (6, 1269), (7, 1327), (8, 1322), (9, 1321), (10, 893)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD6CAYAAABNu5eFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IeGz6EQs1Tm",
        "colab_type": "text"
      },
      "source": [
        "## Custom layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsfOS1ubs0Mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class SaltPepperNoise(layers.Layer):\n",
        "  \n",
        "  def __init__(self, ratio=0.5, **kwargs):\n",
        "    super(SaltPepperNoise, self).__init__(**kwargs)\n",
        "    self.ratio = ratio\n",
        "\n",
        "  # NOTE: this is the definition of the call method of custom layer class (i.e. SaltAndPepper)\n",
        "  def call(self, inputs, training=None):\n",
        "    def noised():\n",
        "      shp = K.shape(inputs)[1:] # input shape\n",
        "      mask_select = K.random_binomial(shape=shp, p=self.ratio)\n",
        "      mask_noise = K.random_binomial(shape=shp, p=0.5) # salt and pepper have the same chance\n",
        "      out = inputs * (1-mask_select) + mask_noise * mask_select\n",
        "      return out\n",
        "    \n",
        "    return K.in_train_phase(noised(), inputs, training=training)\n",
        "  \n",
        "  \n",
        "class StandardizeLayer(layers.Layer):\n",
        "  \n",
        "  def __init__(self, mean, std, **kwargs):\n",
        "    super(StandardizeLayer, self).__init__(**kwargs)\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "     return (inputs - self.mean) / self.std\n",
        "    \n",
        "    \n",
        "class DenseTranspose(layers.Layer):\n",
        "  \n",
        "  def __init__(self, dense, activation=None, **kwargs):\n",
        "    self.dense = dense\n",
        "    self.activation = tf.keras.activations.get(activation)\n",
        "    super().__init__(**kwargs)\n",
        "  \n",
        "  def build(self, batch_input_shape):\n",
        "    print(self.dense.weights[0])\n",
        "    self.non_trainable_weights.append(self.dense.weights[0])\n",
        "    self.biases = self.add_weight(name='bias', initializer='zeros',\n",
        "                                 shape=[self.dense.input_shape[-1]])\n",
        "    super().build(batch_input_shape)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    z = inputs @ K.transpose(self.dense.weights[0]) \n",
        "    return self.activation(z + self.biases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JfAZwCnwl5R",
        "colab_type": "text"
      },
      "source": [
        "# Models and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTeBIMf0zoWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "  return K.sqrt(losses.mse(y_true, y_pred))\n",
        "\n",
        "\n",
        "def tanh_crossentropy(y_true, y_pred):\n",
        "  return -0.5 * ((1 - y_true) * K.log(1 - y_pred) + (1 + y_true) * K.log(1 + y_pred))\n",
        "\n",
        "\n",
        "def get_model(\n",
        "    model_type='nn',\n",
        "    tied=False,\n",
        "    divide_by=2,\n",
        "    hidden_first=256,\n",
        "    hidden_last=64,\n",
        "    dropout=False, \n",
        "    dropout_rate=0.3,\n",
        "    batch_norm=False, \n",
        "    noise_type=None,\n",
        "    salt_pepper_noise_ratio=0.3, \n",
        "    gaussian_noise_std=0.5,\n",
        "    standard=False,\n",
        "    verbose=True\n",
        "):\n",
        "  \n",
        "  input_layer = layers.Input(shape=(IMG_SHAPE, ))\n",
        "  if model_type == 'autoencoder' or model_type == 'autoencoder_nn':\n",
        "    if noise_type is not None:\n",
        "      # noisy layer\n",
        "      if noise_type == 'gaussian':  # Gaussian\n",
        "        noise_layer = layers.GaussianNoise(gaussian_noise_std)(input_layer)\n",
        "      elif noise_type == 'saltpepper':  # Salt and pepper\n",
        "        noise_layer = SaltPepperNoise(ratio=salt_pepper_noise_ratio)(input_layer)\n",
        "      if standard:\n",
        "        noise_layer = StandardizeLayer(mean, std)(noise_layer)\n",
        "      if isinstance(divide_by, list) and divide_by != []:\n",
        "        dim = int(IMG_SHAPE / divide_by[0])\n",
        "        dims = [dim]\n",
        "        encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(noise_layer)\n",
        "      else:\n",
        "        encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(noise_layer)\n",
        "    elif standard:\n",
        "      encode = StandardizeLayer(mean, std)(input_layer)\n",
        "      if dropout:\n",
        "          encode = layers.Dropout(dropout_rate)(encode) \n",
        "    else:\n",
        "      if dropout:\n",
        "        encode = layers.Dropout(dropout_rate)(input_layer)\n",
        "        if isinstance(divide_by, list) and divide_by != []:\n",
        "          dim = int(IMG_SHAPE / divide_by[0])\n",
        "          dims = [dim]\n",
        "          encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "        else:\n",
        "          encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "      else:\n",
        "        if isinstance(divide_by, list) and divide_by != []:\n",
        "          dim = int(IMG_SHAPE / divide_by[0])\n",
        "          dims = [dim]\n",
        "          encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(input_layer)\n",
        "        else:\n",
        "          encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(input_layer)\n",
        "\n",
        "    # encoder\n",
        "    if isinstance(divide_by, list) and divide_by != []:\n",
        "      # saved_hidden_first = hidden_first\n",
        "      # if noise_type or standard:\n",
        "      #  encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "      i = 1\n",
        "      while i < len(divide_by) and int(dim / divide_by[i]) > hidden_last:\n",
        "        dim = int(dim / divide_by[i])\n",
        "        dims.insert(0, dim)\n",
        "        encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "        if batch_norm:\n",
        "          encode = layers.BatchNormalization()(encode)\n",
        "        i += 1\n",
        "    else:\n",
        "      saved_hidden_first = hidden_first\n",
        "      # if noise_type or standard:\n",
        "      #  encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "      while hidden_first > hidden_last * divide_by:\n",
        "        hidden_first = int(hidden_first / divide_by)\n",
        "        encode = layers.Dense(hidden_first, activation=\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "        if batch_norm:\n",
        "          encode = layers.BatchNormalization()(encode)\n",
        "    encode = layers.Dense(hidden_last, activation =\"relu\", kernel_initializer=\"he_normal\", name='encoder')(encode)\n",
        "    encoder = Model(inputs=input_layer, outputs=encode)   \n",
        "    \n",
        "    # decoder\n",
        "    if tied:\n",
        "      i = len(encoder.layers) - 2\n",
        "      end = 1 if not dropout else 2\n",
        "      decode = DenseTranspose(encoder.layers[-1], activation='relu')(encode) \n",
        "      while i > end:\n",
        "        print(encoder.layers[i].name)\n",
        "        if 'dense' in encoder.layers[i].name:\n",
        "          decode = DenseTranspose(encoder.layers[i], activation='relu')(decode) \n",
        "        i -= 1\n",
        "      decode = DenseTranspose(encoder.layers[i], activation='sigmoid', name='decoder')(decode)\n",
        "    else:\n",
        "      if isinstance(divide_by, list) and divide_by != []:\n",
        "        decode = layers.Dense(hidden_last, activation='relu', kernel_initializer=\"he_normal\")(encode) \n",
        "        for dim in dims:\n",
        "          decode = layers.Dense(dim, activation='relu', kernel_initializer=\"he_normal\")(decode) \n",
        "        decode = layers.Dense(IMG_SHAPE, activation='sigmoid', name='decoder')(decode)\n",
        "      else:\n",
        "        decode = layers.Dense(hidden_last, activation='relu', kernel_initializer=\"he_normal\")(encode) \n",
        "        while hidden_first > hidden_last:\n",
        "          hidden_last *= divide_by\n",
        "          decode = layers.Dense(hidden_last, activation='relu', kernel_initializer=\"he_normal\")(decode) \n",
        "        decode = layers.Dense(IMG_SHAPE, activation='sigmoid', name='decoder')(decode)\n",
        "    \n",
        "    outputs = [decode]\n",
        "    if model_type == 'autoencoder_nn':\n",
        "      # classifier\n",
        "      classifier = layers.Dense(N_CLASSES, activation=\"softmax\", name='classifier')(encode)\n",
        "      outputs.insert(0, classifier)\n",
        "    \n",
        "    # model\n",
        "    model = Model(inputs=input_layer, outputs=outputs)\n",
        "    \n",
        "    if len(outputs) > 1:\n",
        "      loss = [\"sparse_categorical_crossentropy\", 'mse']\n",
        "      metrics = {'classifier': 'accuracy', 'decoder': 'mse'}\n",
        "    else:\n",
        "      loss = 'mse'\n",
        "      metrics = {'decoder': 'mse'}\n",
        "      \n",
        "  elif model_type == 'nn':\n",
        "    if isinstance(divide_by, list) and divide_by != []:\n",
        "      dim = int(IMG_SHAPE / divide_by[0])\n",
        "      encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(input_layer)\n",
        "    else:\n",
        "      encode = layers.Dense(hidden_first, activation=\"relu\", kernel_initializer=\"he_normal\")(input_layer)\n",
        "    if dropout:\n",
        "      encode = layers.Dropout(dropout_rate)(encode)\n",
        "    if isinstance(divide_by, list) and divide_by != []:\n",
        "      i = 1\n",
        "      while i < len(divide_by) and int(dim / divide_by[i]) > hidden_last:\n",
        "        dim = int(dim / divide_by[i])\n",
        "        if batch_norm:\n",
        "          encode = layers.BatchNormalization()(encode)\n",
        "        encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "        if dropout:\n",
        "          encode = layers.Dropout(dropout_rate)(encode) \n",
        "        i += 1\n",
        "    else:\n",
        "      while hidden_first > hidden_last:\n",
        "        hidden_first = int(hidden_first / divide_by)\n",
        "        if batch_norm:\n",
        "          encode = layers.BatchNormalization()(encode)\n",
        "        encode = layers.Dense(hidden_first, activation=\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "        if dropout:\n",
        "          encode = layers.Dropout(dropout_rate)(encode) \n",
        "    \n",
        "    classifier = layers.Dense(N_CLASSES, activation=\"softmax\", name='classifier')(encode)\n",
        "    # nn model\n",
        "    model = Model(inputs=input_layer, outputs=classifier)\n",
        "    encoder = None\n",
        "    loss = \"sparse_categorical_crossentropy\"\n",
        "    metrics = {'classifier': 'accuracy'}\n",
        "  \n",
        "  model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
        "  if verbose:\n",
        "    model.summary()  \n",
        "  return model, encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SI6WnDDx_xS",
        "colab_type": "text"
      },
      "source": [
        "# Plain neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td3w71yf9f1D",
        "colab_type": "code",
        "outputId": "e4bfc4cc-057b-4a85-fccb-314109778b3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "monitor = 'val_loss'\n",
        "early_stopping = EarlyStopping(monitor=monitor, patience=10, min_delta=0.001, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=10, verbose=1)\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "model_type = 'nn'\n",
        "if model_type == 'autoencoder_nn':\n",
        "  labels_train = [y_train, x_train]\n",
        "  labels_val = [y_val, x_val]\n",
        "elif model_type == 'nn':\n",
        "  labels_train = y_train\n",
        "  labels_val = y_val\n",
        "elif model_type == 'autoencoder':\n",
        "  labels_train = x_train\n",
        "  labels_val = x_val\n",
        "\n",
        "# define 10-fold cross validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "scores_val = []\n",
        "best_score = 0  # based on accuracy\n",
        "best_model = None\n",
        "best_history = None\n",
        "\n",
        "for idx, (train, val) in enumerate(kfold.split(x_train, labels_train)):\n",
        "  print('FOLD: ', idx + 1)\n",
        "  # create model\n",
        "  model, _ = get_model(\n",
        "      model_type=model_type, \n",
        "      divide_by=[1.5, 2, 2.5, 3], \n",
        "      hidden_first=512, \n",
        "      hidden_last=32, \n",
        "      dropout=True, \n",
        "      batch_norm=False, \n",
        "      standard=False, \n",
        "      verbose=True\n",
        "  )\n",
        "  # Fit the model\n",
        "  history = model.fit(\n",
        "      x_train[train], \n",
        "      labels_train[train], \n",
        "      validation_data=(x_train[val], labels_train[val]), \n",
        "      epochs=300, \n",
        "      batch_size=128, \n",
        "      callbacks=callbacks\n",
        "  )\n",
        "  # evaluate the model\n",
        "  score = model.evaluate(x_val, labels_val, verbose=0)\n",
        "  # save best_model\n",
        "  if score[1] > best_score:  # accuracy\n",
        "    best_score = score[1]\n",
        "    best_model = model\n",
        "    best_history = history\n",
        "  print('Performance on the validation set')\n",
        "  for idx in range(len(model.metrics_names)):\n",
        "    print(\"%s: %.2f\" % (model.metrics_names[idx], score[idx]))\n",
        "  print()\n",
        "  scores_val.append(score)\n",
        "  \n",
        "scores_val = np.array(scores_val)\n",
        "overall_means = np.mean(scores_val, axis=0)\n",
        "overall_stds = np.std(scores_val, axis=0)\n",
        "for idx in range(len(overall_means)):\n",
        "  print('Overall ', model.metrics_names[idx], ': ', '{:.2f}'.format(overall_means[idx]), '+-', '{:.2f}'.format(overall_stds[idx]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD:  1\n",
            "Model: \"model_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_21 (InputLayer)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_71 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_84 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_72 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_85 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_73 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_86 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_74 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 2s 233us/step - loss: 1.7147 - accuracy: 0.4109 - val_loss: 0.8430 - val_accuracy: 0.7625\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.9812 - accuracy: 0.6829 - val_loss: 0.5490 - val_accuracy: 0.8491\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.6949 - accuracy: 0.7847 - val_loss: 0.4296 - val_accuracy: 0.8723\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.5458 - accuracy: 0.8406 - val_loss: 0.3589 - val_accuracy: 0.8964\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.4251 - accuracy: 0.8795 - val_loss: 0.3137 - val_accuracy: 0.9062\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.3911 - accuracy: 0.8905 - val_loss: 0.2934 - val_accuracy: 0.9143\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.3224 - accuracy: 0.9092 - val_loss: 0.2865 - val_accuracy: 0.9250\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.3100 - accuracy: 0.9166 - val_loss: 0.2893 - val_accuracy: 0.9143\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.2600 - accuracy: 0.9295 - val_loss: 0.2846 - val_accuracy: 0.9223\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2294 - accuracy: 0.9367 - val_loss: 0.3114 - val_accuracy: 0.9170\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.2237 - accuracy: 0.9383 - val_loss: 0.2789 - val_accuracy: 0.9304\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2128 - accuracy: 0.9396 - val_loss: 0.2629 - val_accuracy: 0.9304\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 61us/step - loss: 0.1949 - accuracy: 0.9481 - val_loss: 0.2837 - val_accuracy: 0.9277\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.1844 - accuracy: 0.9473 - val_loss: 0.2939 - val_accuracy: 0.9259\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1705 - accuracy: 0.9511 - val_loss: 0.2616 - val_accuracy: 0.9339\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.1663 - accuracy: 0.9541 - val_loss: 0.3047 - val_accuracy: 0.9286\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1543 - accuracy: 0.9573 - val_loss: 0.2643 - val_accuracy: 0.9339\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.1424 - accuracy: 0.9605 - val_loss: 0.2459 - val_accuracy: 0.9384\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.1431 - accuracy: 0.9595 - val_loss: 0.2698 - val_accuracy: 0.9339\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.1308 - accuracy: 0.9637 - val_loss: 0.2681 - val_accuracy: 0.9357\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 62us/step - loss: 0.1341 - accuracy: 0.9652 - val_loss: 0.2785 - val_accuracy: 0.9348\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1260 - accuracy: 0.9645 - val_loss: 0.2574 - val_accuracy: 0.9366\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.1096 - accuracy: 0.9696 - val_loss: 0.2889 - val_accuracy: 0.9304\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.1097 - accuracy: 0.9707 - val_loss: 0.2678 - val_accuracy: 0.9411\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1034 - accuracy: 0.9712 - val_loss: 0.2798 - val_accuracy: 0.9357\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.1079 - accuracy: 0.9686 - val_loss: 0.2747 - val_accuracy: 0.9312\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.0966 - accuracy: 0.9716 - val_loss: 0.2709 - val_accuracy: 0.9482\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1046 - accuracy: 0.9685 - val_loss: 0.2563 - val_accuracy: 0.9420\n",
            "Performance on the validation set\n",
            "loss: 0.30\n",
            "accuracy: 0.92\n",
            "\n",
            "FOLD:  2\n",
            "Model: \"model_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_22 (InputLayer)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_75 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_76 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_77 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_78 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 2s 235us/step - loss: 1.7753 - accuracy: 0.3895 - val_loss: 0.8775 - val_accuracy: 0.7518\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 1.0163 - accuracy: 0.6801 - val_loss: 0.5745 - val_accuracy: 0.8241\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.7308 - accuracy: 0.7763 - val_loss: 0.4017 - val_accuracy: 0.8813\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.5568 - accuracy: 0.8369 - val_loss: 0.3425 - val_accuracy: 0.8982\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.4552 - accuracy: 0.8698 - val_loss: 0.2750 - val_accuracy: 0.9179\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.3862 - accuracy: 0.8924 - val_loss: 0.2603 - val_accuracy: 0.9250\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.3473 - accuracy: 0.9051 - val_loss: 0.2514 - val_accuracy: 0.9259\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.3031 - accuracy: 0.9143 - val_loss: 0.2243 - val_accuracy: 0.9429\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.2657 - accuracy: 0.9244 - val_loss: 0.2362 - val_accuracy: 0.9366\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.2382 - accuracy: 0.9363 - val_loss: 0.2190 - val_accuracy: 0.9420\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2371 - accuracy: 0.9336 - val_loss: 0.2090 - val_accuracy: 0.9438\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2079 - accuracy: 0.9452 - val_loss: 0.2299 - val_accuracy: 0.9455\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2009 - accuracy: 0.9426 - val_loss: 0.2330 - val_accuracy: 0.9402\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1823 - accuracy: 0.9512 - val_loss: 0.2294 - val_accuracy: 0.9429\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1615 - accuracy: 0.9564 - val_loss: 0.2507 - val_accuracy: 0.9429\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1558 - accuracy: 0.9585 - val_loss: 0.2367 - val_accuracy: 0.9384\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1567 - accuracy: 0.9583 - val_loss: 0.2151 - val_accuracy: 0.9429\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1461 - accuracy: 0.9604 - val_loss: 0.2228 - val_accuracy: 0.9473\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 61us/step - loss: 0.1357 - accuracy: 0.9641 - val_loss: 0.2347 - val_accuracy: 0.9402\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 61us/step - loss: 0.1347 - accuracy: 0.9627 - val_loss: 0.2295 - val_accuracy: 0.9429\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.1286 - accuracy: 0.9651 - val_loss: 0.2487 - val_accuracy: 0.9411\n",
            "Performance on the validation set\n",
            "loss: 0.30\n",
            "accuracy: 0.92\n",
            "\n",
            "FOLD:  3\n",
            "Model: \"model_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_23 (InputLayer)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_91 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_79 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_92 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_80 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_93 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_81 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_82 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 2s 244us/step - loss: 1.7912 - accuracy: 0.3798 - val_loss: 0.9105 - val_accuracy: 0.7455\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 1.0418 - accuracy: 0.6645 - val_loss: 0.5955 - val_accuracy: 0.8241\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.7352 - accuracy: 0.7793 - val_loss: 0.4385 - val_accuracy: 0.8652\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.5752 - accuracy: 0.8368 - val_loss: 0.3822 - val_accuracy: 0.8866\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.4575 - accuracy: 0.8695 - val_loss: 0.3542 - val_accuracy: 0.9000\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.3884 - accuracy: 0.8939 - val_loss: 0.3172 - val_accuracy: 0.9107\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.3264 - accuracy: 0.9110 - val_loss: 0.3389 - val_accuracy: 0.9143\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2998 - accuracy: 0.9205 - val_loss: 0.3165 - val_accuracy: 0.9161\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2611 - accuracy: 0.9281 - val_loss: 0.3241 - val_accuracy: 0.9152\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.2512 - accuracy: 0.9304 - val_loss: 0.3008 - val_accuracy: 0.9259\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2165 - accuracy: 0.9414 - val_loss: 0.3056 - val_accuracy: 0.9232\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.2034 - accuracy: 0.9434 - val_loss: 0.2950 - val_accuracy: 0.9268\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 62us/step - loss: 0.1909 - accuracy: 0.9477 - val_loss: 0.3041 - val_accuracy: 0.9170\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1757 - accuracy: 0.9523 - val_loss: 0.2764 - val_accuracy: 0.9348\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1611 - accuracy: 0.9562 - val_loss: 0.3002 - val_accuracy: 0.9268\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1516 - accuracy: 0.9561 - val_loss: 0.3205 - val_accuracy: 0.9295\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1518 - accuracy: 0.9583 - val_loss: 0.3254 - val_accuracy: 0.9268\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.1478 - accuracy: 0.9597 - val_loss: 0.3326 - val_accuracy: 0.9277\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1413 - accuracy: 0.9607 - val_loss: 0.3167 - val_accuracy: 0.9286\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1143 - accuracy: 0.9681 - val_loss: 0.3214 - val_accuracy: 0.9295\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.1167 - accuracy: 0.9677 - val_loss: 0.3041 - val_accuracy: 0.9304\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1101 - accuracy: 0.9698 - val_loss: 0.2997 - val_accuracy: 0.9330\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1184 - accuracy: 0.9678 - val_loss: 0.3669 - val_accuracy: 0.9312\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1061 - accuracy: 0.9706 - val_loss: 0.3248 - val_accuracy: 0.9277\n",
            "Performance on the validation set\n",
            "loss: 0.29\n",
            "accuracy: 0.93\n",
            "\n",
            "FOLD:  4\n",
            "Model: \"model_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_24 (InputLayer)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_95 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_83 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_96 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_84 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_97 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_85 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_98 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_86 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 257us/step - loss: 1.7645 - accuracy: 0.3933 - val_loss: 0.9156 - val_accuracy: 0.7170\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 69us/step - loss: 1.0184 - accuracy: 0.6734 - val_loss: 0.6056 - val_accuracy: 0.8188\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.7009 - accuracy: 0.7891 - val_loss: 0.4275 - val_accuracy: 0.8821\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.5474 - accuracy: 0.8419 - val_loss: 0.3814 - val_accuracy: 0.8893\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.4400 - accuracy: 0.8777 - val_loss: 0.3105 - val_accuracy: 0.9125\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.3777 - accuracy: 0.8948 - val_loss: 0.3133 - val_accuracy: 0.9170\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.3417 - accuracy: 0.9055 - val_loss: 0.3007 - val_accuracy: 0.9170\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.2902 - accuracy: 0.9190 - val_loss: 0.2825 - val_accuracy: 0.9223\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.2637 - accuracy: 0.9299 - val_loss: 0.2771 - val_accuracy: 0.9330\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.2351 - accuracy: 0.9311 - val_loss: 0.2702 - val_accuracy: 0.9286\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.2202 - accuracy: 0.9387 - val_loss: 0.2659 - val_accuracy: 0.9277\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2136 - accuracy: 0.9438 - val_loss: 0.2642 - val_accuracy: 0.9295\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1884 - accuracy: 0.9474 - val_loss: 0.2604 - val_accuracy: 0.9321\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.1786 - accuracy: 0.9483 - val_loss: 0.2445 - val_accuracy: 0.9339\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.1636 - accuracy: 0.9537 - val_loss: 0.2675 - val_accuracy: 0.9295\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1513 - accuracy: 0.9596 - val_loss: 0.2797 - val_accuracy: 0.9268\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1527 - accuracy: 0.9588 - val_loss: 0.2664 - val_accuracy: 0.9321\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1363 - accuracy: 0.9630 - val_loss: 0.2675 - val_accuracy: 0.9339\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1349 - accuracy: 0.9610 - val_loss: 0.2689 - val_accuracy: 0.9384\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1345 - accuracy: 0.9638 - val_loss: 0.2866 - val_accuracy: 0.9348\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1229 - accuracy: 0.9682 - val_loss: 0.2630 - val_accuracy: 0.9375\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1184 - accuracy: 0.9676 - val_loss: 0.2742 - val_accuracy: 0.9348\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1214 - accuracy: 0.9662 - val_loss: 0.2737 - val_accuracy: 0.9339\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1112 - accuracy: 0.9681 - val_loss: 0.2656 - val_accuracy: 0.9312\n",
            "Performance on the validation set\n",
            "loss: 0.28\n",
            "accuracy: 0.93\n",
            "\n",
            "FOLD:  5\n",
            "Model: \"model_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_25 (InputLayer)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_99 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_87 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_100 (Dense)            (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_88 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_101 (Dense)            (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_89 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_102 (Dense)            (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_90 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 2s 246us/step - loss: 1.8103 - accuracy: 0.3781 - val_loss: 0.8884 - val_accuracy: 0.7420\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.9832 - accuracy: 0.6931 - val_loss: 0.5234 - val_accuracy: 0.8339\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.6913 - accuracy: 0.7932 - val_loss: 0.3599 - val_accuracy: 0.8839\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 62us/step - loss: 0.5258 - accuracy: 0.8490 - val_loss: 0.3049 - val_accuracy: 0.9045\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.4573 - accuracy: 0.8758 - val_loss: 0.2605 - val_accuracy: 0.9214\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.3867 - accuracy: 0.8928 - val_loss: 0.2750 - val_accuracy: 0.9223\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 62us/step - loss: 0.3394 - accuracy: 0.9046 - val_loss: 0.2400 - val_accuracy: 0.9277\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2994 - accuracy: 0.9168 - val_loss: 0.2308 - val_accuracy: 0.9375\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2655 - accuracy: 0.9272 - val_loss: 0.2347 - val_accuracy: 0.9330\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2423 - accuracy: 0.9323 - val_loss: 0.2242 - val_accuracy: 0.9330\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2129 - accuracy: 0.9419 - val_loss: 0.2224 - val_accuracy: 0.9366\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1998 - accuracy: 0.9431 - val_loss: 0.2439 - val_accuracy: 0.9348\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.2008 - accuracy: 0.9470 - val_loss: 0.2298 - val_accuracy: 0.9366\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1889 - accuracy: 0.9515 - val_loss: 0.2188 - val_accuracy: 0.9393\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1605 - accuracy: 0.9578 - val_loss: 0.2323 - val_accuracy: 0.9393\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1566 - accuracy: 0.9560 - val_loss: 0.2466 - val_accuracy: 0.9366\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1636 - accuracy: 0.9541 - val_loss: 0.2334 - val_accuracy: 0.9393\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1508 - accuracy: 0.9563 - val_loss: 0.2320 - val_accuracy: 0.9393\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1556 - accuracy: 0.9590 - val_loss: 0.2224 - val_accuracy: 0.9438\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.1452 - accuracy: 0.9600 - val_loss: 0.2406 - val_accuracy: 0.9384\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1214 - accuracy: 0.9667 - val_loss: 0.2263 - val_accuracy: 0.9411\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1216 - accuracy: 0.9660 - val_loss: 0.2477 - val_accuracy: 0.9446\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.1242 - accuracy: 0.9657 - val_loss: 0.2230 - val_accuracy: 0.9429\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 1s 62us/step - loss: 0.1128 - accuracy: 0.9694 - val_loss: 0.2535 - val_accuracy: 0.9455\n",
            "Performance on the validation set\n",
            "loss: 0.27\n",
            "accuracy: 0.93\n",
            "\n",
            "FOLD:  6\n",
            "Model: \"model_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_26 (InputLayer)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_103 (Dense)            (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_91 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_104 (Dense)            (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_92 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_105 (Dense)            (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_93 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_106 (Dense)            (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_94 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 256us/step - loss: 1.6779 - accuracy: 0.4155 - val_loss: 0.8112 - val_accuracy: 0.7696\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.9505 - accuracy: 0.7019 - val_loss: 0.5505 - val_accuracy: 0.8241\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 62us/step - loss: 0.6875 - accuracy: 0.7937 - val_loss: 0.4474 - val_accuracy: 0.8714\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.5367 - accuracy: 0.8449 - val_loss: 0.3716 - val_accuracy: 0.8938\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 63us/step - loss: 0.4499 - accuracy: 0.8723 - val_loss: 0.2967 - val_accuracy: 0.9187\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.3874 - accuracy: 0.8887 - val_loss: 0.3150 - val_accuracy: 0.9152\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.3265 - accuracy: 0.9062 - val_loss: 0.2969 - val_accuracy: 0.9187\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.2891 - accuracy: 0.9214 - val_loss: 0.2792 - val_accuracy: 0.9232\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2671 - accuracy: 0.9265 - val_loss: 0.2945 - val_accuracy: 0.9205\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2504 - accuracy: 0.9307 - val_loss: 0.2724 - val_accuracy: 0.9250\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.2140 - accuracy: 0.9397 - val_loss: 0.2558 - val_accuracy: 0.9295\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.2110 - accuracy: 0.9402 - val_loss: 0.2560 - val_accuracy: 0.9277\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.2103 - accuracy: 0.9420 - val_loss: 0.2620 - val_accuracy: 0.9268\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1803 - accuracy: 0.9494 - val_loss: 0.2651 - val_accuracy: 0.9330\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1808 - accuracy: 0.9478 - val_loss: 0.2603 - val_accuracy: 0.9357\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1567 - accuracy: 0.9556 - val_loss: 0.2610 - val_accuracy: 0.9259\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1491 - accuracy: 0.9610 - val_loss: 0.2571 - val_accuracy: 0.9321\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1507 - accuracy: 0.9602 - val_loss: 0.2874 - val_accuracy: 0.9295\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 62us/step - loss: 0.1448 - accuracy: 0.9618 - val_loss: 0.2626 - val_accuracy: 0.9402\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1308 - accuracy: 0.9639 - val_loss: 0.2919 - val_accuracy: 0.9304\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1171 - accuracy: 0.9694 - val_loss: 0.3049 - val_accuracy: 0.9295\n",
            "Performance on the validation set\n",
            "loss: 0.29\n",
            "accuracy: 0.92\n",
            "\n",
            "FOLD:  7\n",
            "Model: \"model_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_27 (InputLayer)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_107 (Dense)            (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_95 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_108 (Dense)            (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_96 (Dropout)         (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_109 (Dense)            (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_97 (Dropout)         (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_110 (Dense)            (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_98 (Dropout)         (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 257us/step - loss: 1.7266 - accuracy: 0.4052 - val_loss: 0.9135 - val_accuracy: 0.7268\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.9785 - accuracy: 0.6876 - val_loss: 0.5419 - val_accuracy: 0.8429\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.6990 - accuracy: 0.7911 - val_loss: 0.3885 - val_accuracy: 0.8929\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.5284 - accuracy: 0.8498 - val_loss: 0.3434 - val_accuracy: 0.9054\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.4249 - accuracy: 0.8764 - val_loss: 0.3293 - val_accuracy: 0.9134\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.3751 - accuracy: 0.8924 - val_loss: 0.3039 - val_accuracy: 0.9214\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.3210 - accuracy: 0.9105 - val_loss: 0.2974 - val_accuracy: 0.9268\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.2908 - accuracy: 0.9193 - val_loss: 0.2919 - val_accuracy: 0.9259\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.2492 - accuracy: 0.9280 - val_loss: 0.2830 - val_accuracy: 0.9304\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.2404 - accuracy: 0.9329 - val_loss: 0.2957 - val_accuracy: 0.9268\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.2183 - accuracy: 0.9389 - val_loss: 0.3167 - val_accuracy: 0.9268\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.2083 - accuracy: 0.9428 - val_loss: 0.2921 - val_accuracy: 0.9339\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1834 - accuracy: 0.9470 - val_loss: 0.2983 - val_accuracy: 0.9312\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1705 - accuracy: 0.9546 - val_loss: 0.3121 - val_accuracy: 0.9304\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1677 - accuracy: 0.9525 - val_loss: 0.3320 - val_accuracy: 0.9268\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1516 - accuracy: 0.9564 - val_loss: 0.3432 - val_accuracy: 0.9286\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1442 - accuracy: 0.9599 - val_loss: 0.3497 - val_accuracy: 0.9348\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1429 - accuracy: 0.9593 - val_loss: 0.3229 - val_accuracy: 0.9330\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1237 - accuracy: 0.9664 - val_loss: 0.3684 - val_accuracy: 0.9330\n",
            "Performance on the validation set\n",
            "loss: 0.27\n",
            "accuracy: 0.92\n",
            "\n",
            "FOLD:  8\n",
            "Model: \"model_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_28 (InputLayer)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_111 (Dense)            (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_99 (Dropout)         (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_112 (Dense)            (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_100 (Dropout)        (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_113 (Dense)            (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_101 (Dropout)        (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_102 (Dropout)        (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 265us/step - loss: 1.7651 - accuracy: 0.3938 - val_loss: 0.8386 - val_accuracy: 0.7625\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.9761 - accuracy: 0.6940 - val_loss: 0.5125 - val_accuracy: 0.8366\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.6909 - accuracy: 0.7924 - val_loss: 0.3866 - val_accuracy: 0.8813\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.5228 - accuracy: 0.8501 - val_loss: 0.3297 - val_accuracy: 0.9009\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.4369 - accuracy: 0.8759 - val_loss: 0.3046 - val_accuracy: 0.9089\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.3794 - accuracy: 0.8936 - val_loss: 0.2760 - val_accuracy: 0.9179\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.3380 - accuracy: 0.9062 - val_loss: 0.2745 - val_accuracy: 0.9295\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.2976 - accuracy: 0.9176 - val_loss: 0.2478 - val_accuracy: 0.9268\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.2541 - accuracy: 0.9288 - val_loss: 0.2613 - val_accuracy: 0.9223\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.2284 - accuracy: 0.9345 - val_loss: 0.2826 - val_accuracy: 0.9259\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.2211 - accuracy: 0.9392 - val_loss: 0.2599 - val_accuracy: 0.9304\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.2053 - accuracy: 0.9430 - val_loss: 0.2524 - val_accuracy: 0.9357\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.1904 - accuracy: 0.9493 - val_loss: 0.2476 - val_accuracy: 0.9357\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1844 - accuracy: 0.9500 - val_loss: 0.2401 - val_accuracy: 0.9393\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 68us/step - loss: 0.1636 - accuracy: 0.9544 - val_loss: 0.2492 - val_accuracy: 0.9348\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 68us/step - loss: 0.1561 - accuracy: 0.9566 - val_loss: 0.2357 - val_accuracy: 0.9411\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1356 - accuracy: 0.9620 - val_loss: 0.2637 - val_accuracy: 0.9393\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1358 - accuracy: 0.9620 - val_loss: 0.2632 - val_accuracy: 0.9357\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1362 - accuracy: 0.9633 - val_loss: 0.2560 - val_accuracy: 0.9393\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1359 - accuracy: 0.9627 - val_loss: 0.2618 - val_accuracy: 0.9366\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1286 - accuracy: 0.9653 - val_loss: 0.2671 - val_accuracy: 0.9491\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1122 - accuracy: 0.9692 - val_loss: 0.2340 - val_accuracy: 0.9473\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1193 - accuracy: 0.9676 - val_loss: 0.2526 - val_accuracy: 0.9393\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1197 - accuracy: 0.9667 - val_loss: 0.2594 - val_accuracy: 0.9411\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.0931 - accuracy: 0.9747 - val_loss: 0.2955 - val_accuracy: 0.9402\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.0996 - accuracy: 0.9742 - val_loss: 0.2519 - val_accuracy: 0.9411\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.0956 - accuracy: 0.9732 - val_loss: 0.2906 - val_accuracy: 0.9438\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1099 - accuracy: 0.9720 - val_loss: 0.2625 - val_accuracy: 0.9438\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.0941 - accuracy: 0.9747 - val_loss: 0.2670 - val_accuracy: 0.9420\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.0916 - accuracy: 0.9741 - val_loss: 0.2557 - val_accuracy: 0.9429\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.0775 - accuracy: 0.9799 - val_loss: 0.2647 - val_accuracy: 0.9455\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.0788 - accuracy: 0.9785 - val_loss: 0.2988 - val_accuracy: 0.9339\n",
            "Performance on the validation set\n",
            "loss: 0.33\n",
            "accuracy: 0.93\n",
            "\n",
            "FOLD:  9\n",
            "Model: \"model_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_29 (InputLayer)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_103 (Dropout)        (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_116 (Dense)            (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_104 (Dropout)        (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_117 (Dense)            (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_105 (Dropout)        (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_118 (Dense)            (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_106 (Dropout)        (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 272us/step - loss: 1.8166 - accuracy: 0.3741 - val_loss: 0.8662 - val_accuracy: 0.7304\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 1.0323 - accuracy: 0.6746 - val_loss: 0.5778 - val_accuracy: 0.8348\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.7171 - accuracy: 0.7807 - val_loss: 0.4195 - val_accuracy: 0.8911\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.5616 - accuracy: 0.8396 - val_loss: 0.3466 - val_accuracy: 0.9009\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.4442 - accuracy: 0.8777 - val_loss: 0.3079 - val_accuracy: 0.9071\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.3794 - accuracy: 0.8940 - val_loss: 0.3152 - val_accuracy: 0.9143\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.3370 - accuracy: 0.9065 - val_loss: 0.3152 - val_accuracy: 0.9125\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.2924 - accuracy: 0.9173 - val_loss: 0.2963 - val_accuracy: 0.9161\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.2668 - accuracy: 0.9282 - val_loss: 0.2866 - val_accuracy: 0.9196\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.2441 - accuracy: 0.9359 - val_loss: 0.2977 - val_accuracy: 0.9223\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.2169 - accuracy: 0.9382 - val_loss: 0.3069 - val_accuracy: 0.9170\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.2009 - accuracy: 0.9455 - val_loss: 0.3141 - val_accuracy: 0.9143\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1892 - accuracy: 0.9468 - val_loss: 0.3092 - val_accuracy: 0.9196\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1769 - accuracy: 0.9511 - val_loss: 0.3306 - val_accuracy: 0.9205\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.1913 - accuracy: 0.9467 - val_loss: 0.2904 - val_accuracy: 0.9241\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1690 - accuracy: 0.9535 - val_loss: 0.3013 - val_accuracy: 0.9304\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 64us/step - loss: 0.1572 - accuracy: 0.9567 - val_loss: 0.3048 - val_accuracy: 0.9304\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1331 - accuracy: 0.9634 - val_loss: 0.3111 - val_accuracy: 0.9232\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.1381 - accuracy: 0.9628 - val_loss: 0.3355 - val_accuracy: 0.9143\n",
            "Performance on the validation set\n",
            "loss: 0.30\n",
            "accuracy: 0.92\n",
            "\n",
            "FOLD:  10\n",
            "Model: \"model_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_30 (InputLayer)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_119 (Dense)            (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dropout_107 (Dropout)        (None, 522)               0         \n",
            "_________________________________________________________________\n",
            "dense_120 (Dense)            (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dropout_108 (Dropout)        (None, 261)               0         \n",
            "_________________________________________________________________\n",
            "dense_121 (Dense)            (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dropout_109 (Dropout)        (None, 104)               0         \n",
            "_________________________________________________________________\n",
            "dense_122 (Dense)            (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "dropout_110 (Dropout)        (None, 34)                0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           (None, 11)                385       \n",
            "=================================================================\n",
            "Total params: 577,476\n",
            "Trainable params: 577,476\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 273us/step - loss: 1.7260 - accuracy: 0.4105 - val_loss: 0.8195 - val_accuracy: 0.7750\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.9614 - accuracy: 0.6944 - val_loss: 0.5677 - val_accuracy: 0.8304\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.6726 - accuracy: 0.8026 - val_loss: 0.3964 - val_accuracy: 0.8804\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.5210 - accuracy: 0.8521 - val_loss: 0.3643 - val_accuracy: 0.8991\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.4446 - accuracy: 0.8729 - val_loss: 0.3355 - val_accuracy: 0.9080\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.3622 - accuracy: 0.8978 - val_loss: 0.3181 - val_accuracy: 0.9125\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.3286 - accuracy: 0.9112 - val_loss: 0.2999 - val_accuracy: 0.9152\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 68us/step - loss: 0.2942 - accuracy: 0.9192 - val_loss: 0.2855 - val_accuracy: 0.9205\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.2620 - accuracy: 0.9274 - val_loss: 0.2971 - val_accuracy: 0.9268\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.2419 - accuracy: 0.9349 - val_loss: 0.2759 - val_accuracy: 0.9250\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.2215 - accuracy: 0.9386 - val_loss: 0.2794 - val_accuracy: 0.9304\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.2059 - accuracy: 0.9427 - val_loss: 0.2915 - val_accuracy: 0.9304\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 68us/step - loss: 0.1851 - accuracy: 0.9483 - val_loss: 0.2851 - val_accuracy: 0.9286\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1619 - accuracy: 0.9551 - val_loss: 0.2797 - val_accuracy: 0.9411\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 68us/step - loss: 0.1624 - accuracy: 0.9546 - val_loss: 0.3041 - val_accuracy: 0.9330\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.1601 - accuracy: 0.9546 - val_loss: 0.2893 - val_accuracy: 0.9330\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1561 - accuracy: 0.9556 - val_loss: 0.2744 - val_accuracy: 0.9375\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.1399 - accuracy: 0.9622 - val_loss: 0.3114 - val_accuracy: 0.9339\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.1445 - accuracy: 0.9613 - val_loss: 0.2827 - val_accuracy: 0.9375\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1292 - accuracy: 0.9642 - val_loss: 0.2880 - val_accuracy: 0.9411\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.1209 - accuracy: 0.9661 - val_loss: 0.2715 - val_accuracy: 0.9429\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.1153 - accuracy: 0.9675 - val_loss: 0.2947 - val_accuracy: 0.9366\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1086 - accuracy: 0.9698 - val_loss: 0.3030 - val_accuracy: 0.9384\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.1077 - accuracy: 0.9699 - val_loss: 0.3494 - val_accuracy: 0.9312\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.1111 - accuracy: 0.9693 - val_loss: 0.3255 - val_accuracy: 0.9393\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 1s 65us/step - loss: 0.1143 - accuracy: 0.9693 - val_loss: 0.3243 - val_accuracy: 0.9304\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.1040 - accuracy: 0.9720 - val_loss: 0.3344 - val_accuracy: 0.9348\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 1s 69us/step - loss: 0.0926 - accuracy: 0.9765 - val_loss: 0.3226 - val_accuracy: 0.9375\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.0895 - accuracy: 0.9759 - val_loss: 0.3178 - val_accuracy: 0.9348\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 1s 66us/step - loss: 0.0939 - accuracy: 0.9764 - val_loss: 0.3366 - val_accuracy: 0.9357\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 1s 67us/step - loss: 0.0977 - accuracy: 0.9721 - val_loss: 0.3027 - val_accuracy: 0.9393\n",
            "Performance on the validation set\n",
            "loss: 0.32\n",
            "accuracy: 0.93\n",
            "\n",
            "Overall  loss :  0.30 +- 0.02\n",
            "Overall  accuracy :  0.93 +- 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f98FIk3T1MoF",
        "colab_type": "code",
        "outputId": "64db8566-fd4f-48d2-ee62-152358b72e6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "x_plot = list(range(1, len(best_history.history['val_accuracy']) + 1))\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(x_plot, history.history['loss'])\n",
        "    plt.plot(x_plot, history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.plot(x_plot, history.history['accuracy'])\n",
        "    plt.plot(x_plot, history.history['val_accuracy'])\n",
        "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_history(best_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2T/jRfwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEHCAYAAACjh0HiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FZyEF3wVu_E",
        "colab_type": "code",
        "outputId": "b9e86c12-fd3d-4f1b-8c3b-aacd57b864d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# labels_val_from_categorical = np.argmax(labels_val, axis=1)\n",
        "predictions = np.argmax(best_model.predict(x_val), axis=1)\n",
        "print(classification_report(labels_val, predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.94       269\n",
            "           1       0.92      0.91      0.92       234\n",
            "           2       0.95      0.90      0.92       254\n",
            "           3       0.97      0.98      0.97       265\n",
            "           4       0.93      0.96      0.94       263\n",
            "           5       0.90      0.91      0.91       263\n",
            "           6       0.86      0.90      0.88       240\n",
            "           7       0.97      0.97      0.97       277\n",
            "           8       0.96      0.93      0.94       283\n",
            "           9       0.88      0.88      0.88       253\n",
            "          10       0.97      0.96      0.96       199\n",
            "\n",
            "    accuracy                           0.93      2800\n",
            "   macro avg       0.93      0.93      0.93      2800\n",
            "weighted avg       0.93      0.93      0.93      2800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flw70YN7sJAx",
        "colab_type": "text"
      },
      "source": [
        "# Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9eeb485a-193c-4e20-fe8d-81fa047cef34",
        "id": "Cn99NR8CsjCe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "monitor = 'val_loss'\n",
        "early_stopping = EarlyStopping(monitor=monitor, patience=10, min_delta=0.001, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=10, verbose=1)\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "model_type = 'autoencoder'\n",
        "if model_type == 'autoencoder_nn':\n",
        "  labels_train = [y_train, x_train]\n",
        "  labels_val = [y_val, x_val]\n",
        "elif model_type == 'nn':\n",
        "  labels_train = y_train\n",
        "  labels_val = y_val\n",
        "elif model_type == 'autoencoder':\n",
        "  labels_train = x_train\n",
        "  labels_val = x_val\n",
        "\n",
        "autoencoder, encoder = get_model(\n",
        "    model_type=model_type, \n",
        "    tied=True,\n",
        "    divide_by=[1.5, 2, 2.5, 3],\n",
        "    hidden_first=512, \n",
        "    hidden_last=32, \n",
        "    noise_type=None, \n",
        "    dropout=False, \n",
        "    batch_norm=False, \n",
        "    standard=False, \n",
        "    verbose=True\n",
        ")\n",
        "history_autoencoder = autoencoder.fit(\n",
        "    x_train, \n",
        "    labels_train,\n",
        "    validation_data=(x_val, labels_val), \n",
        "    epochs=300, \n",
        "    batch_size=128, \n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'encoder_30/kernel:0' shape=(34, 32) dtype=float32>\n",
            "dense_58\n",
            "<tf.Variable 'dense_58_1/kernel:0' shape=(104, 34) dtype=float32>\n",
            "dense_57\n",
            "<tf.Variable 'dense_57_1/kernel:0' shape=(261, 104) dtype=float32>\n",
            "dense_56\n",
            "<tf.Variable 'dense_56_1/kernel:0' shape=(522, 261) dtype=float32>\n",
            "<tf.Variable 'dense_55_1/kernel:0' shape=(784, 522) dtype=float32>\n",
            "Model: \"model_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_30 (InputLayer)        [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 34)                3570      \n",
            "_________________________________________________________________\n",
            "encoder (Dense)              (None, 32)                1120      \n",
            "_________________________________________________________________\n",
            "dense_transpose_17 (DenseTra (None, 34)                1154      \n",
            "_________________________________________________________________\n",
            "dense_transpose_18 (DenseTra (None, 104)               3674      \n",
            "_________________________________________________________________\n",
            "dense_transpose_19 (DenseTra (None, 261)               27509     \n",
            "_________________________________________________________________\n",
            "dense_transpose_20 (DenseTra (None, 522)               137025    \n",
            "_________________________________________________________________\n",
            "decoder (DenseTranspose)     (None, 784)               410554    \n",
            "=================================================================\n",
            "Total params: 579,916\n",
            "Trainable params: 579,916\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 11200 samples, validate on 2800 samples\n",
            "Epoch 1/300\n",
            "11200/11200 [==============================] - 2s 151us/sample - loss: 0.0868 - mean_squared_error: 0.0868 - val_loss: 0.0624 - val_mean_squared_error: 0.0624\n",
            "Epoch 2/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0548 - mean_squared_error: 0.0548 - val_loss: 0.0482 - val_mean_squared_error: 0.0482\n",
            "Epoch 3/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0425 - mean_squared_error: 0.0425 - val_loss: 0.0401 - val_mean_squared_error: 0.0401\n",
            "Epoch 4/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0356 - val_mean_squared_error: 0.0356\n",
            "Epoch 5/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0326 - mean_squared_error: 0.0326 - val_loss: 0.0321 - val_mean_squared_error: 0.0321\n",
            "Epoch 6/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0295 - mean_squared_error: 0.0295 - val_loss: 0.0298 - val_mean_squared_error: 0.0298\n",
            "Epoch 7/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0274 - mean_squared_error: 0.0274 - val_loss: 0.0279 - val_mean_squared_error: 0.0279\n",
            "Epoch 8/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0257 - mean_squared_error: 0.0257 - val_loss: 0.0267 - val_mean_squared_error: 0.0267\n",
            "Epoch 9/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - val_loss: 0.0257 - val_mean_squared_error: 0.0257\n",
            "Epoch 10/300\n",
            "11200/11200 [==============================] - 1s 65us/sample - loss: 0.0234 - mean_squared_error: 0.0234 - val_loss: 0.0245 - val_mean_squared_error: 0.0245\n",
            "Epoch 11/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0239 - val_mean_squared_error: 0.0239\n",
            "Epoch 12/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0232 - val_mean_squared_error: 0.0232\n",
            "Epoch 13/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0226 - val_mean_squared_error: 0.0226\n",
            "Epoch 14/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0219 - val_mean_squared_error: 0.0219\n",
            "Epoch 15/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0195 - mean_squared_error: 0.0195 - val_loss: 0.0214 - val_mean_squared_error: 0.0214\n",
            "Epoch 16/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0207 - val_mean_squared_error: 0.0207\n",
            "Epoch 17/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0210 - val_mean_squared_error: 0.0210\n",
            "Epoch 18/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0202 - val_mean_squared_error: 0.0202\n",
            "Epoch 19/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0196 - val_mean_squared_error: 0.0196\n",
            "Epoch 20/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0196 - val_mean_squared_error: 0.0196\n",
            "Epoch 21/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
            "Epoch 22/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0163 - mean_squared_error: 0.0163 - val_loss: 0.0190 - val_mean_squared_error: 0.0190\n",
            "Epoch 23/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0159 - mean_squared_error: 0.0159 - val_loss: 0.0191 - val_mean_squared_error: 0.0191\n",
            "Epoch 24/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.0186 - val_mean_squared_error: 0.0186\n",
            "Epoch 25/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0153 - mean_squared_error: 0.0153 - val_loss: 0.0185 - val_mean_squared_error: 0.0185\n",
            "Epoch 26/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0152 - mean_squared_error: 0.0152 - val_loss: 0.0182 - val_mean_squared_error: 0.0182\n",
            "Epoch 27/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0149 - mean_squared_error: 0.0149 - val_loss: 0.0184 - val_mean_squared_error: 0.0184\n",
            "Epoch 28/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0146 - mean_squared_error: 0.0146 - val_loss: 0.0178 - val_mean_squared_error: 0.0178\n",
            "Epoch 29/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0144 - mean_squared_error: 0.0144 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
            "Epoch 30/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0142 - mean_squared_error: 0.0142 - val_loss: 0.0176 - val_mean_squared_error: 0.0176\n",
            "Epoch 31/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0140 - mean_squared_error: 0.0140 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
            "Epoch 32/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0137 - mean_squared_error: 0.0137 - val_loss: 0.0174 - val_mean_squared_error: 0.0174\n",
            "Epoch 33/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0135 - mean_squared_error: 0.0135 - val_loss: 0.0172 - val_mean_squared_error: 0.0172\n",
            "Epoch 34/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0132 - mean_squared_error: 0.0132 - val_loss: 0.0172 - val_mean_squared_error: 0.0172\n",
            "Epoch 35/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0132 - mean_squared_error: 0.0132 - val_loss: 0.0173 - val_mean_squared_error: 0.0173\n",
            "Epoch 36/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0130 - mean_squared_error: 0.0130 - val_loss: 0.0167 - val_mean_squared_error: 0.0167\n",
            "Epoch 37/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0128 - mean_squared_error: 0.0128 - val_loss: 0.0166 - val_mean_squared_error: 0.0166\n",
            "Epoch 38/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0126 - mean_squared_error: 0.0126 - val_loss: 0.0169 - val_mean_squared_error: 0.0169\n",
            "Epoch 39/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0126 - mean_squared_error: 0.0126 - val_loss: 0.0166 - val_mean_squared_error: 0.0166\n",
            "Epoch 40/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0123 - mean_squared_error: 0.0123 - val_loss: 0.0165 - val_mean_squared_error: 0.0165\n",
            "Epoch 41/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0123 - mean_squared_error: 0.0123 - val_loss: 0.0166 - val_mean_squared_error: 0.0166\n",
            "Epoch 42/300\n",
            "11200/11200 [==============================] - 1s 69us/sample - loss: 0.0122 - mean_squared_error: 0.0122 - val_loss: 0.0164 - val_mean_squared_error: 0.0164\n",
            "Epoch 43/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0163 - val_mean_squared_error: 0.0163\n",
            "Epoch 44/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0164 - val_mean_squared_error: 0.0164\n",
            "Epoch 45/300\n",
            "11200/11200 [==============================] - 1s 68us/sample - loss: 0.0118 - mean_squared_error: 0.0118 - val_loss: 0.0163 - val_mean_squared_error: 0.0163\n",
            "Epoch 46/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0116 - mean_squared_error: 0.0116 - val_loss: 0.0163 - val_mean_squared_error: 0.0163\n",
            "Epoch 47/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0116 - mean_squared_error: 0.0116 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
            "Epoch 48/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0114 - mean_squared_error: 0.0114 - val_loss: 0.0164 - val_mean_squared_error: 0.0164\n",
            "Epoch 49/300\n",
            "11200/11200 [==============================] - 1s 67us/sample - loss: 0.0112 - mean_squared_error: 0.0112 - val_loss: 0.0161 - val_mean_squared_error: 0.0161\n",
            "Epoch 50/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
            "Epoch 51/300\n",
            "11200/11200 [==============================] - 1s 66us/sample - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
            "Epoch 52/300\n",
            "11200/11200 [==============================] - 1s 110us/sample - loss: 0.0110 - mean_squared_error: 0.0110 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k6R9UxDW2Uy",
        "colab_type": "code",
        "outputId": "eab5c326-faaa-43f7-92e8-026367419854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# print accuracy\n",
        "x_plot = list(range(1, len(history_autoencoder.history['val_loss']) + 1))\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(x_plot, history.history['loss'])\n",
        "    plt.plot(x_plot, history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "plot_history(history_autoencoder)\n",
        "\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "n=10\n",
        "plt.figure(figsize=(40, 4))\n",
        "for i in range(n):\n",
        "    # display original images\n",
        "    ax = plt.subplot(3, 20, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "    # display encoded images\n",
        "    ax = plt.subplot(3, 20, i + 1 + 20)\n",
        "    plt.imshow(encoded_imgs[i].reshape(8,4))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "    # display reconstructed images\n",
        "    ax = plt.subplot(3, 20, 2*20 +i+ 1)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "  \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEJCAYAAACOr7BbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmLAsIIwxxoRlAWGMMSYsCwhjjDFh/X9qRFJyXsNr3QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABEYAAADrCAYAAABkdZM2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAKrFxAgAAAAAAKjWPwW1tmPvpARhAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 2880x288 with 30 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ0eUWrqBKjh",
        "colab_type": "text"
      },
      "source": [
        "## Train neural net with features learned by the autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WklzvF9rYA-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_encoder_nn(\n",
        "    encoder,\n",
        "    encoder_trainable=False,\n",
        "    divide_by=2,\n",
        "    hidden_first=128,\n",
        "    hidden_last=32,\n",
        "    dropout=True, \n",
        "    dropout_rate=0.3,\n",
        "    batch_norm=False, \n",
        "    standard=False,\n",
        "    verbose=True\n",
        "):\n",
        "  \n",
        "  encoder.trainable = encoder_trainable\n",
        "  encoder_nn = Sequential()\n",
        "  encoder_nn.add(encoder)\n",
        "  \n",
        "  \"\"\" if isinstance(divide_by, list) and divide_by != []:\n",
        "    dim = int(IMG_SHAPE / divide_by[0])\n",
        "    encoder_nn.add(layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "  else:\n",
        "    encoder_nn.add(layers.Dense(hidden_first, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "  if dropout:\n",
        "    encoder_nn.add(layers.Dropout(dropout_rate)) \"\"\"\n",
        "  if isinstance(divide_by, list) and divide_by != []:\n",
        "    i = 0\n",
        "    while i < len(divide_by) and int(dim / divide_by[i]) > hidden_last:\n",
        "      if batch_norm:\n",
        "        encoder_nn.add(layers.BatchNormalization())\n",
        "      encoder_nn.add(layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "      if dropout:\n",
        "        encoder_nn.add(layers.Dropout(dropout_rate))\n",
        "      dim = int(dim / divide_by[i])\n",
        "      i += 1\n",
        "  else:\n",
        "    while hidden_first > hidden_last:\n",
        "      if batch_norm:\n",
        "        encoder_nn.add(layers.BatchNormalization())\n",
        "      encoder_nn.add(layers.Dense(hidden_first, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
        "      if dropout:\n",
        "        encoder_nn.add(layers.Dropout(dropout_rate))\n",
        "      hidden_first = int(hidden_first / divide_by)\n",
        "  encoder_nn.add(layers.Dense(N_CLASSES, activation=\"softmax\", kernel_initializer=\"he_normal\"))\n",
        "  encoder_nn.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  if verbose:\n",
        "    encoder_nn.summary()\n",
        "  \n",
        "  return encoder_nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymY9NxiBsLDK",
        "colab_type": "code",
        "outputId": "ada7509b-0c82-426a-f876-10feda14f2bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "monitor = 'val_loss'\n",
        "early_stopping = EarlyStopping(monitor=monitor, patience=10, min_delta=0.001, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=10, verbose=1)\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "model_type = 'nn'\n",
        "if model_type == 'autoencoder_nn':\n",
        "  labels_train = [y_train, x_train]\n",
        "  labels_val = [y_val, x_val]\n",
        "elif model_type == 'nn':\n",
        "  labels_train = y_train\n",
        "  labels_val = y_val\n",
        "elif model_type == 'autoencoder':\n",
        "  labels_train = x_train\n",
        "  labels_val = x_val\n",
        "\n",
        "# define 10-fold cross validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "scores_val = []\n",
        "best_score = 0  # based on accuracy\n",
        "best_model = None\n",
        "best_history = None\n",
        "\n",
        "for idx, (train, val) in enumerate(kfold.split(x_train, labels_train)):\n",
        "  print('FOLD: ', idx + 1)\n",
        "  # create model\n",
        "  model = get_encoder_nn(encoder, hidden_first=32, hidden_last=32, dropout=True, encoder_trainable=False)\n",
        "  # Fit the model\n",
        "  history = model.fit(\n",
        "      x_train[train], \n",
        "      labels_train[train], \n",
        "      validation_data=(x_train[val], labels_train[val]), \n",
        "      epochs=300, \n",
        "      batch_size=128, \n",
        "      callbacks=callbacks\n",
        "  )\n",
        "  # evaluate the model\n",
        "  score = model.evaluate(x_val, labels_val, verbose=0)\n",
        "  # save best_model\n",
        "  if score[1] > best_score:  # accuracy\n",
        "    best_score = score[1]\n",
        "    best_model = model\n",
        "    best_history = history\n",
        "  print('Performance on the validation set')\n",
        "  for idx in range(len(model.metrics_names)):\n",
        "    print(\"%s: %.2f\" % (model.metrics_names[idx], score[idx]))\n",
        "  print()\n",
        "  scores_val.append(score)\n",
        "  \n",
        "scores_val = np.array(scores_val)\n",
        "overall_means = np.mean(scores_val, axis=0)\n",
        "overall_stds = np.std(scores_val, axis=0)\n",
        "for idx in range(len(overall_means)):\n",
        "  print('Overall ', model.metrics_names[idx], ': ', '{:.2f}'.format(overall_means[idx]), '+-', '{:.2f}'.format(overall_stds[idx]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD:  1\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_17 (Model)             (None, 32)                578211    \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 578,574\n",
            "Trainable params: 363\n",
            "Non-trainable params: 578,211\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 115us/sample - loss: 14.0545 - acc: 0.0589 - val_loss: 6.4689 - val_acc: 0.1143\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 5.3876 - acc: 0.1374 - val_loss: 4.5079 - val_acc: 0.1482\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 4.2366 - acc: 0.1738 - val_loss: 3.7023 - val_acc: 0.2250\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 3.4689 - acc: 0.2299 - val_loss: 3.0291 - val_acc: 0.3054\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 2.8564 - acc: 0.2974 - val_loss: 2.5264 - val_acc: 0.3554\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 2.3833 - acc: 0.3739 - val_loss: 2.1400 - val_acc: 0.4402\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 2.0276 - acc: 0.4363 - val_loss: 1.8330 - val_acc: 0.4920\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.7610 - acc: 0.4980 - val_loss: 1.5991 - val_acc: 0.5420\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.5522 - acc: 0.5437 - val_loss: 1.4263 - val_acc: 0.5955\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.3918 - acc: 0.5816 - val_loss: 1.2916 - val_acc: 0.6259\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.2698 - acc: 0.6143 - val_loss: 1.1942 - val_acc: 0.6509\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.1716 - acc: 0.6395 - val_loss: 1.1007 - val_acc: 0.6759\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 1.0918 - acc: 0.6618 - val_loss: 1.0399 - val_acc: 0.6938\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.0294 - acc: 0.6791 - val_loss: 0.9808 - val_acc: 0.7063\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.9764 - acc: 0.6911 - val_loss: 0.9410 - val_acc: 0.7188\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.9342 - acc: 0.7032 - val_loss: 0.9068 - val_acc: 0.7250\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8979 - acc: 0.7137 - val_loss: 0.8734 - val_acc: 0.7330\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.8686 - acc: 0.7202 - val_loss: 0.8535 - val_acc: 0.7455\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8414 - acc: 0.7276 - val_loss: 0.8392 - val_acc: 0.7527\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8192 - acc: 0.7338 - val_loss: 0.8112 - val_acc: 0.7571\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8023 - acc: 0.7346 - val_loss: 0.7953 - val_acc: 0.7634\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7844 - acc: 0.7442 - val_loss: 0.7791 - val_acc: 0.7625\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7695 - acc: 0.7491 - val_loss: 0.7710 - val_acc: 0.7643\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7579 - acc: 0.7524 - val_loss: 0.7626 - val_acc: 0.7696\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7467 - acc: 0.7575 - val_loss: 0.7547 - val_acc: 0.7643\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7383 - acc: 0.7599 - val_loss: 0.7480 - val_acc: 0.7714\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7291 - acc: 0.7639 - val_loss: 0.7378 - val_acc: 0.7688\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.7209 - acc: 0.7666 - val_loss: 0.7295 - val_acc: 0.7696\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7162 - acc: 0.7688 - val_loss: 0.7281 - val_acc: 0.7688\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7083 - acc: 0.7698 - val_loss: 0.7217 - val_acc: 0.7705\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7041 - acc: 0.7723 - val_loss: 0.7230 - val_acc: 0.7705\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6998 - acc: 0.7734 - val_loss: 0.7182 - val_acc: 0.7732\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6949 - acc: 0.7744 - val_loss: 0.7147 - val_acc: 0.7777\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6913 - acc: 0.7742 - val_loss: 0.7144 - val_acc: 0.7795\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6878 - acc: 0.7748 - val_loss: 0.7090 - val_acc: 0.7732\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6859 - acc: 0.7745 - val_loss: 0.7058 - val_acc: 0.7786\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6835 - acc: 0.7765 - val_loss: 0.7122 - val_acc: 0.7768\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6812 - acc: 0.7762 - val_loss: 0.7008 - val_acc: 0.7830\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6749 - acc: 0.7784 - val_loss: 0.6977 - val_acc: 0.7812\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6753 - acc: 0.7788 - val_loss: 0.6994 - val_acc: 0.7857\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6751 - acc: 0.7757 - val_loss: 0.6952 - val_acc: 0.7839\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6694 - acc: 0.7821 - val_loss: 0.6870 - val_acc: 0.7884\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6666 - acc: 0.7798 - val_loss: 0.6985 - val_acc: 0.7848\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6678 - acc: 0.7804 - val_loss: 0.6838 - val_acc: 0.7857\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6658 - acc: 0.7821 - val_loss: 0.6913 - val_acc: 0.7857\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6633 - acc: 0.7822 - val_loss: 0.6926 - val_acc: 0.7857\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6609 - acc: 0.7836 - val_loss: 0.6815 - val_acc: 0.7902\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6600 - acc: 0.7838 - val_loss: 0.6847 - val_acc: 0.7875\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6605 - acc: 0.7850 - val_loss: 0.6786 - val_acc: 0.7884\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6594 - acc: 0.7851 - val_loss: 0.6888 - val_acc: 0.7839\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6575 - acc: 0.7856 - val_loss: 0.6760 - val_acc: 0.7839\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 1s 57us/sample - loss: 0.6559 - acc: 0.7848 - val_loss: 0.6749 - val_acc: 0.7875\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6558 - acc: 0.7827 - val_loss: 0.6809 - val_acc: 0.7866\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6543 - acc: 0.7892 - val_loss: 0.6727 - val_acc: 0.7848\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6550 - acc: 0.7858 - val_loss: 0.6774 - val_acc: 0.7902\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6542 - acc: 0.7862 - val_loss: 0.7005 - val_acc: 0.7875\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6528 - acc: 0.7847 - val_loss: 0.6692 - val_acc: 0.7893\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6515 - acc: 0.7855 - val_loss: 0.6709 - val_acc: 0.7902\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6499 - acc: 0.7869 - val_loss: 0.6718 - val_acc: 0.7920\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6491 - acc: 0.7893 - val_loss: 0.6725 - val_acc: 0.7929\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6483 - acc: 0.7864 - val_loss: 0.6762 - val_acc: 0.7911\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6474 - acc: 0.7870 - val_loss: 0.6741 - val_acc: 0.7920\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6487 - acc: 0.7888 - val_loss: 0.6691 - val_acc: 0.7929\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6469 - acc: 0.7888 - val_loss: 0.6876 - val_acc: 0.7866\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6447 - acc: 0.7910 - val_loss: 0.6675 - val_acc: 0.7946\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6466 - acc: 0.7874 - val_loss: 0.6872 - val_acc: 0.7893\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6459 - acc: 0.7889 - val_loss: 0.6703 - val_acc: 0.7920\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6444 - acc: 0.7885 - val_loss: 0.6689 - val_acc: 0.7911\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6446 - acc: 0.7899 - val_loss: 0.6615 - val_acc: 0.7964\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6425 - acc: 0.7884 - val_loss: 0.6700 - val_acc: 0.7893\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6428 - acc: 0.7899 - val_loss: 0.6693 - val_acc: 0.7929\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6444 - acc: 0.7875 - val_loss: 0.6737 - val_acc: 0.7973\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6430 - acc: 0.7886 - val_loss: 0.6594 - val_acc: 0.7982\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6448 - acc: 0.7900 - val_loss: 0.6658 - val_acc: 0.7955\n",
            "Epoch 75/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6418 - acc: 0.7905 - val_loss: 0.6599 - val_acc: 0.7982\n",
            "Epoch 76/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6417 - acc: 0.7921 - val_loss: 0.6620 - val_acc: 0.7973\n",
            "Epoch 77/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6396 - acc: 0.7917 - val_loss: 0.6624 - val_acc: 0.7929\n",
            "Epoch 78/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6403 - acc: 0.7897 - val_loss: 0.6586 - val_acc: 0.7964\n",
            "Epoch 79/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6388 - acc: 0.7919 - val_loss: 0.6621 - val_acc: 0.7991\n",
            "Epoch 80/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6394 - acc: 0.7916 - val_loss: 0.6609 - val_acc: 0.7902\n",
            "Epoch 81/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6403 - acc: 0.7916 - val_loss: 0.6674 - val_acc: 0.7893\n",
            "Epoch 82/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6403 - acc: 0.7910 - val_loss: 0.6661 - val_acc: 0.7911\n",
            "Epoch 83/300\n",
            "10080/10080 [==============================] - 1s 82us/sample - loss: 0.6412 - acc: 0.7905 - val_loss: 0.6602 - val_acc: 0.7937\n",
            "Performance on the validation set\n",
            "loss: 0.66\n",
            "acc: 0.78\n",
            "\n",
            "FOLD:  2\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_17 (Model)             (None, 32)                578211    \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 578,574\n",
            "Trainable params: 363\n",
            "Non-trainable params: 578,211\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 124us/sample - loss: 8.6095 - acc: 0.0689 - val_loss: 6.5741 - val_acc: 0.0643\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 5.6496 - acc: 0.0893 - val_loss: 5.1878 - val_acc: 0.1036\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 4.4116 - acc: 0.1414 - val_loss: 4.0398 - val_acc: 0.1696\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 3.4038 - acc: 0.2227 - val_loss: 3.1571 - val_acc: 0.2554\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 2.6587 - acc: 0.3126 - val_loss: 2.5306 - val_acc: 0.3500\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 2.1390 - acc: 0.3976 - val_loss: 2.0797 - val_acc: 0.4214\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.7791 - acc: 0.4693 - val_loss: 1.7672 - val_acc: 0.4652\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.5291 - acc: 0.5276 - val_loss: 1.5267 - val_acc: 0.5241\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.3438 - acc: 0.5708 - val_loss: 1.3568 - val_acc: 0.5554\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.2069 - acc: 0.6082 - val_loss: 1.2357 - val_acc: 0.5938\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.1069 - acc: 0.6401 - val_loss: 1.1304 - val_acc: 0.6187\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.0294 - acc: 0.6611 - val_loss: 1.0625 - val_acc: 0.6509\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.9673 - acc: 0.6791 - val_loss: 1.0065 - val_acc: 0.6634\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.9212 - acc: 0.6909 - val_loss: 0.9526 - val_acc: 0.6830\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8813 - acc: 0.7076 - val_loss: 0.9197 - val_acc: 0.6848\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8490 - acc: 0.7188 - val_loss: 0.8992 - val_acc: 0.7027\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8242 - acc: 0.7258 - val_loss: 0.8609 - val_acc: 0.7143\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8016 - acc: 0.7347 - val_loss: 0.8493 - val_acc: 0.7170\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7850 - acc: 0.7414 - val_loss: 0.8238 - val_acc: 0.7268\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7660 - acc: 0.7481 - val_loss: 0.8129 - val_acc: 0.7304\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7551 - acc: 0.7519 - val_loss: 0.7972 - val_acc: 0.7357\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7440 - acc: 0.7556 - val_loss: 0.7836 - val_acc: 0.7402\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7323 - acc: 0.7592 - val_loss: 0.7733 - val_acc: 0.7473\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7229 - acc: 0.7627 - val_loss: 0.7576 - val_acc: 0.7527\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7158 - acc: 0.7658 - val_loss: 0.7540 - val_acc: 0.7545\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7106 - acc: 0.7663 - val_loss: 0.7474 - val_acc: 0.7545\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7029 - acc: 0.7722 - val_loss: 0.7443 - val_acc: 0.7563\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6966 - acc: 0.7732 - val_loss: 0.7307 - val_acc: 0.7643\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6929 - acc: 0.7715 - val_loss: 0.7281 - val_acc: 0.7589\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6887 - acc: 0.7728 - val_loss: 0.7277 - val_acc: 0.7634\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6849 - acc: 0.7771 - val_loss: 0.7240 - val_acc: 0.7607\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6809 - acc: 0.7761 - val_loss: 0.7178 - val_acc: 0.7616\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6787 - acc: 0.7784 - val_loss: 0.7117 - val_acc: 0.7643\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6726 - acc: 0.7794 - val_loss: 0.7103 - val_acc: 0.7679\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6713 - acc: 0.7790 - val_loss: 0.7064 - val_acc: 0.7661\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6676 - acc: 0.7808 - val_loss: 0.7036 - val_acc: 0.7679\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6669 - acc: 0.7831 - val_loss: 0.6985 - val_acc: 0.7661\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6668 - acc: 0.7842 - val_loss: 0.6953 - val_acc: 0.7732\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6629 - acc: 0.7799 - val_loss: 0.6945 - val_acc: 0.7670\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6606 - acc: 0.7822 - val_loss: 0.6989 - val_acc: 0.7589\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6574 - acc: 0.7860 - val_loss: 0.7066 - val_acc: 0.7607\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6575 - acc: 0.7848 - val_loss: 0.6921 - val_acc: 0.7652\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6555 - acc: 0.7850 - val_loss: 0.6968 - val_acc: 0.7714\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6538 - acc: 0.7880 - val_loss: 0.6875 - val_acc: 0.7714\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6520 - acc: 0.7871 - val_loss: 0.6950 - val_acc: 0.7607\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6517 - acc: 0.7887 - val_loss: 0.6952 - val_acc: 0.7625\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6496 - acc: 0.7883 - val_loss: 0.6861 - val_acc: 0.7625\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6505 - acc: 0.7893 - val_loss: 0.6914 - val_acc: 0.7625\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6510 - acc: 0.7863 - val_loss: 0.6844 - val_acc: 0.7714\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6469 - acc: 0.7882 - val_loss: 0.6825 - val_acc: 0.7661\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6462 - acc: 0.7897 - val_loss: 0.6825 - val_acc: 0.7741\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6440 - acc: 0.7915 - val_loss: 0.6804 - val_acc: 0.7741\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6447 - acc: 0.7901 - val_loss: 0.6820 - val_acc: 0.7696\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6457 - acc: 0.7870 - val_loss: 0.7056 - val_acc: 0.7670\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6433 - acc: 0.7906 - val_loss: 0.6800 - val_acc: 0.7741\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6417 - acc: 0.7894 - val_loss: 0.6788 - val_acc: 0.7723\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6433 - acc: 0.7894 - val_loss: 0.6718 - val_acc: 0.7723\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6404 - acc: 0.7922 - val_loss: 0.6808 - val_acc: 0.7750\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6415 - acc: 0.7900 - val_loss: 0.6825 - val_acc: 0.7661\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6397 - acc: 0.7923 - val_loss: 0.6945 - val_acc: 0.7714\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6385 - acc: 0.7929 - val_loss: 0.6696 - val_acc: 0.7786\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6401 - acc: 0.7927 - val_loss: 0.6828 - val_acc: 0.7759\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6371 - acc: 0.7922 - val_loss: 0.6825 - val_acc: 0.7705\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6377 - acc: 0.7938 - val_loss: 0.6862 - val_acc: 0.7679\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6396 - acc: 0.7927 - val_loss: 0.6744 - val_acc: 0.7759\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6365 - acc: 0.7919 - val_loss: 0.6720 - val_acc: 0.7839\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6353 - acc: 0.7923 - val_loss: 0.6697 - val_acc: 0.7759\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6372 - acc: 0.7917 - val_loss: 0.6799 - val_acc: 0.7705\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6356 - acc: 0.7946 - val_loss: 0.6674 - val_acc: 0.7759\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6373 - acc: 0.7925 - val_loss: 0.6694 - val_acc: 0.7768\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6330 - acc: 0.7930 - val_loss: 0.6819 - val_acc: 0.7750\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6371 - acc: 0.7921 - val_loss: 0.6727 - val_acc: 0.7750\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6345 - acc: 0.7938 - val_loss: 0.6715 - val_acc: 0.7750\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6369 - acc: 0.7931 - val_loss: 0.6747 - val_acc: 0.7750\n",
            "Epoch 75/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6350 - acc: 0.7912 - val_loss: 0.6763 - val_acc: 0.7741\n",
            "Epoch 76/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6333 - acc: 0.7933 - val_loss: 0.6683 - val_acc: 0.7759\n",
            "Epoch 77/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6332 - acc: 0.7945 - val_loss: 0.6700 - val_acc: 0.7786\n",
            "Epoch 78/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6336 - acc: 0.7928 - val_loss: 0.6700 - val_acc: 0.7732\n",
            "Epoch 79/300\n",
            "10080/10080 [==============================] - 1s 81us/sample - loss: 0.6335 - acc: 0.7922 - val_loss: 0.6675 - val_acc: 0.7759\n",
            "Performance on the validation set\n",
            "loss: 0.66\n",
            "acc: 0.77\n",
            "\n",
            "FOLD:  3\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_17 (Model)             (None, 32)                578211    \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 578,574\n",
            "Trainable params: 363\n",
            "Non-trainable params: 578,211\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 128us/sample - loss: 10.2407 - acc: 0.0888 - val_loss: 6.6120 - val_acc: 0.0920\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 5.3381 - acc: 0.1021 - val_loss: 4.5054 - val_acc: 0.1161\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 4.0173 - acc: 0.1506 - val_loss: 3.5345 - val_acc: 0.1973\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 3.1380 - acc: 0.2265 - val_loss: 2.7547 - val_acc: 0.2661\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 2.4823 - acc: 0.2999 - val_loss: 2.2085 - val_acc: 0.3393\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 2.0018 - acc: 0.3809 - val_loss: 1.7947 - val_acc: 0.4161\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.6590 - acc: 0.4548 - val_loss: 1.5101 - val_acc: 0.4964\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.4187 - acc: 0.5236 - val_loss: 1.3027 - val_acc: 0.5634\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.2513 - acc: 0.5775 - val_loss: 1.1675 - val_acc: 0.6205\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 1.1308 - acc: 0.6181 - val_loss: 1.0611 - val_acc: 0.6455\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.0424 - acc: 0.6524 - val_loss: 0.9868 - val_acc: 0.6723\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.9783 - acc: 0.6783 - val_loss: 0.9323 - val_acc: 0.7018\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.9276 - acc: 0.6930 - val_loss: 0.8888 - val_acc: 0.7205\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8892 - acc: 0.7073 - val_loss: 0.8566 - val_acc: 0.7357\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8591 - acc: 0.7166 - val_loss: 0.8286 - val_acc: 0.7384\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8317 - acc: 0.7301 - val_loss: 0.8018 - val_acc: 0.7500\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8106 - acc: 0.7364 - val_loss: 0.7835 - val_acc: 0.7473\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7937 - acc: 0.7429 - val_loss: 0.7636 - val_acc: 0.7527\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.7786 - acc: 0.7495 - val_loss: 0.7493 - val_acc: 0.7509\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7625 - acc: 0.7541 - val_loss: 0.7388 - val_acc: 0.7607\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7541 - acc: 0.7575 - val_loss: 0.7335 - val_acc: 0.7625\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7433 - acc: 0.7614 - val_loss: 0.7235 - val_acc: 0.7643\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7338 - acc: 0.7649 - val_loss: 0.7101 - val_acc: 0.7705\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7260 - acc: 0.7677 - val_loss: 0.7085 - val_acc: 0.7661\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7177 - acc: 0.7682 - val_loss: 0.6944 - val_acc: 0.7732\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7145 - acc: 0.7681 - val_loss: 0.6992 - val_acc: 0.7679\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7084 - acc: 0.7717 - val_loss: 0.6856 - val_acc: 0.7759\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7030 - acc: 0.7728 - val_loss: 0.6801 - val_acc: 0.7786\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6978 - acc: 0.7781 - val_loss: 0.6805 - val_acc: 0.7741\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6945 - acc: 0.7772 - val_loss: 0.6724 - val_acc: 0.7777\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6911 - acc: 0.7782 - val_loss: 0.6707 - val_acc: 0.7830\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6858 - acc: 0.7790 - val_loss: 0.6758 - val_acc: 0.7750\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6865 - acc: 0.7780 - val_loss: 0.6618 - val_acc: 0.7848\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6810 - acc: 0.7810 - val_loss: 0.6639 - val_acc: 0.7857\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6801 - acc: 0.7794 - val_loss: 0.6605 - val_acc: 0.7795\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6776 - acc: 0.7838 - val_loss: 0.6617 - val_acc: 0.7804\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6762 - acc: 0.7826 - val_loss: 0.6524 - val_acc: 0.7866\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6707 - acc: 0.7842 - val_loss: 0.6569 - val_acc: 0.7830\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6695 - acc: 0.7833 - val_loss: 0.6555 - val_acc: 0.7848\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6679 - acc: 0.7853 - val_loss: 0.6503 - val_acc: 0.7830\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6683 - acc: 0.7864 - val_loss: 0.6517 - val_acc: 0.7839\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6646 - acc: 0.7858 - val_loss: 0.6457 - val_acc: 0.7884\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6637 - acc: 0.7874 - val_loss: 0.6494 - val_acc: 0.7911\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6627 - acc: 0.7882 - val_loss: 0.6431 - val_acc: 0.7857\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6615 - acc: 0.7859 - val_loss: 0.6557 - val_acc: 0.7821\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6586 - acc: 0.7885 - val_loss: 0.6422 - val_acc: 0.7821\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6572 - acc: 0.7898 - val_loss: 0.6477 - val_acc: 0.7759\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6571 - acc: 0.7872 - val_loss: 0.6399 - val_acc: 0.7857\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6592 - acc: 0.7884 - val_loss: 0.6372 - val_acc: 0.7866\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6546 - acc: 0.7903 - val_loss: 0.6347 - val_acc: 0.7884\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6551 - acc: 0.7898 - val_loss: 0.6407 - val_acc: 0.7857\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6517 - acc: 0.7906 - val_loss: 0.6330 - val_acc: 0.7893\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6538 - acc: 0.7891 - val_loss: 0.6392 - val_acc: 0.7893\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6543 - acc: 0.7892 - val_loss: 0.6459 - val_acc: 0.7866\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6530 - acc: 0.7893 - val_loss: 0.6425 - val_acc: 0.7795\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6508 - acc: 0.7890 - val_loss: 0.6305 - val_acc: 0.7857\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6500 - acc: 0.7904 - val_loss: 0.6300 - val_acc: 0.7902\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6494 - acc: 0.7910 - val_loss: 0.6359 - val_acc: 0.7866\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6463 - acc: 0.7917 - val_loss: 0.6331 - val_acc: 0.7884\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6459 - acc: 0.7920 - val_loss: 0.6272 - val_acc: 0.7893\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6468 - acc: 0.7905 - val_loss: 0.6270 - val_acc: 0.7857\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6467 - acc: 0.7917 - val_loss: 0.6304 - val_acc: 0.7866\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6464 - acc: 0.7919 - val_loss: 0.6289 - val_acc: 0.7857\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6466 - acc: 0.7900 - val_loss: 0.6287 - val_acc: 0.7875\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6444 - acc: 0.7935 - val_loss: 0.6293 - val_acc: 0.7839\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6440 - acc: 0.7932 - val_loss: 0.6270 - val_acc: 0.7857\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6424 - acc: 0.7905 - val_loss: 0.6264 - val_acc: 0.7884\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6424 - acc: 0.7936 - val_loss: 0.6284 - val_acc: 0.7929\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6421 - acc: 0.7927 - val_loss: 0.6246 - val_acc: 0.7902\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6420 - acc: 0.7933 - val_loss: 0.6264 - val_acc: 0.7857\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6423 - acc: 0.7946 - val_loss: 0.6270 - val_acc: 0.7937\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6409 - acc: 0.7920 - val_loss: 0.6274 - val_acc: 0.7893\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6405 - acc: 0.7924 - val_loss: 0.6314 - val_acc: 0.7857\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6425 - acc: 0.7928 - val_loss: 0.6268 - val_acc: 0.7884\n",
            "Epoch 75/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6397 - acc: 0.7934 - val_loss: 0.6209 - val_acc: 0.7893\n",
            "Epoch 76/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6391 - acc: 0.7929 - val_loss: 0.6264 - val_acc: 0.7884\n",
            "Epoch 77/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6382 - acc: 0.7931 - val_loss: 0.6220 - val_acc: 0.7857\n",
            "Epoch 78/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6393 - acc: 0.7944 - val_loss: 0.6339 - val_acc: 0.7821\n",
            "Epoch 79/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6415 - acc: 0.7937 - val_loss: 0.6220 - val_acc: 0.7911\n",
            "Epoch 80/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6397 - acc: 0.7937 - val_loss: 0.6219 - val_acc: 0.7982\n",
            "Epoch 81/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6391 - acc: 0.7944 - val_loss: 0.6283 - val_acc: 0.7902\n",
            "Epoch 82/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6376 - acc: 0.7953 - val_loss: 0.6298 - val_acc: 0.7884\n",
            "Epoch 83/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6390 - acc: 0.7942 - val_loss: 0.6210 - val_acc: 0.7920\n",
            "Epoch 84/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6395 - acc: 0.7917 - val_loss: 0.6205 - val_acc: 0.7964\n",
            "Epoch 85/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6367 - acc: 0.7943 - val_loss: 0.6181 - val_acc: 0.7964\n",
            "Epoch 86/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6381 - acc: 0.7944 - val_loss: 0.6198 - val_acc: 0.7875\n",
            "Epoch 87/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6390 - acc: 0.7930 - val_loss: 0.6198 - val_acc: 0.7946\n",
            "Epoch 88/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6356 - acc: 0.7979 - val_loss: 0.6246 - val_acc: 0.7946\n",
            "Epoch 89/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6363 - acc: 0.7952 - val_loss: 0.6215 - val_acc: 0.7875\n",
            "Epoch 90/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6354 - acc: 0.7941 - val_loss: 0.6156 - val_acc: 0.7964\n",
            "Epoch 91/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6346 - acc: 0.7975 - val_loss: 0.6252 - val_acc: 0.7875\n",
            "Epoch 92/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6355 - acc: 0.7941 - val_loss: 0.6291 - val_acc: 0.7875\n",
            "Epoch 93/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6353 - acc: 0.7956 - val_loss: 0.6191 - val_acc: 0.7964\n",
            "Epoch 94/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6373 - acc: 0.7958 - val_loss: 0.6185 - val_acc: 0.7875\n",
            "Epoch 95/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6356 - acc: 0.7933 - val_loss: 0.6207 - val_acc: 0.7884\n",
            "Epoch 96/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6351 - acc: 0.7968 - val_loss: 0.6190 - val_acc: 0.7857\n",
            "Epoch 97/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6343 - acc: 0.7937 - val_loss: 0.6210 - val_acc: 0.7964\n",
            "Epoch 98/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6332 - acc: 0.7969 - val_loss: 0.6218 - val_acc: 0.7920\n",
            "Epoch 99/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6344 - acc: 0.7943 - val_loss: 0.6249 - val_acc: 0.7920\n",
            "Epoch 100/300\n",
            "10080/10080 [==============================] - 1s 82us/sample - loss: 0.6345 - acc: 0.7966 - val_loss: 0.6159 - val_acc: 0.7893\n",
            "Performance on the validation set\n",
            "loss: 0.64\n",
            "acc: 0.79\n",
            "\n",
            "FOLD:  4\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_17 (Model)             (None, 32)                578211    \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 578,574\n",
            "Trainable params: 363\n",
            "Non-trainable params: 578,211\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 130us/sample - loss: 10.2038 - acc: 0.1227 - val_loss: 6.2322 - val_acc: 0.1366\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 5.0476 - acc: 0.1670 - val_loss: 3.8530 - val_acc: 0.2196\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 3.4869 - acc: 0.2374 - val_loss: 3.0523 - val_acc: 0.2937\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 2.7838 - acc: 0.3092 - val_loss: 2.4373 - val_acc: 0.3562\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 2.2587 - acc: 0.3779 - val_loss: 1.9997 - val_acc: 0.4375\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.8852 - acc: 0.4435 - val_loss: 1.6771 - val_acc: 0.5071\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.6159 - acc: 0.5003 - val_loss: 1.4649 - val_acc: 0.5554\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.4241 - acc: 0.5462 - val_loss: 1.2947 - val_acc: 0.5866\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.2804 - acc: 0.5861 - val_loss: 1.1750 - val_acc: 0.6277\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.1735 - acc: 0.6186 - val_loss: 1.0814 - val_acc: 0.6571\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.0887 - acc: 0.6466 - val_loss: 1.0096 - val_acc: 0.6812\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.0214 - acc: 0.6680 - val_loss: 0.9559 - val_acc: 0.6920\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.9692 - acc: 0.6848 - val_loss: 0.9049 - val_acc: 0.7089\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.9249 - acc: 0.7005 - val_loss: 0.8736 - val_acc: 0.7170\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8897 - acc: 0.7136 - val_loss: 0.8437 - val_acc: 0.7223\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8599 - acc: 0.7156 - val_loss: 0.8221 - val_acc: 0.7348\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8337 - acc: 0.7279 - val_loss: 0.7897 - val_acc: 0.7402\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8128 - acc: 0.7337 - val_loss: 0.7742 - val_acc: 0.7393\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7918 - acc: 0.7422 - val_loss: 0.7556 - val_acc: 0.7491\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7776 - acc: 0.7466 - val_loss: 0.7463 - val_acc: 0.7527\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7631 - acc: 0.7515 - val_loss: 0.7310 - val_acc: 0.7455\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7507 - acc: 0.7573 - val_loss: 0.7222 - val_acc: 0.7652\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7424 - acc: 0.7616 - val_loss: 0.7085 - val_acc: 0.7598\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7292 - acc: 0.7654 - val_loss: 0.7045 - val_acc: 0.7670\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.7225 - acc: 0.7626 - val_loss: 0.6929 - val_acc: 0.7723\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7159 - acc: 0.7668 - val_loss: 0.6846 - val_acc: 0.7750\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7089 - acc: 0.7692 - val_loss: 0.6820 - val_acc: 0.7750\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7041 - acc: 0.7732 - val_loss: 0.6802 - val_acc: 0.7714\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6976 - acc: 0.7733 - val_loss: 0.6701 - val_acc: 0.7768\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6932 - acc: 0.7755 - val_loss: 0.6646 - val_acc: 0.7759\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6898 - acc: 0.7753 - val_loss: 0.6621 - val_acc: 0.7830\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6844 - acc: 0.7784 - val_loss: 0.6570 - val_acc: 0.7821\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6836 - acc: 0.7783 - val_loss: 0.6557 - val_acc: 0.7804\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6788 - acc: 0.7792 - val_loss: 0.6509 - val_acc: 0.7839\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6748 - acc: 0.7811 - val_loss: 0.6576 - val_acc: 0.7723\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6756 - acc: 0.7828 - val_loss: 0.6486 - val_acc: 0.7795\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6728 - acc: 0.7818 - val_loss: 0.6550 - val_acc: 0.7759\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6706 - acc: 0.7816 - val_loss: 0.6397 - val_acc: 0.7893\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6654 - acc: 0.7875 - val_loss: 0.6432 - val_acc: 0.7830\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6671 - acc: 0.7834 - val_loss: 0.6461 - val_acc: 0.7777\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6635 - acc: 0.7858 - val_loss: 0.6420 - val_acc: 0.7848\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6617 - acc: 0.7865 - val_loss: 0.6344 - val_acc: 0.7866\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6610 - acc: 0.7888 - val_loss: 0.6358 - val_acc: 0.7920\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6595 - acc: 0.7866 - val_loss: 0.6358 - val_acc: 0.7839\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6570 - acc: 0.7882 - val_loss: 0.6346 - val_acc: 0.7848\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6558 - acc: 0.7893 - val_loss: 0.6388 - val_acc: 0.7857\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6560 - acc: 0.7888 - val_loss: 0.6426 - val_acc: 0.7795\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6539 - acc: 0.7874 - val_loss: 0.6382 - val_acc: 0.7866\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6546 - acc: 0.7873 - val_loss: 0.6379 - val_acc: 0.7830\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6511 - acc: 0.7865 - val_loss: 0.6410 - val_acc: 0.7821\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6531 - acc: 0.7899 - val_loss: 0.6330 - val_acc: 0.7902\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6544 - acc: 0.7913 - val_loss: 0.6349 - val_acc: 0.7866\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6491 - acc: 0.7889 - val_loss: 0.6323 - val_acc: 0.7812\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6484 - acc: 0.7936 - val_loss: 0.6286 - val_acc: 0.7884\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6483 - acc: 0.7932 - val_loss: 0.6238 - val_acc: 0.7866\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6477 - acc: 0.7901 - val_loss: 0.6239 - val_acc: 0.7929\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6481 - acc: 0.7910 - val_loss: 0.6430 - val_acc: 0.7804\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6486 - acc: 0.7909 - val_loss: 0.6271 - val_acc: 0.7866\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6444 - acc: 0.7933 - val_loss: 0.6280 - val_acc: 0.7866\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6448 - acc: 0.7917 - val_loss: 0.6261 - val_acc: 0.7848\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6467 - acc: 0.7900 - val_loss: 0.6268 - val_acc: 0.7839\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6450 - acc: 0.7920 - val_loss: 0.6242 - val_acc: 0.7884\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6430 - acc: 0.7937 - val_loss: 0.6267 - val_acc: 0.7902\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6433 - acc: 0.7931 - val_loss: 0.6237 - val_acc: 0.7839\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 1s 89us/sample - loss: 0.6428 - acc: 0.7929 - val_loss: 0.6266 - val_acc: 0.7937\n",
            "Performance on the validation set\n",
            "loss: 0.66\n",
            "acc: 0.78\n",
            "\n",
            "FOLD:  5\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_17 (Model)             (None, 32)                578211    \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 578,574\n",
            "Trainable params: 363\n",
            "Non-trainable params: 578,211\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 134us/sample - loss: 9.9594 - acc: 0.1353 - val_loss: 6.6612 - val_acc: 0.1804\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 4.9063 - acc: 0.1940 - val_loss: 4.1760 - val_acc: 0.2045\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 3.5140 - acc: 0.2383 - val_loss: 3.2929 - val_acc: 0.2670\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 2.7285 - acc: 0.3161 - val_loss: 2.6396 - val_acc: 0.3357\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 2.1914 - acc: 0.4043 - val_loss: 2.1877 - val_acc: 0.4071\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.8336 - acc: 0.4751 - val_loss: 1.8746 - val_acc: 0.4714\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.5902 - acc: 0.5348 - val_loss: 1.6657 - val_acc: 0.5116\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.4237 - acc: 0.5785 - val_loss: 1.5036 - val_acc: 0.5714\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.3023 - acc: 0.6122 - val_loss: 1.3946 - val_acc: 0.6089\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.2086 - acc: 0.6358 - val_loss: 1.3001 - val_acc: 0.6321\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.1322 - acc: 0.6562 - val_loss: 1.2221 - val_acc: 0.6554\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 1.0711 - acc: 0.6700 - val_loss: 1.1600 - val_acc: 0.6679\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.0197 - acc: 0.6812 - val_loss: 1.1017 - val_acc: 0.6821\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.9766 - acc: 0.6941 - val_loss: 1.0574 - val_acc: 0.6902\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.9392 - acc: 0.7045 - val_loss: 1.0113 - val_acc: 0.7054\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.9058 - acc: 0.7118 - val_loss: 0.9755 - val_acc: 0.7071\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8775 - acc: 0.7189 - val_loss: 0.9344 - val_acc: 0.7205\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8518 - acc: 0.7243 - val_loss: 0.9077 - val_acc: 0.7107\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8291 - acc: 0.7302 - val_loss: 0.8847 - val_acc: 0.7188\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8089 - acc: 0.7377 - val_loss: 0.8526 - val_acc: 0.7348\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.7911 - acc: 0.7412 - val_loss: 0.8303 - val_acc: 0.7339\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7755 - acc: 0.7472 - val_loss: 0.8156 - val_acc: 0.7491\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7615 - acc: 0.7498 - val_loss: 0.7925 - val_acc: 0.7455\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7476 - acc: 0.7541 - val_loss: 0.7826 - val_acc: 0.7563\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.7364 - acc: 0.7567 - val_loss: 0.7736 - val_acc: 0.7536\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7276 - acc: 0.7583 - val_loss: 0.7595 - val_acc: 0.7598\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.7187 - acc: 0.7629 - val_loss: 0.7522 - val_acc: 0.7563\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7110 - acc: 0.7642 - val_loss: 0.7354 - val_acc: 0.7643\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7027 - acc: 0.7659 - val_loss: 0.7225 - val_acc: 0.7768\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6989 - acc: 0.7694 - val_loss: 0.7241 - val_acc: 0.7705\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6904 - acc: 0.7714 - val_loss: 0.7119 - val_acc: 0.7768\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6860 - acc: 0.7736 - val_loss: 0.7006 - val_acc: 0.7741\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6826 - acc: 0.7738 - val_loss: 0.6960 - val_acc: 0.7839\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6772 - acc: 0.7766 - val_loss: 0.7011 - val_acc: 0.7768\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6737 - acc: 0.7769 - val_loss: 0.6881 - val_acc: 0.7848\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6709 - acc: 0.7790 - val_loss: 0.6835 - val_acc: 0.7884\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6661 - acc: 0.7812 - val_loss: 0.6804 - val_acc: 0.7866\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6644 - acc: 0.7817 - val_loss: 0.6798 - val_acc: 0.7875\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6613 - acc: 0.7845 - val_loss: 0.6733 - val_acc: 0.7937\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6599 - acc: 0.7832 - val_loss: 0.6767 - val_acc: 0.7937\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6577 - acc: 0.7854 - val_loss: 0.6690 - val_acc: 0.7929\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6535 - acc: 0.7867 - val_loss: 0.6703 - val_acc: 0.7929\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6563 - acc: 0.7847 - val_loss: 0.6622 - val_acc: 0.7929\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6528 - acc: 0.7865 - val_loss: 0.6687 - val_acc: 0.7830\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6526 - acc: 0.7844 - val_loss: 0.6617 - val_acc: 0.7920\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6505 - acc: 0.7856 - val_loss: 0.6626 - val_acc: 0.7955\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6481 - acc: 0.7890 - val_loss: 0.6594 - val_acc: 0.7937\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6485 - acc: 0.7890 - val_loss: 0.6602 - val_acc: 0.7946\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6463 - acc: 0.7895 - val_loss: 0.6555 - val_acc: 0.7920\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6448 - acc: 0.7886 - val_loss: 0.6586 - val_acc: 0.7902\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6439 - acc: 0.7887 - val_loss: 0.6523 - val_acc: 0.8018\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6449 - acc: 0.7889 - val_loss: 0.6519 - val_acc: 0.7946\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6416 - acc: 0.7893 - val_loss: 0.6509 - val_acc: 0.7991\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6405 - acc: 0.7909 - val_loss: 0.6513 - val_acc: 0.7991\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6412 - acc: 0.7892 - val_loss: 0.6488 - val_acc: 0.7982\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6403 - acc: 0.7911 - val_loss: 0.6583 - val_acc: 0.7875\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6408 - acc: 0.7913 - val_loss: 0.6492 - val_acc: 0.8027\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6402 - acc: 0.7879 - val_loss: 0.6548 - val_acc: 0.7955\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6390 - acc: 0.7918 - val_loss: 0.6444 - val_acc: 0.7982\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6365 - acc: 0.7920 - val_loss: 0.6502 - val_acc: 0.7973\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6363 - acc: 0.7942 - val_loss: 0.6583 - val_acc: 0.7884\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6401 - acc: 0.7903 - val_loss: 0.6503 - val_acc: 0.7946\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6354 - acc: 0.7943 - val_loss: 0.6465 - val_acc: 0.7973\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6366 - acc: 0.7937 - val_loss: 0.6471 - val_acc: 0.7929\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6359 - acc: 0.7925 - val_loss: 0.6419 - val_acc: 0.7991\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6379 - acc: 0.7914 - val_loss: 0.6459 - val_acc: 0.7964\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6358 - acc: 0.7916 - val_loss: 0.6440 - val_acc: 0.7982\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6346 - acc: 0.7937 - val_loss: 0.6445 - val_acc: 0.8009\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6351 - acc: 0.7922 - val_loss: 0.6445 - val_acc: 0.7991\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6341 - acc: 0.7938 - val_loss: 0.6457 - val_acc: 0.7946\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6343 - acc: 0.7955 - val_loss: 0.6493 - val_acc: 0.7955\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6359 - acc: 0.7934 - val_loss: 0.6473 - val_acc: 0.7955\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6343 - acc: 0.7933 - val_loss: 0.6420 - val_acc: 0.8000\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6325 - acc: 0.7913 - val_loss: 0.6464 - val_acc: 0.7991\n",
            "Epoch 75/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6320 - acc: 0.7937 - val_loss: 0.6409 - val_acc: 0.8018\n",
            "Epoch 76/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6335 - acc: 0.7940 - val_loss: 0.6409 - val_acc: 0.8000\n",
            "Epoch 77/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6333 - acc: 0.7914 - val_loss: 0.6383 - val_acc: 0.8009\n",
            "Epoch 78/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6330 - acc: 0.7940 - val_loss: 0.6486 - val_acc: 0.7973\n",
            "Epoch 79/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6315 - acc: 0.7933 - val_loss: 0.6401 - val_acc: 0.8036\n",
            "Epoch 80/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6323 - acc: 0.7954 - val_loss: 0.6477 - val_acc: 0.7946\n",
            "Epoch 81/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6321 - acc: 0.7965 - val_loss: 0.6411 - val_acc: 0.7982\n",
            "Epoch 82/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6315 - acc: 0.7946 - val_loss: 0.6457 - val_acc: 0.7982\n",
            "Epoch 83/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6309 - acc: 0.7949 - val_loss: 0.6486 - val_acc: 0.7911\n",
            "Epoch 84/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6331 - acc: 0.7934 - val_loss: 0.6492 - val_acc: 0.7991\n",
            "Epoch 85/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6331 - acc: 0.7908 - val_loss: 0.6415 - val_acc: 0.8000\n",
            "Epoch 86/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6316 - acc: 0.7945 - val_loss: 0.6410 - val_acc: 0.8000\n",
            "Epoch 87/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6309 - acc: 0.7956 - val_loss: 0.6353 - val_acc: 0.8018\n",
            "Epoch 88/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6295 - acc: 0.7943 - val_loss: 0.6445 - val_acc: 0.7946\n",
            "Epoch 89/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6296 - acc: 0.7960 - val_loss: 0.6421 - val_acc: 0.7982\n",
            "Epoch 90/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6314 - acc: 0.7940 - val_loss: 0.6400 - val_acc: 0.8009\n",
            "Epoch 91/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6300 - acc: 0.7963 - val_loss: 0.6457 - val_acc: 0.8009\n",
            "Epoch 92/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6322 - acc: 0.7938 - val_loss: 0.6415 - val_acc: 0.7973\n",
            "Epoch 93/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6304 - acc: 0.7959 - val_loss: 0.6435 - val_acc: 0.8009\n",
            "Epoch 94/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6307 - acc: 0.7946 - val_loss: 0.6502 - val_acc: 0.7893\n",
            "Epoch 95/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6309 - acc: 0.7953 - val_loss: 0.6432 - val_acc: 0.7964\n",
            "Epoch 96/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6316 - acc: 0.7948 - val_loss: 0.6427 - val_acc: 0.7991\n",
            "Epoch 97/300\n",
            "10080/10080 [==============================] - 1s 88us/sample - loss: 0.6308 - acc: 0.7938 - val_loss: 0.6375 - val_acc: 0.8098\n",
            "Performance on the validation set\n",
            "loss: 0.64\n",
            "acc: 0.79\n",
            "\n",
            "FOLD:  6\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_17 (Model)             (None, 32)                578211    \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 578,574\n",
            "Trainable params: 363\n",
            "Non-trainable params: 578,211\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 138us/sample - loss: 16.3024 - acc: 0.0725 - val_loss: 8.8834 - val_acc: 0.0643\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 7.4559 - acc: 0.1194 - val_loss: 5.8941 - val_acc: 0.1571\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 5.1045 - acc: 0.1767 - val_loss: 4.3231 - val_acc: 0.2036\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 4.0034 - acc: 0.2340 - val_loss: 3.5288 - val_acc: 0.2705\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 3.2829 - acc: 0.2969 - val_loss: 2.9038 - val_acc: 0.3384\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 2.7192 - acc: 0.3580 - val_loss: 2.4219 - val_acc: 0.3839\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 2.2792 - acc: 0.4151 - val_loss: 2.0477 - val_acc: 0.4500\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 1.9350 - acc: 0.4691 - val_loss: 1.7477 - val_acc: 0.4830\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.6636 - acc: 0.5078 - val_loss: 1.5245 - val_acc: 0.5241\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.4582 - acc: 0.5514 - val_loss: 1.3527 - val_acc: 0.5580\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.2991 - acc: 0.5878 - val_loss: 1.2220 - val_acc: 0.5991\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.1859 - acc: 0.6151 - val_loss: 1.1264 - val_acc: 0.6357\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 1.0963 - acc: 0.6437 - val_loss: 1.0608 - val_acc: 0.6500\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 1.0290 - acc: 0.6671 - val_loss: 0.9939 - val_acc: 0.6750\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.9770 - acc: 0.6867 - val_loss: 0.9484 - val_acc: 0.6911\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.9365 - acc: 0.6999 - val_loss: 0.9109 - val_acc: 0.7045\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 0.8998 - acc: 0.7112 - val_loss: 0.8777 - val_acc: 0.7143\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8706 - acc: 0.7195 - val_loss: 0.8598 - val_acc: 0.7152\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.8474 - acc: 0.7276 - val_loss: 0.8317 - val_acc: 0.7339\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8257 - acc: 0.7369 - val_loss: 0.8188 - val_acc: 0.7277\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8063 - acc: 0.7418 - val_loss: 0.8005 - val_acc: 0.7348\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7917 - acc: 0.7453 - val_loss: 0.7891 - val_acc: 0.7420\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7797 - acc: 0.7477 - val_loss: 0.7734 - val_acc: 0.7563\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7672 - acc: 0.7539 - val_loss: 0.7617 - val_acc: 0.7563\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7547 - acc: 0.7581 - val_loss: 0.7581 - val_acc: 0.7536\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7447 - acc: 0.7602 - val_loss: 0.7458 - val_acc: 0.7589\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7352 - acc: 0.7644 - val_loss: 0.7491 - val_acc: 0.7589\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.7301 - acc: 0.7647 - val_loss: 0.7358 - val_acc: 0.7607\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7227 - acc: 0.7688 - val_loss: 0.7241 - val_acc: 0.7643\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7173 - acc: 0.7683 - val_loss: 0.7204 - val_acc: 0.7625\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.7098 - acc: 0.7722 - val_loss: 0.7182 - val_acc: 0.7723\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7075 - acc: 0.7689 - val_loss: 0.7131 - val_acc: 0.7580\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.7001 - acc: 0.7761 - val_loss: 0.7076 - val_acc: 0.7688\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6954 - acc: 0.7785 - val_loss: 0.7121 - val_acc: 0.7607\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6950 - acc: 0.7808 - val_loss: 0.7008 - val_acc: 0.7714\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6881 - acc: 0.7802 - val_loss: 0.6947 - val_acc: 0.7670\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6854 - acc: 0.7800 - val_loss: 0.6938 - val_acc: 0.7634\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6807 - acc: 0.7823 - val_loss: 0.6936 - val_acc: 0.7714\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6784 - acc: 0.7851 - val_loss: 0.6961 - val_acc: 0.7616\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6757 - acc: 0.7829 - val_loss: 0.6858 - val_acc: 0.7705\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6711 - acc: 0.7838 - val_loss: 0.6838 - val_acc: 0.7741\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6711 - acc: 0.7843 - val_loss: 0.6810 - val_acc: 0.7679\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6662 - acc: 0.7864 - val_loss: 0.6790 - val_acc: 0.7723\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6676 - acc: 0.7865 - val_loss: 0.6873 - val_acc: 0.7723\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6649 - acc: 0.7863 - val_loss: 0.6769 - val_acc: 0.7732\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6619 - acc: 0.7890 - val_loss: 0.6735 - val_acc: 0.7732\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6593 - acc: 0.7877 - val_loss: 0.6756 - val_acc: 0.7759\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6593 - acc: 0.7896 - val_loss: 0.6739 - val_acc: 0.7679\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6574 - acc: 0.7863 - val_loss: 0.6710 - val_acc: 0.7777\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6543 - acc: 0.7895 - val_loss: 0.6734 - val_acc: 0.7777\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6553 - acc: 0.7892 - val_loss: 0.6658 - val_acc: 0.7759\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6557 - acc: 0.7896 - val_loss: 0.6679 - val_acc: 0.7768\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6504 - acc: 0.7917 - val_loss: 0.6692 - val_acc: 0.7741\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6497 - acc: 0.7923 - val_loss: 0.6698 - val_acc: 0.7661\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6500 - acc: 0.7915 - val_loss: 0.6697 - val_acc: 0.7732\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6485 - acc: 0.7907 - val_loss: 0.6675 - val_acc: 0.7714\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6475 - acc: 0.7894 - val_loss: 0.6617 - val_acc: 0.7804\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 0.6489 - acc: 0.7886 - val_loss: 0.6590 - val_acc: 0.7759\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6452 - acc: 0.7941 - val_loss: 0.6774 - val_acc: 0.7714\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6461 - acc: 0.7917 - val_loss: 0.6653 - val_acc: 0.7786\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6436 - acc: 0.7933 - val_loss: 0.6588 - val_acc: 0.7830\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6419 - acc: 0.7960 - val_loss: 0.6629 - val_acc: 0.7723\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6417 - acc: 0.7935 - val_loss: 0.6672 - val_acc: 0.7688\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6424 - acc: 0.7935 - val_loss: 0.6556 - val_acc: 0.7777\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6408 - acc: 0.7958 - val_loss: 0.6610 - val_acc: 0.7705\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 0.6404 - acc: 0.7935 - val_loss: 0.6538 - val_acc: 0.7741\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6393 - acc: 0.7954 - val_loss: 0.6564 - val_acc: 0.7723\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6380 - acc: 0.7947 - val_loss: 0.6565 - val_acc: 0.7777\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6383 - acc: 0.7947 - val_loss: 0.6593 - val_acc: 0.7839\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6372 - acc: 0.7946 - val_loss: 0.6566 - val_acc: 0.7768\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6370 - acc: 0.7954 - val_loss: 0.6512 - val_acc: 0.7759\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6382 - acc: 0.7937 - val_loss: 0.6545 - val_acc: 0.7786\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6364 - acc: 0.7935 - val_loss: 0.6663 - val_acc: 0.7786\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6386 - acc: 0.7926 - val_loss: 0.6571 - val_acc: 0.7795\n",
            "Epoch 75/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6340 - acc: 0.7975 - val_loss: 0.6567 - val_acc: 0.7786\n",
            "Epoch 76/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6343 - acc: 0.7968 - val_loss: 0.6559 - val_acc: 0.7750\n",
            "Epoch 77/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6323 - acc: 0.7978 - val_loss: 0.6553 - val_acc: 0.7804\n",
            "Epoch 78/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6324 - acc: 0.7970 - val_loss: 0.6579 - val_acc: 0.7786\n",
            "Epoch 79/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6327 - acc: 0.7967 - val_loss: 0.6508 - val_acc: 0.7866\n",
            "Epoch 80/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6336 - acc: 0.7962 - val_loss: 0.6537 - val_acc: 0.7786\n",
            "Epoch 81/300\n",
            "10080/10080 [==============================] - 1s 88us/sample - loss: 0.6337 - acc: 0.7938 - val_loss: 0.6601 - val_acc: 0.7804\n",
            "Performance on the validation set\n",
            "loss: 0.65\n",
            "acc: 0.78\n",
            "\n",
            "FOLD:  7\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_17 (Model)             (None, 32)                578211    \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 578,574\n",
            "Trainable params: 363\n",
            "Non-trainable params: 578,211\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 138us/sample - loss: 8.4339 - acc: 0.1632 - val_loss: 4.8080 - val_acc: 0.2223\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 4.4756 - acc: 0.2150 - val_loss: 3.7904 - val_acc: 0.2580\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 3.6096 - acc: 0.2784 - val_loss: 3.0515 - val_acc: 0.3232\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 2.9135 - acc: 0.3439 - val_loss: 2.4748 - val_acc: 0.3902\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 2.3795 - acc: 0.4095 - val_loss: 2.0296 - val_acc: 0.4455\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 1.9789 - acc: 0.4675 - val_loss: 1.7135 - val_acc: 0.5071\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.6819 - acc: 0.5205 - val_loss: 1.4767 - val_acc: 0.5598\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.4674 - acc: 0.5690 - val_loss: 1.3098 - val_acc: 0.6027\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.3063 - acc: 0.6061 - val_loss: 1.1856 - val_acc: 0.6384\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.1870 - acc: 0.6398 - val_loss: 1.0904 - val_acc: 0.6607\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.0962 - acc: 0.6648 - val_loss: 1.0185 - val_acc: 0.6777\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.0242 - acc: 0.6868 - val_loss: 0.9617 - val_acc: 0.6893\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.9676 - acc: 0.7024 - val_loss: 0.9142 - val_acc: 0.7098\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.9213 - acc: 0.7146 - val_loss: 0.8778 - val_acc: 0.7143\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8832 - acc: 0.7273 - val_loss: 0.8487 - val_acc: 0.7214\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8534 - acc: 0.7329 - val_loss: 0.8230 - val_acc: 0.7259\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.8267 - acc: 0.7393 - val_loss: 0.8014 - val_acc: 0.7286\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8058 - acc: 0.7466 - val_loss: 0.7835 - val_acc: 0.7357\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7875 - acc: 0.7530 - val_loss: 0.7698 - val_acc: 0.7437\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7724 - acc: 0.7535 - val_loss: 0.7591 - val_acc: 0.7509\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7599 - acc: 0.7597 - val_loss: 0.7429 - val_acc: 0.7607\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7488 - acc: 0.7598 - val_loss: 0.7355 - val_acc: 0.7589\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7396 - acc: 0.7633 - val_loss: 0.7339 - val_acc: 0.7643\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7289 - acc: 0.7689 - val_loss: 0.7185 - val_acc: 0.7714\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7215 - acc: 0.7677 - val_loss: 0.7138 - val_acc: 0.7714\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7153 - acc: 0.7707 - val_loss: 0.7094 - val_acc: 0.7696\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7074 - acc: 0.7722 - val_loss: 0.7114 - val_acc: 0.7786\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7022 - acc: 0.7747 - val_loss: 0.7011 - val_acc: 0.7696\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6988 - acc: 0.7756 - val_loss: 0.6929 - val_acc: 0.7768\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6935 - acc: 0.7770 - val_loss: 0.6873 - val_acc: 0.7795\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6884 - acc: 0.7802 - val_loss: 0.6854 - val_acc: 0.7777\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6859 - acc: 0.7803 - val_loss: 0.6807 - val_acc: 0.7857\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6799 - acc: 0.7812 - val_loss: 0.6817 - val_acc: 0.7768\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6791 - acc: 0.7824 - val_loss: 0.6758 - val_acc: 0.7839\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6756 - acc: 0.7830 - val_loss: 0.6776 - val_acc: 0.7768\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 0.6727 - acc: 0.7842 - val_loss: 0.6779 - val_acc: 0.7786\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6724 - acc: 0.7819 - val_loss: 0.6711 - val_acc: 0.7812\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6688 - acc: 0.7834 - val_loss: 0.6721 - val_acc: 0.7741\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6663 - acc: 0.7870 - val_loss: 0.6694 - val_acc: 0.7857\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6643 - acc: 0.7857 - val_loss: 0.6719 - val_acc: 0.7821\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6609 - acc: 0.7866 - val_loss: 0.6616 - val_acc: 0.7848\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6614 - acc: 0.7854 - val_loss: 0.6657 - val_acc: 0.7839\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6607 - acc: 0.7866 - val_loss: 0.6604 - val_acc: 0.7902\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6582 - acc: 0.7873 - val_loss: 0.6606 - val_acc: 0.7857\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6585 - acc: 0.7891 - val_loss: 0.6606 - val_acc: 0.7866\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6559 - acc: 0.7869 - val_loss: 0.6601 - val_acc: 0.7839\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6567 - acc: 0.7883 - val_loss: 0.6577 - val_acc: 0.7911\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6535 - acc: 0.7885 - val_loss: 0.6629 - val_acc: 0.7857\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6544 - acc: 0.7860 - val_loss: 0.6625 - val_acc: 0.7920\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6504 - acc: 0.7910 - val_loss: 0.6505 - val_acc: 0.7920\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6533 - acc: 0.7879 - val_loss: 0.6517 - val_acc: 0.7955\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6511 - acc: 0.7870 - val_loss: 0.6580 - val_acc: 0.7875\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6526 - acc: 0.7912 - val_loss: 0.6594 - val_acc: 0.7893\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6513 - acc: 0.7916 - val_loss: 0.6539 - val_acc: 0.7946\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6495 - acc: 0.7874 - val_loss: 0.6528 - val_acc: 0.7920\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6469 - acc: 0.7894 - val_loss: 0.6509 - val_acc: 0.7911\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6474 - acc: 0.7872 - val_loss: 0.6518 - val_acc: 0.7911\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6485 - acc: 0.7883 - val_loss: 0.6523 - val_acc: 0.7902\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6485 - acc: 0.7887 - val_loss: 0.6508 - val_acc: 0.7929\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 1s 90us/sample - loss: 0.6447 - acc: 0.7899 - val_loss: 0.6507 - val_acc: 0.7893\n",
            "Performance on the validation set\n",
            "loss: 0.66\n",
            "acc: 0.78\n",
            "\n",
            "FOLD:  8\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_17 (Model)             (None, 32)                578211    \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 578,574\n",
            "Trainable params: 363\n",
            "Non-trainable params: 578,211\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 137us/sample - loss: 11.8687 - acc: 0.1129 - val_loss: 7.0609 - val_acc: 0.1232\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 5.2204 - acc: 0.1909 - val_loss: 4.1572 - val_acc: 0.2098\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 3.6261 - acc: 0.2500 - val_loss: 3.3549 - val_acc: 0.2723\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 2.9364 - acc: 0.3086 - val_loss: 2.7520 - val_acc: 0.3232\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 2.4001 - acc: 0.3672 - val_loss: 2.2628 - val_acc: 0.3795\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.9895 - acc: 0.4243 - val_loss: 1.8918 - val_acc: 0.4527\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.6826 - acc: 0.4854 - val_loss: 1.6136 - val_acc: 0.5241\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.4576 - acc: 0.5412 - val_loss: 1.4114 - val_acc: 0.5741\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.2925 - acc: 0.5881 - val_loss: 1.2630 - val_acc: 0.6196\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 1.1681 - acc: 0.6285 - val_loss: 1.1462 - val_acc: 0.6554\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.0759 - acc: 0.6552 - val_loss: 1.0683 - val_acc: 0.6786\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 1.0080 - acc: 0.6787 - val_loss: 1.0018 - val_acc: 0.6982\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.9511 - acc: 0.6933 - val_loss: 0.9545 - val_acc: 0.7089\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.9060 - acc: 0.7077 - val_loss: 0.9117 - val_acc: 0.7268\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8743 - acc: 0.7188 - val_loss: 0.8827 - val_acc: 0.7366\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 38us/sample - loss: 0.8415 - acc: 0.7276 - val_loss: 0.8490 - val_acc: 0.7384\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8175 - acc: 0.7365 - val_loss: 0.8268 - val_acc: 0.7420\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7956 - acc: 0.7428 - val_loss: 0.8102 - val_acc: 0.7509\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.7791 - acc: 0.7517 - val_loss: 0.7858 - val_acc: 0.7607\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7638 - acc: 0.7563 - val_loss: 0.7732 - val_acc: 0.7634\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7519 - acc: 0.7578 - val_loss: 0.7688 - val_acc: 0.7616\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.7388 - acc: 0.7611 - val_loss: 0.7544 - val_acc: 0.7598\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7307 - acc: 0.7641 - val_loss: 0.7420 - val_acc: 0.7723\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7239 - acc: 0.7662 - val_loss: 0.7317 - val_acc: 0.7750\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7145 - acc: 0.7694 - val_loss: 0.7227 - val_acc: 0.7777\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.7073 - acc: 0.7744 - val_loss: 0.7227 - val_acc: 0.7705\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.7014 - acc: 0.7750 - val_loss: 0.7123 - val_acc: 0.7812\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6959 - acc: 0.7781 - val_loss: 0.7074 - val_acc: 0.7812\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6912 - acc: 0.7783 - val_loss: 0.7120 - val_acc: 0.7759\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6887 - acc: 0.7795 - val_loss: 0.6982 - val_acc: 0.7821\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6817 - acc: 0.7821 - val_loss: 0.6929 - val_acc: 0.7830\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6790 - acc: 0.7813 - val_loss: 0.6934 - val_acc: 0.7875\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6751 - acc: 0.7848 - val_loss: 0.6961 - val_acc: 0.7866\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6721 - acc: 0.7845 - val_loss: 0.6891 - val_acc: 0.7902\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6690 - acc: 0.7861 - val_loss: 0.6889 - val_acc: 0.7911\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 0.6688 - acc: 0.7862 - val_loss: 0.6816 - val_acc: 0.7866\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6652 - acc: 0.7869 - val_loss: 0.6842 - val_acc: 0.7937\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6612 - acc: 0.7883 - val_loss: 0.6819 - val_acc: 0.7875\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 0.6608 - acc: 0.7875 - val_loss: 0.6768 - val_acc: 0.7875\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6600 - acc: 0.7886 - val_loss: 0.6732 - val_acc: 0.7893\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6573 - acc: 0.7881 - val_loss: 0.6779 - val_acc: 0.7866\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6550 - acc: 0.7901 - val_loss: 0.6699 - val_acc: 0.7937\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6525 - acc: 0.7909 - val_loss: 0.6717 - val_acc: 0.7920\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6533 - acc: 0.7913 - val_loss: 0.6768 - val_acc: 0.7902\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6520 - acc: 0.7907 - val_loss: 0.6747 - val_acc: 0.7902\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6479 - acc: 0.7932 - val_loss: 0.6731 - val_acc: 0.7893\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6509 - acc: 0.7897 - val_loss: 0.6690 - val_acc: 0.7911\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6473 - acc: 0.7937 - val_loss: 0.6684 - val_acc: 0.7884\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6482 - acc: 0.7913 - val_loss: 0.6675 - val_acc: 0.7946\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6455 - acc: 0.7925 - val_loss: 0.6720 - val_acc: 0.7884\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6452 - acc: 0.7928 - val_loss: 0.6662 - val_acc: 0.7920\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6443 - acc: 0.7920 - val_loss: 0.6599 - val_acc: 0.7946\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6453 - acc: 0.7917 - val_loss: 0.6741 - val_acc: 0.7911\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6432 - acc: 0.7932 - val_loss: 0.6605 - val_acc: 0.7955\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6427 - acc: 0.7917 - val_loss: 0.6615 - val_acc: 0.7946\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6410 - acc: 0.7944 - val_loss: 0.6598 - val_acc: 0.8009\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6404 - acc: 0.7950 - val_loss: 0.6628 - val_acc: 0.7955\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6416 - acc: 0.7893 - val_loss: 0.6622 - val_acc: 0.7991\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6397 - acc: 0.7940 - val_loss: 0.6668 - val_acc: 0.8000\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6418 - acc: 0.7943 - val_loss: 0.6646 - val_acc: 0.7964\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6399 - acc: 0.7944 - val_loss: 0.6581 - val_acc: 0.7902\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6390 - acc: 0.7929 - val_loss: 0.6567 - val_acc: 0.7937\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6368 - acc: 0.7944 - val_loss: 0.6612 - val_acc: 0.7830\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6378 - acc: 0.7936 - val_loss: 0.6680 - val_acc: 0.7946\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6369 - acc: 0.7916 - val_loss: 0.6546 - val_acc: 0.7955\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6381 - acc: 0.7921 - val_loss: 0.6651 - val_acc: 0.7884\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6360 - acc: 0.7975 - val_loss: 0.6547 - val_acc: 0.7920\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6355 - acc: 0.7954 - val_loss: 0.6552 - val_acc: 0.7973\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6355 - acc: 0.7952 - val_loss: 0.6593 - val_acc: 0.7884\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6344 - acc: 0.7970 - val_loss: 0.6570 - val_acc: 0.7884\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6339 - acc: 0.7944 - val_loss: 0.6566 - val_acc: 0.7937\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6341 - acc: 0.7978 - val_loss: 0.6605 - val_acc: 0.7902\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 0.6337 - acc: 0.7946 - val_loss: 0.6667 - val_acc: 0.7946\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 1s 52us/sample - loss: 0.6374 - acc: 0.7945 - val_loss: 0.6650 - val_acc: 0.7937\n",
            "Epoch 75/300\n",
            "10080/10080 [==============================] - 1s 87us/sample - loss: 0.6331 - acc: 0.7957 - val_loss: 0.6549 - val_acc: 0.7866\n",
            "Performance on the validation set\n",
            "loss: 0.65\n",
            "acc: 0.78\n",
            "\n",
            "FOLD:  9\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_17 (Model)             (None, 32)                578211    \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 578,574\n",
            "Trainable params: 363\n",
            "Non-trainable params: 578,211\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 140us/sample - loss: 11.0729 - acc: 0.1064 - val_loss: 6.7773 - val_acc: 0.1205\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 5.3217 - acc: 0.1317 - val_loss: 4.4056 - val_acc: 0.1518\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 3.9754 - acc: 0.1792 - val_loss: 3.4370 - val_acc: 0.2223\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 3.1287 - acc: 0.2566 - val_loss: 2.7187 - val_acc: 0.3116\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 2.4878 - acc: 0.3292 - val_loss: 2.1955 - val_acc: 0.3839\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 2.0227 - acc: 0.4038 - val_loss: 1.8187 - val_acc: 0.4554\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 1.6899 - acc: 0.4712 - val_loss: 1.5543 - val_acc: 0.5196\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.4544 - acc: 0.5320 - val_loss: 1.3731 - val_acc: 0.5768\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.2904 - acc: 0.5817 - val_loss: 1.2469 - val_acc: 0.6116\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.1687 - acc: 0.6184 - val_loss: 1.1484 - val_acc: 0.6482\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 1.0784 - acc: 0.6493 - val_loss: 1.0792 - val_acc: 0.6634\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.0115 - acc: 0.6679 - val_loss: 1.0195 - val_acc: 0.6893\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.9576 - acc: 0.6885 - val_loss: 0.9756 - val_acc: 0.7080\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.9135 - acc: 0.7024 - val_loss: 0.9405 - val_acc: 0.7054\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8768 - acc: 0.7168 - val_loss: 0.9089 - val_acc: 0.7286\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8483 - acc: 0.7271 - val_loss: 0.8839 - val_acc: 0.7330\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8236 - acc: 0.7359 - val_loss: 0.8640 - val_acc: 0.7366\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8033 - acc: 0.7418 - val_loss: 0.8438 - val_acc: 0.7464\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7842 - acc: 0.7513 - val_loss: 0.8256 - val_acc: 0.7509\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7683 - acc: 0.7566 - val_loss: 0.8146 - val_acc: 0.7571\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7563 - acc: 0.7584 - val_loss: 0.7972 - val_acc: 0.7625\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.7447 - acc: 0.7623 - val_loss: 0.7874 - val_acc: 0.7643\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7343 - acc: 0.7634 - val_loss: 0.7811 - val_acc: 0.7634\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7242 - acc: 0.7686 - val_loss: 0.7699 - val_acc: 0.7696\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.7174 - acc: 0.7688 - val_loss: 0.7675 - val_acc: 0.7705\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7093 - acc: 0.7709 - val_loss: 0.7576 - val_acc: 0.7670\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7039 - acc: 0.7749 - val_loss: 0.7575 - val_acc: 0.7670\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6968 - acc: 0.7751 - val_loss: 0.7451 - val_acc: 0.7679\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6911 - acc: 0.7776 - val_loss: 0.7407 - val_acc: 0.7696\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6870 - acc: 0.7788 - val_loss: 0.7355 - val_acc: 0.7795\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 0.6841 - acc: 0.7817 - val_loss: 0.7292 - val_acc: 0.7777\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6802 - acc: 0.7791 - val_loss: 0.7273 - val_acc: 0.7777\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6772 - acc: 0.7815 - val_loss: 0.7244 - val_acc: 0.7830\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6727 - acc: 0.7833 - val_loss: 0.7235 - val_acc: 0.7732\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6713 - acc: 0.7839 - val_loss: 0.7194 - val_acc: 0.7750\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6687 - acc: 0.7844 - val_loss: 0.7208 - val_acc: 0.7839\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6663 - acc: 0.7831 - val_loss: 0.7116 - val_acc: 0.7786\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6633 - acc: 0.7860 - val_loss: 0.7128 - val_acc: 0.7786\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6606 - acc: 0.7873 - val_loss: 0.7096 - val_acc: 0.7848\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6598 - acc: 0.7873 - val_loss: 0.7145 - val_acc: 0.7750\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6570 - acc: 0.7891 - val_loss: 0.7045 - val_acc: 0.7848\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6569 - acc: 0.7883 - val_loss: 0.7048 - val_acc: 0.7768\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6542 - acc: 0.7893 - val_loss: 0.6995 - val_acc: 0.7812\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6538 - acc: 0.7886 - val_loss: 0.7026 - val_acc: 0.7786\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6498 - acc: 0.7922 - val_loss: 0.6952 - val_acc: 0.7821\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6488 - acc: 0.7912 - val_loss: 0.7003 - val_acc: 0.7839\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6467 - acc: 0.7930 - val_loss: 0.6927 - val_acc: 0.7821\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6474 - acc: 0.7908 - val_loss: 0.6919 - val_acc: 0.7795\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6470 - acc: 0.7913 - val_loss: 0.6930 - val_acc: 0.7812\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6446 - acc: 0.7915 - val_loss: 0.6839 - val_acc: 0.7839\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6432 - acc: 0.7937 - val_loss: 0.6888 - val_acc: 0.7920\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6456 - acc: 0.7885 - val_loss: 0.6881 - val_acc: 0.7821\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6421 - acc: 0.7927 - val_loss: 0.6838 - val_acc: 0.7857\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6438 - acc: 0.7906 - val_loss: 0.6907 - val_acc: 0.7821\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6424 - acc: 0.7931 - val_loss: 0.7010 - val_acc: 0.7777\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6451 - acc: 0.7923 - val_loss: 0.6819 - val_acc: 0.7848\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6401 - acc: 0.7926 - val_loss: 0.6825 - val_acc: 0.7848\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6386 - acc: 0.7941 - val_loss: 0.6777 - val_acc: 0.7875\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6382 - acc: 0.7940 - val_loss: 0.6801 - val_acc: 0.7920\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6374 - acc: 0.7945 - val_loss: 0.6796 - val_acc: 0.7857\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6390 - acc: 0.7922 - val_loss: 0.6847 - val_acc: 0.7875\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6358 - acc: 0.7938 - val_loss: 0.6763 - val_acc: 0.7902\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6370 - acc: 0.7945 - val_loss: 0.6898 - val_acc: 0.7821\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6360 - acc: 0.7952 - val_loss: 0.6796 - val_acc: 0.7795\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6352 - acc: 0.7941 - val_loss: 0.6785 - val_acc: 0.7830\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6354 - acc: 0.7955 - val_loss: 0.6770 - val_acc: 0.7884\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6360 - acc: 0.7963 - val_loss: 0.6783 - val_acc: 0.7848\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6338 - acc: 0.7931 - val_loss: 0.6845 - val_acc: 0.7893\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6343 - acc: 0.7945 - val_loss: 0.6736 - val_acc: 0.7884\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6328 - acc: 0.7955 - val_loss: 0.6808 - val_acc: 0.7911\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6325 - acc: 0.7963 - val_loss: 0.6788 - val_acc: 0.7946\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6352 - acc: 0.7938 - val_loss: 0.6730 - val_acc: 0.7804\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6328 - acc: 0.7958 - val_loss: 0.6747 - val_acc: 0.7893\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6319 - acc: 0.7954 - val_loss: 0.6768 - val_acc: 0.7884\n",
            "Epoch 75/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6336 - acc: 0.7943 - val_loss: 0.6766 - val_acc: 0.7875\n",
            "Epoch 76/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6317 - acc: 0.7971 - val_loss: 0.6736 - val_acc: 0.7991\n",
            "Epoch 77/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6304 - acc: 0.7959 - val_loss: 0.6727 - val_acc: 0.7920\n",
            "Epoch 78/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6273 - acc: 0.7987 - val_loss: 0.6701 - val_acc: 0.7991\n",
            "Epoch 79/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6315 - acc: 0.7970 - val_loss: 0.6735 - val_acc: 0.7946\n",
            "Epoch 80/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6306 - acc: 0.7966 - val_loss: 0.6790 - val_acc: 0.7911\n",
            "Epoch 81/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6282 - acc: 0.7969 - val_loss: 0.6732 - val_acc: 0.7884\n",
            "Epoch 82/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6307 - acc: 0.7966 - val_loss: 0.6688 - val_acc: 0.7920\n",
            "Epoch 83/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6290 - acc: 0.7979 - val_loss: 0.6755 - val_acc: 0.7955\n",
            "Epoch 84/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6298 - acc: 0.7962 - val_loss: 0.6662 - val_acc: 0.7955\n",
            "Epoch 85/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6289 - acc: 0.7971 - val_loss: 0.6679 - val_acc: 0.7991\n",
            "Epoch 86/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6294 - acc: 0.7947 - val_loss: 0.6718 - val_acc: 0.7920\n",
            "Epoch 87/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6292 - acc: 0.7966 - val_loss: 0.6743 - val_acc: 0.7911\n",
            "Epoch 88/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6297 - acc: 0.7965 - val_loss: 0.6746 - val_acc: 0.7893\n",
            "Epoch 89/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6277 - acc: 0.7966 - val_loss: 0.6677 - val_acc: 0.7875\n",
            "Epoch 90/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6283 - acc: 0.7948 - val_loss: 0.6699 - val_acc: 0.7964\n",
            "Epoch 91/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6283 - acc: 0.7965 - val_loss: 0.6670 - val_acc: 0.8000\n",
            "Epoch 92/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6293 - acc: 0.7957 - val_loss: 0.6660 - val_acc: 0.7982\n",
            "Epoch 93/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6282 - acc: 0.7975 - val_loss: 0.6686 - val_acc: 0.7991\n",
            "Epoch 94/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6278 - acc: 0.7949 - val_loss: 0.6651 - val_acc: 0.7937\n",
            "Epoch 95/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6277 - acc: 0.7975 - val_loss: 0.6681 - val_acc: 0.7929\n",
            "Epoch 96/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6268 - acc: 0.7970 - val_loss: 0.6677 - val_acc: 0.7929\n",
            "Epoch 97/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6246 - acc: 0.7980 - val_loss: 0.6686 - val_acc: 0.7946\n",
            "Epoch 98/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6265 - acc: 0.7970 - val_loss: 0.6758 - val_acc: 0.7839\n",
            "Epoch 99/300\n",
            "10080/10080 [==============================] - 0s 31us/sample - loss: 0.6267 - acc: 0.7979 - val_loss: 0.6713 - val_acc: 0.7839\n",
            "Epoch 100/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6266 - acc: 0.7977 - val_loss: 0.6712 - val_acc: 0.7893\n",
            "Epoch 101/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6269 - acc: 0.7971 - val_loss: 0.6678 - val_acc: 0.7991\n",
            "Epoch 102/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6272 - acc: 0.7965 - val_loss: 0.6678 - val_acc: 0.7955\n",
            "Epoch 103/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6273 - acc: 0.7972 - val_loss: 0.6660 - val_acc: 0.7893\n",
            "Epoch 104/300\n",
            "10080/10080 [==============================] - 1s 89us/sample - loss: 0.6273 - acc: 0.7959 - val_loss: 0.6684 - val_acc: 0.7866\n",
            "Performance on the validation set\n",
            "loss: 0.64\n",
            "acc: 0.78\n",
            "\n",
            "FOLD:  10\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_17 (Model)             (None, 32)                578211    \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 11)                363       \n",
            "=================================================================\n",
            "Total params: 578,574\n",
            "Trainable params: 363\n",
            "Non-trainable params: 578,211\n",
            "_________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 1s 139us/sample - loss: 16.2075 - acc: 0.1610 - val_loss: 8.2363 - val_acc: 0.1732\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 5.6439 - acc: 0.1483 - val_loss: 4.3785 - val_acc: 0.1536\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 3.9545 - acc: 0.1687 - val_loss: 3.5336 - val_acc: 0.2179\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 3.1553 - acc: 0.2434 - val_loss: 2.8548 - val_acc: 0.2759\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 2.5433 - acc: 0.3203 - val_loss: 2.3418 - val_acc: 0.3580\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 2.1011 - acc: 0.3983 - val_loss: 1.9679 - val_acc: 0.4339\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.7847 - acc: 0.4617 - val_loss: 1.7049 - val_acc: 0.4884\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 1.5566 - acc: 0.5232 - val_loss: 1.5125 - val_acc: 0.5312\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.3907 - acc: 0.5656 - val_loss: 1.3726 - val_acc: 0.5759\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 1.2688 - acc: 0.6071 - val_loss: 1.2624 - val_acc: 0.6071\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 1.1708 - acc: 0.6350 - val_loss: 1.1675 - val_acc: 0.6330\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 1.0949 - acc: 0.6592 - val_loss: 1.1049 - val_acc: 0.6482\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 1.0341 - acc: 0.6761 - val_loss: 1.0543 - val_acc: 0.6643\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.9816 - acc: 0.6893 - val_loss: 0.9952 - val_acc: 0.6812\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.9411 - acc: 0.7045 - val_loss: 0.9553 - val_acc: 0.6929\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.9043 - acc: 0.7174 - val_loss: 0.9229 - val_acc: 0.7009\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.8759 - acc: 0.7238 - val_loss: 0.8970 - val_acc: 0.7089\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.8502 - acc: 0.7315 - val_loss: 0.8684 - val_acc: 0.7205\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8256 - acc: 0.7391 - val_loss: 0.8460 - val_acc: 0.7330\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.8076 - acc: 0.7432 - val_loss: 0.8237 - val_acc: 0.7384\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7902 - acc: 0.7490 - val_loss: 0.8190 - val_acc: 0.7402\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7756 - acc: 0.7533 - val_loss: 0.7979 - val_acc: 0.7411\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7631 - acc: 0.7569 - val_loss: 0.7833 - val_acc: 0.7500\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.7507 - acc: 0.7606 - val_loss: 0.7785 - val_acc: 0.7491\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7408 - acc: 0.7642 - val_loss: 0.7570 - val_acc: 0.7598\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7318 - acc: 0.7671 - val_loss: 0.7596 - val_acc: 0.7473\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7230 - acc: 0.7687 - val_loss: 0.7414 - val_acc: 0.7589\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7161 - acc: 0.7726 - val_loss: 0.7423 - val_acc: 0.7571\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.7108 - acc: 0.7730 - val_loss: 0.7389 - val_acc: 0.7518\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.7025 - acc: 0.7782 - val_loss: 0.7224 - val_acc: 0.7616\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6986 - acc: 0.7812 - val_loss: 0.7265 - val_acc: 0.7571\n",
            "Epoch 32/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6934 - acc: 0.7803 - val_loss: 0.7229 - val_acc: 0.7679\n",
            "Epoch 33/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6893 - acc: 0.7812 - val_loss: 0.7081 - val_acc: 0.7643\n",
            "Epoch 34/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6853 - acc: 0.7812 - val_loss: 0.7038 - val_acc: 0.7679\n",
            "Epoch 35/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6813 - acc: 0.7850 - val_loss: 0.7023 - val_acc: 0.7679\n",
            "Epoch 36/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6783 - acc: 0.7838 - val_loss: 0.7002 - val_acc: 0.7741\n",
            "Epoch 37/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6745 - acc: 0.7861 - val_loss: 0.6960 - val_acc: 0.7625\n",
            "Epoch 38/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6712 - acc: 0.7884 - val_loss: 0.6952 - val_acc: 0.7723\n",
            "Epoch 39/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6699 - acc: 0.7865 - val_loss: 0.6886 - val_acc: 0.7732\n",
            "Epoch 40/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 0.6671 - acc: 0.7881 - val_loss: 0.6928 - val_acc: 0.7652\n",
            "Epoch 41/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6687 - acc: 0.7859 - val_loss: 0.6829 - val_acc: 0.7768\n",
            "Epoch 42/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6630 - acc: 0.7878 - val_loss: 0.6843 - val_acc: 0.7688\n",
            "Epoch 43/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6608 - acc: 0.7897 - val_loss: 0.6731 - val_acc: 0.7795\n",
            "Epoch 44/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6596 - acc: 0.7888 - val_loss: 0.6755 - val_acc: 0.7750\n",
            "Epoch 45/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6587 - acc: 0.7865 - val_loss: 0.6720 - val_acc: 0.7786\n",
            "Epoch 46/300\n",
            "10080/10080 [==============================] - 0s 37us/sample - loss: 0.6560 - acc: 0.7876 - val_loss: 0.6736 - val_acc: 0.7786\n",
            "Epoch 47/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6563 - acc: 0.7908 - val_loss: 0.6721 - val_acc: 0.7741\n",
            "Epoch 48/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6548 - acc: 0.7909 - val_loss: 0.6791 - val_acc: 0.7759\n",
            "Epoch 49/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6534 - acc: 0.7899 - val_loss: 0.6640 - val_acc: 0.7768\n",
            "Epoch 50/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6518 - acc: 0.7914 - val_loss: 0.6686 - val_acc: 0.7821\n",
            "Epoch 51/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6525 - acc: 0.7941 - val_loss: 0.6845 - val_acc: 0.7688\n",
            "Epoch 52/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6513 - acc: 0.7908 - val_loss: 0.6758 - val_acc: 0.7741\n",
            "Epoch 53/300\n",
            "10080/10080 [==============================] - 0s 32us/sample - loss: 0.6491 - acc: 0.7917 - val_loss: 0.6717 - val_acc: 0.7750\n",
            "Epoch 54/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6489 - acc: 0.7937 - val_loss: 0.6680 - val_acc: 0.7812\n",
            "Epoch 55/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6495 - acc: 0.7912 - val_loss: 0.6600 - val_acc: 0.7812\n",
            "Epoch 56/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6472 - acc: 0.7937 - val_loss: 0.6598 - val_acc: 0.7786\n",
            "Epoch 57/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6469 - acc: 0.7925 - val_loss: 0.6801 - val_acc: 0.7634\n",
            "Epoch 58/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6461 - acc: 0.7939 - val_loss: 0.6614 - val_acc: 0.7830\n",
            "Epoch 59/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6446 - acc: 0.7957 - val_loss: 0.6672 - val_acc: 0.7759\n",
            "Epoch 60/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6442 - acc: 0.7932 - val_loss: 0.6586 - val_acc: 0.7786\n",
            "Epoch 61/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6461 - acc: 0.7923 - val_loss: 0.6644 - val_acc: 0.7804\n",
            "Epoch 62/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6437 - acc: 0.7934 - val_loss: 0.6603 - val_acc: 0.7777\n",
            "Epoch 63/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6446 - acc: 0.7923 - val_loss: 0.6512 - val_acc: 0.7821\n",
            "Epoch 64/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6443 - acc: 0.7924 - val_loss: 0.6583 - val_acc: 0.7786\n",
            "Epoch 65/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6413 - acc: 0.7946 - val_loss: 0.6544 - val_acc: 0.7804\n",
            "Epoch 66/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6409 - acc: 0.7938 - val_loss: 0.6734 - val_acc: 0.7786\n",
            "Epoch 67/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6411 - acc: 0.7940 - val_loss: 0.6715 - val_acc: 0.7679\n",
            "Epoch 68/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6411 - acc: 0.7932 - val_loss: 0.6556 - val_acc: 0.7875\n",
            "Epoch 69/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6394 - acc: 0.7959 - val_loss: 0.6552 - val_acc: 0.7786\n",
            "Epoch 70/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6396 - acc: 0.7942 - val_loss: 0.6472 - val_acc: 0.7821\n",
            "Epoch 71/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6396 - acc: 0.7950 - val_loss: 0.6515 - val_acc: 0.7750\n",
            "Epoch 72/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6402 - acc: 0.7926 - val_loss: 0.6533 - val_acc: 0.7804\n",
            "Epoch 73/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6383 - acc: 0.7937 - val_loss: 0.6528 - val_acc: 0.7812\n",
            "Epoch 74/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6409 - acc: 0.7935 - val_loss: 0.6614 - val_acc: 0.7786\n",
            "Epoch 75/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6396 - acc: 0.7948 - val_loss: 0.6551 - val_acc: 0.7830\n",
            "Epoch 76/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6369 - acc: 0.7956 - val_loss: 0.6484 - val_acc: 0.7839\n",
            "Epoch 77/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6373 - acc: 0.7926 - val_loss: 0.6565 - val_acc: 0.7804\n",
            "Epoch 78/300\n",
            "10080/10080 [==============================] - 0s 35us/sample - loss: 0.6382 - acc: 0.7942 - val_loss: 0.6456 - val_acc: 0.7804\n",
            "Epoch 79/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6383 - acc: 0.7935 - val_loss: 0.6502 - val_acc: 0.7812\n",
            "Epoch 80/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6360 - acc: 0.7927 - val_loss: 0.6527 - val_acc: 0.7857\n",
            "Epoch 81/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6366 - acc: 0.7950 - val_loss: 0.6471 - val_acc: 0.7866\n",
            "Epoch 82/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6363 - acc: 0.7934 - val_loss: 0.6545 - val_acc: 0.7812\n",
            "Epoch 83/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6343 - acc: 0.7951 - val_loss: 0.6460 - val_acc: 0.7821\n",
            "Epoch 84/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6342 - acc: 0.7947 - val_loss: 0.6493 - val_acc: 0.7804\n",
            "Epoch 85/300\n",
            "10080/10080 [==============================] - 0s 34us/sample - loss: 0.6365 - acc: 0.7941 - val_loss: 0.6489 - val_acc: 0.7821\n",
            "Epoch 86/300\n",
            "10080/10080 [==============================] - 0s 33us/sample - loss: 0.6348 - acc: 0.7995 - val_loss: 0.6615 - val_acc: 0.7759\n",
            "Epoch 87/300\n",
            "10080/10080 [==============================] - 0s 36us/sample - loss: 0.6353 - acc: 0.7955 - val_loss: 0.6523 - val_acc: 0.7795\n",
            "Epoch 88/300\n",
            "10080/10080 [==============================] - 1s 89us/sample - loss: 0.6346 - acc: 0.7948 - val_loss: 0.6570 - val_acc: 0.7777\n",
            "Performance on the validation set\n",
            "loss: 0.65\n",
            "acc: 0.79\n",
            "\n",
            "Overall  loss :  0.65 +- 0.01\n",
            "Overall  acc :  0.78 +- 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLM2b-ozNCki",
        "colab_type": "code",
        "outputId": "f8abf172-8998-47bc-bf2b-c1c883410ab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "x_plot = list(range(1, len(best_history.history['val_accuracy']) + 1))\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(x_plot, history.history['loss'])\n",
        "    plt.plot(x_plot, history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.plot(x_plot, history.history['accuracy'])\n",
        "    plt.plot(x_plot, history.history['val_accuracy'])\n",
        "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_history(best_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIhGnRCAiEnFKBCIiEff/A7h3bOUxi4g8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7fd-_loairL",
        "colab_type": "code",
        "outputId": "f829419d-0bf2-4240-dedf-8fcd0e82c6ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(labels_val, best_model.predict_classes(x_val)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.94       271\n",
            "           1       0.90      0.90      0.90       262\n",
            "           2       0.92      0.94      0.93       244\n",
            "           3       0.99      0.98      0.99       260\n",
            "           4       0.95      0.92      0.94       273\n",
            "           5       0.91      0.91      0.91       251\n",
            "           6       0.88      0.91      0.89       235\n",
            "           7       0.96      0.96      0.96       273\n",
            "           8       0.96      0.95      0.96       254\n",
            "           9       0.92      0.89      0.90       289\n",
            "          10       0.94      0.95      0.95       188\n",
            "\n",
            "    accuracy                           0.93      2800\n",
            "   macro avg       0.93      0.93      0.93      2800\n",
            "weighted avg       0.93      0.93      0.93      2800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxbMs2I6dcr0",
        "colab_type": "text"
      },
      "source": [
        "# Autoencoder tied with the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWd6qMK5dlEY",
        "colab_type": "code",
        "outputId": "b4489617-4237-425e-f581-f065d4adc571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "monitor = 'val_loss'\n",
        "early_stopping = EarlyStopping(monitor=monitor, patience=10, min_delta=0.001, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=10, verbose=1)\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "model_type = 'autoencoder_nn'\n",
        "if model_type == 'autoencoder_nn':\n",
        "  labels_train = [y_train, x_train]\n",
        "  labels_val = [y_val, x_val]\n",
        "elif model_type == 'nn':\n",
        "  labels_train = y_train\n",
        "  labels_val = y_val\n",
        "elif model_type == 'autoencoder':\n",
        "  labels_train = x_train\n",
        "  labels_val = x_val\n",
        "\n",
        "# define 10-fold cross validation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "scores_val = []\n",
        "best_score = 0  # based on accuracy\n",
        "best_model = None\n",
        "best_history = None\n",
        "\n",
        "for idx, (train, val) in enumerate(kfold.split(x_train, labels_train[0])):\n",
        "  print('FOLD: ', idx + 1)\n",
        "  # create model\n",
        "  autoencoder_nn, _ = get_model(\n",
        "      model_type=model_type, \n",
        "      divide_by=2, \n",
        "      hidden_first=512, \n",
        "      hidden_last=32, \n",
        "      dropout=True, \n",
        "      batch_norm=False, \n",
        "      standard=False, \n",
        "      verbose=True\n",
        "  )\n",
        "  # Fit the model\n",
        "  history = autoencoder_nn.fit(\n",
        "      x_train[train], \n",
        "      [labels_train[0][train], labels_train[1][train]], \n",
        "      validation_data=(x_train[val], [labels_train[0][val], labels_train[1][val]]), \n",
        "      epochs=300, \n",
        "      batch_size=128, \n",
        "      callbacks=callbacks\n",
        "  )\n",
        "  # evaluate the model\n",
        "  score = autoencoder_nn.evaluate(x_val, labels_val, verbose=0)\n",
        "  # save best_model\n",
        "  if score[3] > best_score:  # classifier accuracy\n",
        "    best_score = score[3]\n",
        "    best_model = autoencoder_nn\n",
        "    best_history = history\n",
        "  print('Performance on the validation set')\n",
        "  for idx in range(len(autoencoder_nn.metrics_names)):\n",
        "    print(\"%s: %.2f\" % (autoencoder_nn.metrics_names[idx], score[idx]))\n",
        "  print()\n",
        "  scores_val.append(score)\n",
        "  \n",
        "scores_val = np.array(scores_val)\n",
        "overall_means = np.mean(scores_val, axis=0)\n",
        "overall_stds = np.std(scores_val, axis=0)\n",
        "for idx in range(len(overall_means)):\n",
        "  print('Overall ', autoencoder_nn.metrics_names[idx], ': ', '{:.2f}'.format(overall_means[idx]), '+-', '{:.2f}'.format(overall_stds[idx]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD:  1\n",
            "Model: \"model_38\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_19 (InputLayer)           (None, 784)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_177 (Dense)               (None, 512)          401920      input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 512)          0           dense_177[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_178 (Dense)               (None, 256)          131328      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 256)          0           dense_178[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_179 (Dense)               (None, 128)          32896       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 128)          0           dense_179[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_180 (Dense)               (None, 64)           8256        dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Dense)                 (None, 32)           2080        dense_180[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_181 (Dense)               (None, 32)           1056        encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_182 (Dense)               (None, 64)           2112        dense_181[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_183 (Dense)               (None, 128)          8320        dense_182[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_184 (Dense)               (None, 256)          33024       dense_183[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_185 (Dense)               (None, 512)          131584      dense_184[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           363         encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          402192      dense_185[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,131\n",
            "Trainable params: 1,155,131\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 284us/step - loss: 1.6873 - classifier_loss: 1.5914 - decoder_loss: 0.0938 - classifier_accuracy: 0.4570 - decoder_mse: 0.0939 - val_loss: 0.8216 - val_classifier_loss: 0.7524 - val_decoder_loss: 0.0692 - val_classifier_accuracy: 0.7696 - val_decoder_mse: 0.0693\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 115us/step - loss: 0.8327 - classifier_loss: 0.7625 - decoder_loss: 0.0693 - classifier_accuracy: 0.7583 - decoder_mse: 0.0693 - val_loss: 0.5265 - val_classifier_loss: 0.4654 - val_decoder_loss: 0.0626 - val_classifier_accuracy: 0.8554 - val_decoder_mse: 0.0627\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 116us/step - loss: 0.6083 - classifier_loss: 0.5421 - decoder_loss: 0.0655 - classifier_accuracy: 0.8353 - decoder_mse: 0.0655 - val_loss: 0.4293 - val_classifier_loss: 0.3685 - val_decoder_loss: 0.0611 - val_classifier_accuracy: 0.8973 - val_decoder_mse: 0.0612\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.4855 - classifier_loss: 0.4217 - decoder_loss: 0.0638 - classifier_accuracy: 0.8729 - decoder_mse: 0.0638 - val_loss: 0.3670 - val_classifier_loss: 0.3086 - val_decoder_loss: 0.0596 - val_classifier_accuracy: 0.9125 - val_decoder_mse: 0.0597\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 115us/step - loss: 0.4114 - classifier_loss: 0.3487 - decoder_loss: 0.0632 - classifier_accuracy: 0.8900 - decoder_mse: 0.0632 - val_loss: 0.3517 - val_classifier_loss: 0.2926 - val_decoder_loss: 0.0586 - val_classifier_accuracy: 0.9232 - val_decoder_mse: 0.0587\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 115us/step - loss: 0.3497 - classifier_loss: 0.2877 - decoder_loss: 0.0621 - classifier_accuracy: 0.9130 - decoder_mse: 0.0621 - val_loss: 0.3429 - val_classifier_loss: 0.2870 - val_decoder_loss: 0.0580 - val_classifier_accuracy: 0.9161 - val_decoder_mse: 0.0580\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 115us/step - loss: 0.3191 - classifier_loss: 0.2574 - decoder_loss: 0.0617 - classifier_accuracy: 0.9202 - decoder_mse: 0.0617 - val_loss: 0.3211 - val_classifier_loss: 0.2646 - val_decoder_loss: 0.0583 - val_classifier_accuracy: 0.9250 - val_decoder_mse: 0.0584\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 116us/step - loss: 0.2837 - classifier_loss: 0.2221 - decoder_loss: 0.0613 - classifier_accuracy: 0.9295 - decoder_mse: 0.0613 - val_loss: 0.3126 - val_classifier_loss: 0.2597 - val_decoder_loss: 0.0569 - val_classifier_accuracy: 0.9286 - val_decoder_mse: 0.0570\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.2627 - classifier_loss: 0.2017 - decoder_loss: 0.0610 - classifier_accuracy: 0.9358 - decoder_mse: 0.0610 - val_loss: 0.3021 - val_classifier_loss: 0.2472 - val_decoder_loss: 0.0571 - val_classifier_accuracy: 0.9196 - val_decoder_mse: 0.0572\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.2456 - classifier_loss: 0.1847 - decoder_loss: 0.0609 - classifier_accuracy: 0.9438 - decoder_mse: 0.0609 - val_loss: 0.3074 - val_classifier_loss: 0.2520 - val_decoder_loss: 0.0564 - val_classifier_accuracy: 0.9268 - val_decoder_mse: 0.0564\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.2403 - classifier_loss: 0.1796 - decoder_loss: 0.0605 - classifier_accuracy: 0.9433 - decoder_mse: 0.0605 - val_loss: 0.3201 - val_classifier_loss: 0.2638 - val_decoder_loss: 0.0564 - val_classifier_accuracy: 0.9277 - val_decoder_mse: 0.0565\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 116us/step - loss: 0.2117 - classifier_loss: 0.1514 - decoder_loss: 0.0602 - classifier_accuracy: 0.9526 - decoder_mse: 0.0602 - val_loss: 0.3021 - val_classifier_loss: 0.2489 - val_decoder_loss: 0.0556 - val_classifier_accuracy: 0.9321 - val_decoder_mse: 0.0557\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 116us/step - loss: 0.2182 - classifier_loss: 0.1584 - decoder_loss: 0.0601 - classifier_accuracy: 0.9512 - decoder_mse: 0.0601 - val_loss: 0.3145 - val_classifier_loss: 0.2600 - val_decoder_loss: 0.0558 - val_classifier_accuracy: 0.9330 - val_decoder_mse: 0.0559\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.1828 - classifier_loss: 0.1236 - decoder_loss: 0.0595 - classifier_accuracy: 0.9600 - decoder_mse: 0.0595 - val_loss: 0.3468 - val_classifier_loss: 0.2954 - val_decoder_loss: 0.0545 - val_classifier_accuracy: 0.9286 - val_decoder_mse: 0.0546\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.1912 - classifier_loss: 0.1319 - decoder_loss: 0.0592 - classifier_accuracy: 0.9582 - decoder_mse: 0.0592 - val_loss: 0.3259 - val_classifier_loss: 0.2741 - val_decoder_loss: 0.0542 - val_classifier_accuracy: 0.9295 - val_decoder_mse: 0.0543\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 116us/step - loss: 0.1748 - classifier_loss: 0.1159 - decoder_loss: 0.0590 - classifier_accuracy: 0.9641 - decoder_mse: 0.0590 - val_loss: 0.3283 - val_classifier_loss: 0.2769 - val_decoder_loss: 0.0540 - val_classifier_accuracy: 0.9286 - val_decoder_mse: 0.0541\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 115us/step - loss: 0.1795 - classifier_loss: 0.1207 - decoder_loss: 0.0587 - classifier_accuracy: 0.9610 - decoder_mse: 0.0587 - val_loss: 0.3133 - val_classifier_loss: 0.2609 - val_decoder_loss: 0.0546 - val_classifier_accuracy: 0.9286 - val_decoder_mse: 0.0546\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.1759 - classifier_loss: 0.1168 - decoder_loss: 0.0589 - classifier_accuracy: 0.9607 - decoder_mse: 0.0589 - val_loss: 0.3350 - val_classifier_loss: 0.2805 - val_decoder_loss: 0.0538 - val_classifier_accuracy: 0.9268 - val_decoder_mse: 0.0539\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 115us/step - loss: 0.1727 - classifier_loss: 0.1142 - decoder_loss: 0.0584 - classifier_accuracy: 0.9647 - decoder_mse: 0.0584 - val_loss: 0.3117 - val_classifier_loss: 0.2591 - val_decoder_loss: 0.0534 - val_classifier_accuracy: 0.9321 - val_decoder_mse: 0.0535\n",
            "Performance on the validation set\n",
            "loss: 0.32\n",
            "classifier_loss: 0.26\n",
            "decoder_loss: 0.06\n",
            "classifier_accuracy: 0.93\n",
            "decoder_mse: 0.06\n",
            "\n",
            "FOLD:  2\n",
            "Model: \"model_40\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_20 (InputLayer)           (None, 784)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_186 (Dense)               (None, 512)          401920      input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 512)          0           dense_186[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_187 (Dense)               (None, 256)          131328      dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 256)          0           dense_187[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_188 (Dense)               (None, 128)          32896       dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 128)          0           dense_188[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_189 (Dense)               (None, 64)           8256        dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Dense)                 (None, 32)           2080        dense_189[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_190 (Dense)               (None, 32)           1056        encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_191 (Dense)               (None, 64)           2112        dense_190[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_192 (Dense)               (None, 128)          8320        dense_191[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_193 (Dense)               (None, 256)          33024       dense_192[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_194 (Dense)               (None, 512)          131584      dense_193[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           363         encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          402192      dense_194[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,131\n",
            "Trainable params: 1,155,131\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 301us/step - loss: 1.6269 - classifier_loss: 1.5299 - decoder_loss: 0.0951 - classifier_accuracy: 0.4802 - decoder_mse: 0.0952 - val_loss: 0.8872 - val_classifier_loss: 0.8124 - val_decoder_loss: 0.0727 - val_classifier_accuracy: 0.7429 - val_decoder_mse: 0.0726\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.8192 - classifier_loss: 0.7499 - decoder_loss: 0.0700 - classifier_accuracy: 0.7634 - decoder_mse: 0.0700 - val_loss: 0.6168 - val_classifier_loss: 0.5493 - val_decoder_loss: 0.0646 - val_classifier_accuracy: 0.8330 - val_decoder_mse: 0.0646\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.5866 - classifier_loss: 0.5218 - decoder_loss: 0.0651 - classifier_accuracy: 0.8438 - decoder_mse: 0.0651 - val_loss: 0.4918 - val_classifier_loss: 0.4284 - val_decoder_loss: 0.0625 - val_classifier_accuracy: 0.8670 - val_decoder_mse: 0.0625\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.4756 - classifier_loss: 0.4118 - decoder_loss: 0.0638 - classifier_accuracy: 0.8768 - decoder_mse: 0.0638 - val_loss: 0.4205 - val_classifier_loss: 0.3586 - val_decoder_loss: 0.0619 - val_classifier_accuracy: 0.8857 - val_decoder_mse: 0.0619\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.3941 - classifier_loss: 0.3315 - decoder_loss: 0.0632 - classifier_accuracy: 0.9008 - decoder_mse: 0.0632 - val_loss: 0.3937 - val_classifier_loss: 0.3334 - val_decoder_loss: 0.0603 - val_classifier_accuracy: 0.9080 - val_decoder_mse: 0.0603\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.3375 - classifier_loss: 0.2751 - decoder_loss: 0.0623 - classifier_accuracy: 0.9166 - decoder_mse: 0.0623 - val_loss: 0.3662 - val_classifier_loss: 0.3047 - val_decoder_loss: 0.0606 - val_classifier_accuracy: 0.8982 - val_decoder_mse: 0.0606\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.3165 - classifier_loss: 0.2541 - decoder_loss: 0.0624 - classifier_accuracy: 0.9225 - decoder_mse: 0.0624 - val_loss: 0.3552 - val_classifier_loss: 0.2933 - val_decoder_loss: 0.0601 - val_classifier_accuracy: 0.9062 - val_decoder_mse: 0.0601\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.2910 - classifier_loss: 0.2299 - decoder_loss: 0.0616 - classifier_accuracy: 0.9308 - decoder_mse: 0.0616 - val_loss: 0.3616 - val_classifier_loss: 0.3029 - val_decoder_loss: 0.0587 - val_classifier_accuracy: 0.9125 - val_decoder_mse: 0.0587\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.2621 - classifier_loss: 0.2006 - decoder_loss: 0.0615 - classifier_accuracy: 0.9359 - decoder_mse: 0.0615 - val_loss: 0.3707 - val_classifier_loss: 0.3103 - val_decoder_loss: 0.0581 - val_classifier_accuracy: 0.9152 - val_decoder_mse: 0.0581\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 124us/step - loss: 0.2460 - classifier_loss: 0.1849 - decoder_loss: 0.0609 - classifier_accuracy: 0.9430 - decoder_mse: 0.0609 - val_loss: 0.3351 - val_classifier_loss: 0.2773 - val_decoder_loss: 0.0575 - val_classifier_accuracy: 0.9187 - val_decoder_mse: 0.0575\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.2229 - classifier_loss: 0.1623 - decoder_loss: 0.0606 - classifier_accuracy: 0.9491 - decoder_mse: 0.0606 - val_loss: 0.3475 - val_classifier_loss: 0.2881 - val_decoder_loss: 0.0576 - val_classifier_accuracy: 0.9187 - val_decoder_mse: 0.0576\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.2153 - classifier_loss: 0.1552 - decoder_loss: 0.0603 - classifier_accuracy: 0.9525 - decoder_mse: 0.0603 - val_loss: 0.3299 - val_classifier_loss: 0.2714 - val_decoder_loss: 0.0568 - val_classifier_accuracy: 0.9223 - val_decoder_mse: 0.0568\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.2119 - classifier_loss: 0.1515 - decoder_loss: 0.0600 - classifier_accuracy: 0.9518 - decoder_mse: 0.0600 - val_loss: 0.3270 - val_classifier_loss: 0.2691 - val_decoder_loss: 0.0564 - val_classifier_accuracy: 0.9214 - val_decoder_mse: 0.0564\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1906 - classifier_loss: 0.1309 - decoder_loss: 0.0599 - classifier_accuracy: 0.9604 - decoder_mse: 0.0599 - val_loss: 0.3492 - val_classifier_loss: 0.2900 - val_decoder_loss: 0.0570 - val_classifier_accuracy: 0.9187 - val_decoder_mse: 0.0570\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.1879 - classifier_loss: 0.1281 - decoder_loss: 0.0598 - classifier_accuracy: 0.9599 - decoder_mse: 0.0598 - val_loss: 0.3242 - val_classifier_loss: 0.2657 - val_decoder_loss: 0.0565 - val_classifier_accuracy: 0.9250 - val_decoder_mse: 0.0564\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.1806 - classifier_loss: 0.1211 - decoder_loss: 0.0597 - classifier_accuracy: 0.9620 - decoder_mse: 0.0597 - val_loss: 0.3932 - val_classifier_loss: 0.3346 - val_decoder_loss: 0.0562 - val_classifier_accuracy: 0.9152 - val_decoder_mse: 0.0562\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1634 - classifier_loss: 0.1041 - decoder_loss: 0.0592 - classifier_accuracy: 0.9676 - decoder_mse: 0.0592 - val_loss: 0.3328 - val_classifier_loss: 0.2760 - val_decoder_loss: 0.0555 - val_classifier_accuracy: 0.9259 - val_decoder_mse: 0.0555\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.1691 - classifier_loss: 0.1099 - decoder_loss: 0.0591 - classifier_accuracy: 0.9658 - decoder_mse: 0.0591 - val_loss: 0.3498 - val_classifier_loss: 0.2917 - val_decoder_loss: 0.0553 - val_classifier_accuracy: 0.9232 - val_decoder_mse: 0.0553\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1627 - classifier_loss: 0.1039 - decoder_loss: 0.0588 - classifier_accuracy: 0.9681 - decoder_mse: 0.0588 - val_loss: 0.3401 - val_classifier_loss: 0.2807 - val_decoder_loss: 0.0554 - val_classifier_accuracy: 0.9295 - val_decoder_mse: 0.0554\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 116us/step - loss: 0.1609 - classifier_loss: 0.1021 - decoder_loss: 0.0588 - classifier_accuracy: 0.9667 - decoder_mse: 0.0588 - val_loss: 0.3493 - val_classifier_loss: 0.2917 - val_decoder_loss: 0.0553 - val_classifier_accuracy: 0.9304 - val_decoder_mse: 0.0552\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1564 - classifier_loss: 0.0980 - decoder_loss: 0.0585 - classifier_accuracy: 0.9677 - decoder_mse: 0.0585 - val_loss: 0.3223 - val_classifier_loss: 0.2648 - val_decoder_loss: 0.0545 - val_classifier_accuracy: 0.9321 - val_decoder_mse: 0.0544\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1435 - classifier_loss: 0.0856 - decoder_loss: 0.0581 - classifier_accuracy: 0.9735 - decoder_mse: 0.0581 - val_loss: 0.3417 - val_classifier_loss: 0.2863 - val_decoder_loss: 0.0540 - val_classifier_accuracy: 0.9241 - val_decoder_mse: 0.0540\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.1410 - classifier_loss: 0.0828 - decoder_loss: 0.0580 - classifier_accuracy: 0.9721 - decoder_mse: 0.0580 - val_loss: 0.3766 - val_classifier_loss: 0.3196 - val_decoder_loss: 0.0537 - val_classifier_accuracy: 0.9214 - val_decoder_mse: 0.0537\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1410 - classifier_loss: 0.0830 - decoder_loss: 0.0579 - classifier_accuracy: 0.9732 - decoder_mse: 0.0579 - val_loss: 0.3836 - val_classifier_loss: 0.3264 - val_decoder_loss: 0.0539 - val_classifier_accuracy: 0.9312 - val_decoder_mse: 0.0538\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1427 - classifier_loss: 0.0847 - decoder_loss: 0.0581 - classifier_accuracy: 0.9721 - decoder_mse: 0.0581 - val_loss: 0.3686 - val_classifier_loss: 0.3094 - val_decoder_loss: 0.0543 - val_classifier_accuracy: 0.9304 - val_decoder_mse: 0.0542\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1356 - classifier_loss: 0.0779 - decoder_loss: 0.0577 - classifier_accuracy: 0.9750 - decoder_mse: 0.0577 - val_loss: 0.3495 - val_classifier_loss: 0.2931 - val_decoder_loss: 0.0533 - val_classifier_accuracy: 0.9259 - val_decoder_mse: 0.0533\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.1367 - classifier_loss: 0.0791 - decoder_loss: 0.0574 - classifier_accuracy: 0.9746 - decoder_mse: 0.0574 - val_loss: 0.3451 - val_classifier_loss: 0.2883 - val_decoder_loss: 0.0538 - val_classifier_accuracy: 0.9339 - val_decoder_mse: 0.0538\n",
            "Epoch 28/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1263 - classifier_loss: 0.0691 - decoder_loss: 0.0570 - classifier_accuracy: 0.9769 - decoder_mse: 0.0570 - val_loss: 0.3975 - val_classifier_loss: 0.3428 - val_decoder_loss: 0.0532 - val_classifier_accuracy: 0.9304 - val_decoder_mse: 0.0532\n",
            "Epoch 29/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1283 - classifier_loss: 0.0714 - decoder_loss: 0.0569 - classifier_accuracy: 0.9770 - decoder_mse: 0.0569 - val_loss: 0.3840 - val_classifier_loss: 0.3282 - val_decoder_loss: 0.0528 - val_classifier_accuracy: 0.9214 - val_decoder_mse: 0.0527\n",
            "Epoch 30/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1220 - classifier_loss: 0.0652 - decoder_loss: 0.0568 - classifier_accuracy: 0.9810 - decoder_mse: 0.0568 - val_loss: 0.3498 - val_classifier_loss: 0.2938 - val_decoder_loss: 0.0522 - val_classifier_accuracy: 0.9348 - val_decoder_mse: 0.0522\n",
            "Epoch 31/300\n",
            "10080/10080 [==============================] - 1s 116us/step - loss: 0.1222 - classifier_loss: 0.0650 - decoder_loss: 0.0571 - classifier_accuracy: 0.9805 - decoder_mse: 0.0571 - val_loss: 0.3595 - val_classifier_loss: 0.3029 - val_decoder_loss: 0.0532 - val_classifier_accuracy: 0.9259 - val_decoder_mse: 0.0532\n",
            "Performance on the validation set\n",
            "loss: 0.31\n",
            "classifier_loss: 0.26\n",
            "decoder_loss: 0.05\n",
            "classifier_accuracy: 0.94\n",
            "decoder_mse: 0.05\n",
            "\n",
            "FOLD:  3\n",
            "Model: \"model_42\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_21 (InputLayer)           (None, 784)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_195 (Dense)               (None, 512)          401920      input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 512)          0           dense_195[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_196 (Dense)               (None, 256)          131328      dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 256)          0           dense_196[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_197 (Dense)               (None, 128)          32896       dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 128)          0           dense_197[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_198 (Dense)               (None, 64)           8256        dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Dense)                 (None, 32)           2080        dense_198[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_199 (Dense)               (None, 32)           1056        encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_200 (Dense)               (None, 64)           2112        dense_199[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_201 (Dense)               (None, 128)          8320        dense_200[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_202 (Dense)               (None, 256)          33024       dense_201[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_203 (Dense)               (None, 512)          131584      dense_202[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           363         encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          402192      dense_203[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,131\n",
            "Trainable params: 1,155,131\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 307us/step - loss: 1.6246 - classifier_loss: 1.5273 - decoder_loss: 0.0952 - classifier_accuracy: 0.4853 - decoder_mse: 0.0953 - val_loss: 0.7614 - val_classifier_loss: 0.6928 - val_decoder_loss: 0.0713 - val_classifier_accuracy: 0.7821 - val_decoder_mse: 0.0714\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 115us/step - loss: 0.7929 - classifier_loss: 0.7230 - decoder_loss: 0.0692 - classifier_accuracy: 0.7745 - decoder_mse: 0.0692 - val_loss: 0.4833 - val_classifier_loss: 0.4194 - val_decoder_loss: 0.0643 - val_classifier_accuracy: 0.8732 - val_decoder_mse: 0.0644\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.5876 - classifier_loss: 0.5227 - decoder_loss: 0.0657 - classifier_accuracy: 0.8397 - decoder_mse: 0.0657 - val_loss: 0.4192 - val_classifier_loss: 0.3564 - val_decoder_loss: 0.0626 - val_classifier_accuracy: 0.8938 - val_decoder_mse: 0.0627\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.4609 - classifier_loss: 0.3962 - decoder_loss: 0.0643 - classifier_accuracy: 0.8793 - decoder_mse: 0.0643 - val_loss: 0.3646 - val_classifier_loss: 0.3002 - val_decoder_loss: 0.0619 - val_classifier_accuracy: 0.9071 - val_decoder_mse: 0.0619\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 116us/step - loss: 0.3980 - classifier_loss: 0.3342 - decoder_loss: 0.0637 - classifier_accuracy: 0.8963 - decoder_mse: 0.0637 - val_loss: 0.3324 - val_classifier_loss: 0.2707 - val_decoder_loss: 0.0607 - val_classifier_accuracy: 0.9179 - val_decoder_mse: 0.0607\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.3601 - classifier_loss: 0.2976 - decoder_loss: 0.0630 - classifier_accuracy: 0.9082 - decoder_mse: 0.0630 - val_loss: 0.3195 - val_classifier_loss: 0.2604 - val_decoder_loss: 0.0597 - val_classifier_accuracy: 0.9205 - val_decoder_mse: 0.0598\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.3103 - classifier_loss: 0.2481 - decoder_loss: 0.0625 - classifier_accuracy: 0.9257 - decoder_mse: 0.0625 - val_loss: 0.3226 - val_classifier_loss: 0.2604 - val_decoder_loss: 0.0595 - val_classifier_accuracy: 0.9241 - val_decoder_mse: 0.0595\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.2829 - classifier_loss: 0.2208 - decoder_loss: 0.0619 - classifier_accuracy: 0.9312 - decoder_mse: 0.0619 - val_loss: 0.3171 - val_classifier_loss: 0.2571 - val_decoder_loss: 0.0584 - val_classifier_accuracy: 0.9214 - val_decoder_mse: 0.0585\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.2599 - classifier_loss: 0.1984 - decoder_loss: 0.0617 - classifier_accuracy: 0.9392 - decoder_mse: 0.0617 - val_loss: 0.3148 - val_classifier_loss: 0.2539 - val_decoder_loss: 0.0583 - val_classifier_accuracy: 0.9214 - val_decoder_mse: 0.0584\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.2411 - classifier_loss: 0.1795 - decoder_loss: 0.0614 - classifier_accuracy: 0.9435 - decoder_mse: 0.0614 - val_loss: 0.3095 - val_classifier_loss: 0.2487 - val_decoder_loss: 0.0583 - val_classifier_accuracy: 0.9259 - val_decoder_mse: 0.0583\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.2176 - classifier_loss: 0.1567 - decoder_loss: 0.0608 - classifier_accuracy: 0.9493 - decoder_mse: 0.0609 - val_loss: 0.2940 - val_classifier_loss: 0.2345 - val_decoder_loss: 0.0574 - val_classifier_accuracy: 0.9277 - val_decoder_mse: 0.0574\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 115us/step - loss: 0.2141 - classifier_loss: 0.1535 - decoder_loss: 0.0607 - classifier_accuracy: 0.9513 - decoder_mse: 0.0607 - val_loss: 0.3344 - val_classifier_loss: 0.2755 - val_decoder_loss: 0.0570 - val_classifier_accuracy: 0.9232 - val_decoder_mse: 0.0570\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 116us/step - loss: 0.2110 - classifier_loss: 0.1502 - decoder_loss: 0.0607 - classifier_accuracy: 0.9521 - decoder_mse: 0.0607 - val_loss: 0.2899 - val_classifier_loss: 0.2301 - val_decoder_loss: 0.0575 - val_classifier_accuracy: 0.9357 - val_decoder_mse: 0.0576\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.2031 - classifier_loss: 0.1425 - decoder_loss: 0.0607 - classifier_accuracy: 0.9538 - decoder_mse: 0.0607 - val_loss: 0.3146 - val_classifier_loss: 0.2561 - val_decoder_loss: 0.0572 - val_classifier_accuracy: 0.9339 - val_decoder_mse: 0.0573\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1989 - classifier_loss: 0.1385 - decoder_loss: 0.0605 - classifier_accuracy: 0.9575 - decoder_mse: 0.0605 - val_loss: 0.2810 - val_classifier_loss: 0.2245 - val_decoder_loss: 0.0567 - val_classifier_accuracy: 0.9366 - val_decoder_mse: 0.0568\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.1801 - classifier_loss: 0.1198 - decoder_loss: 0.0602 - classifier_accuracy: 0.9622 - decoder_mse: 0.0602 - val_loss: 0.3123 - val_classifier_loss: 0.2535 - val_decoder_loss: 0.0565 - val_classifier_accuracy: 0.9366 - val_decoder_mse: 0.0566\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1772 - classifier_loss: 0.1175 - decoder_loss: 0.0597 - classifier_accuracy: 0.9629 - decoder_mse: 0.0597 - val_loss: 0.2654 - val_classifier_loss: 0.2086 - val_decoder_loss: 0.0555 - val_classifier_accuracy: 0.9429 - val_decoder_mse: 0.0556\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 116us/step - loss: 0.1666 - classifier_loss: 0.1072 - decoder_loss: 0.0594 - classifier_accuracy: 0.9669 - decoder_mse: 0.0594 - val_loss: 0.3131 - val_classifier_loss: 0.2535 - val_decoder_loss: 0.0554 - val_classifier_accuracy: 0.9384 - val_decoder_mse: 0.0555\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 115us/step - loss: 0.1571 - classifier_loss: 0.0976 - decoder_loss: 0.0594 - classifier_accuracy: 0.9689 - decoder_mse: 0.0594 - val_loss: 0.2796 - val_classifier_loss: 0.2226 - val_decoder_loss: 0.0550 - val_classifier_accuracy: 0.9384 - val_decoder_mse: 0.0551\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1571 - classifier_loss: 0.0979 - decoder_loss: 0.0591 - classifier_accuracy: 0.9688 - decoder_mse: 0.0591 - val_loss: 0.2837 - val_classifier_loss: 0.2257 - val_decoder_loss: 0.0554 - val_classifier_accuracy: 0.9500 - val_decoder_mse: 0.0554\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1632 - classifier_loss: 0.1040 - decoder_loss: 0.0590 - classifier_accuracy: 0.9654 - decoder_mse: 0.0590 - val_loss: 0.3071 - val_classifier_loss: 0.2493 - val_decoder_loss: 0.0550 - val_classifier_accuracy: 0.9384 - val_decoder_mse: 0.0551\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.1580 - classifier_loss: 0.0991 - decoder_loss: 0.0590 - classifier_accuracy: 0.9687 - decoder_mse: 0.0590 - val_loss: 0.3033 - val_classifier_loss: 0.2465 - val_decoder_loss: 0.0547 - val_classifier_accuracy: 0.9420 - val_decoder_mse: 0.0548\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 115us/step - loss: 0.1403 - classifier_loss: 0.0814 - decoder_loss: 0.0587 - classifier_accuracy: 0.9736 - decoder_mse: 0.0587 - val_loss: 0.3174 - val_classifier_loss: 0.2622 - val_decoder_loss: 0.0542 - val_classifier_accuracy: 0.9357 - val_decoder_mse: 0.0542\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.1342 - classifier_loss: 0.0757 - decoder_loss: 0.0586 - classifier_accuracy: 0.9751 - decoder_mse: 0.0586 - val_loss: 0.3234 - val_classifier_loss: 0.2684 - val_decoder_loss: 0.0540 - val_classifier_accuracy: 0.9357 - val_decoder_mse: 0.0540\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.1393 - classifier_loss: 0.0811 - decoder_loss: 0.0581 - classifier_accuracy: 0.9751 - decoder_mse: 0.0581 - val_loss: 0.3252 - val_classifier_loss: 0.2674 - val_decoder_loss: 0.0545 - val_classifier_accuracy: 0.9411 - val_decoder_mse: 0.0545\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 1s 116us/step - loss: 0.1323 - classifier_loss: 0.0749 - decoder_loss: 0.0580 - classifier_accuracy: 0.9761 - decoder_mse: 0.0580 - val_loss: 0.3387 - val_classifier_loss: 0.2833 - val_decoder_loss: 0.0534 - val_classifier_accuracy: 0.9366 - val_decoder_mse: 0.0534\n",
            "Epoch 27/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.1293 - classifier_loss: 0.0718 - decoder_loss: 0.0576 - classifier_accuracy: 0.9768 - decoder_mse: 0.0576 - val_loss: 0.3069 - val_classifier_loss: 0.2520 - val_decoder_loss: 0.0525 - val_classifier_accuracy: 0.9411 - val_decoder_mse: 0.0525\n",
            "Performance on the validation set\n",
            "loss: 0.32\n",
            "classifier_loss: 0.27\n",
            "decoder_loss: 0.06\n",
            "classifier_accuracy: 0.93\n",
            "decoder_mse: 0.06\n",
            "\n",
            "FOLD:  4\n",
            "Model: \"model_44\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_22 (InputLayer)           (None, 784)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_204 (Dense)               (None, 512)          401920      input_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 512)          0           dense_204[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_205 (Dense)               (None, 256)          131328      dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 256)          0           dense_205[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_206 (Dense)               (None, 128)          32896       dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 128)          0           dense_206[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_207 (Dense)               (None, 64)           8256        dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Dense)                 (None, 32)           2080        dense_207[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_208 (Dense)               (None, 32)           1056        encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_209 (Dense)               (None, 64)           2112        dense_208[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_210 (Dense)               (None, 128)          8320        dense_209[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_211 (Dense)               (None, 256)          33024       dense_210[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_212 (Dense)               (None, 512)          131584      dense_211[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           363         encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          402192      dense_212[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,131\n",
            "Trainable params: 1,155,131\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 316us/step - loss: 1.6297 - classifier_loss: 1.5354 - decoder_loss: 0.0923 - classifier_accuracy: 0.4786 - decoder_mse: 0.0924 - val_loss: 0.7899 - val_classifier_loss: 0.7216 - val_decoder_loss: 0.0680 - val_classifier_accuracy: 0.7857 - val_decoder_mse: 0.0682\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.8285 - classifier_loss: 0.7602 - decoder_loss: 0.0680 - classifier_accuracy: 0.7615 - decoder_mse: 0.0681 - val_loss: 0.5014 - val_classifier_loss: 0.4399 - val_decoder_loss: 0.0610 - val_classifier_accuracy: 0.8679 - val_decoder_mse: 0.0611\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.5923 - classifier_loss: 0.5277 - decoder_loss: 0.0644 - classifier_accuracy: 0.8402 - decoder_mse: 0.0644 - val_loss: 0.4111 - val_classifier_loss: 0.3512 - val_decoder_loss: 0.0594 - val_classifier_accuracy: 0.8911 - val_decoder_mse: 0.0595\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.4864 - classifier_loss: 0.4231 - decoder_loss: 0.0634 - classifier_accuracy: 0.8698 - decoder_mse: 0.0634 - val_loss: 0.3940 - val_classifier_loss: 0.3331 - val_decoder_loss: 0.0591 - val_classifier_accuracy: 0.9000 - val_decoder_mse: 0.0591\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.4232 - classifier_loss: 0.3600 - decoder_loss: 0.0633 - classifier_accuracy: 0.8898 - decoder_mse: 0.0633 - val_loss: 0.3381 - val_classifier_loss: 0.2778 - val_decoder_loss: 0.0588 - val_classifier_accuracy: 0.9116 - val_decoder_mse: 0.0589\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.3551 - classifier_loss: 0.2926 - decoder_loss: 0.0624 - classifier_accuracy: 0.9118 - decoder_mse: 0.0624 - val_loss: 0.3115 - val_classifier_loss: 0.2543 - val_decoder_loss: 0.0571 - val_classifier_accuracy: 0.9223 - val_decoder_mse: 0.0571\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.3054 - classifier_loss: 0.2431 - decoder_loss: 0.0619 - classifier_accuracy: 0.9245 - decoder_mse: 0.0619 - val_loss: 0.2856 - val_classifier_loss: 0.2269 - val_decoder_loss: 0.0571 - val_classifier_accuracy: 0.9366 - val_decoder_mse: 0.0571\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.2971 - classifier_loss: 0.2355 - decoder_loss: 0.0616 - classifier_accuracy: 0.9277 - decoder_mse: 0.0615 - val_loss: 0.2958 - val_classifier_loss: 0.2379 - val_decoder_loss: 0.0570 - val_classifier_accuracy: 0.9321 - val_decoder_mse: 0.0570\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.2600 - classifier_loss: 0.1992 - decoder_loss: 0.0611 - classifier_accuracy: 0.9356 - decoder_mse: 0.0611 - val_loss: 0.2868 - val_classifier_loss: 0.2302 - val_decoder_loss: 0.0564 - val_classifier_accuracy: 0.9321 - val_decoder_mse: 0.0564\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.2371 - classifier_loss: 0.1761 - decoder_loss: 0.0610 - classifier_accuracy: 0.9456 - decoder_mse: 0.0610 - val_loss: 0.2853 - val_classifier_loss: 0.2291 - val_decoder_loss: 0.0558 - val_classifier_accuracy: 0.9366 - val_decoder_mse: 0.0559\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.2444 - classifier_loss: 0.1839 - decoder_loss: 0.0606 - classifier_accuracy: 0.9426 - decoder_mse: 0.0606 - val_loss: 0.2769 - val_classifier_loss: 0.2190 - val_decoder_loss: 0.0555 - val_classifier_accuracy: 0.9393 - val_decoder_mse: 0.0556\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.2245 - classifier_loss: 0.1641 - decoder_loss: 0.0602 - classifier_accuracy: 0.9478 - decoder_mse: 0.0602 - val_loss: 0.2771 - val_classifier_loss: 0.2201 - val_decoder_loss: 0.0552 - val_classifier_accuracy: 0.9384 - val_decoder_mse: 0.0552\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.2040 - classifier_loss: 0.1441 - decoder_loss: 0.0600 - classifier_accuracy: 0.9518 - decoder_mse: 0.0600 - val_loss: 0.2728 - val_classifier_loss: 0.2175 - val_decoder_loss: 0.0548 - val_classifier_accuracy: 0.9455 - val_decoder_mse: 0.0549\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1975 - classifier_loss: 0.1374 - decoder_loss: 0.0600 - classifier_accuracy: 0.9561 - decoder_mse: 0.0600 - val_loss: 0.2884 - val_classifier_loss: 0.2324 - val_decoder_loss: 0.0543 - val_classifier_accuracy: 0.9330 - val_decoder_mse: 0.0544\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1889 - classifier_loss: 0.1292 - decoder_loss: 0.0598 - classifier_accuracy: 0.9603 - decoder_mse: 0.0598 - val_loss: 0.2675 - val_classifier_loss: 0.2132 - val_decoder_loss: 0.0548 - val_classifier_accuracy: 0.9429 - val_decoder_mse: 0.0548\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.1744 - classifier_loss: 0.1152 - decoder_loss: 0.0593 - classifier_accuracy: 0.9613 - decoder_mse: 0.0593 - val_loss: 0.2664 - val_classifier_loss: 0.2131 - val_decoder_loss: 0.0538 - val_classifier_accuracy: 0.9393 - val_decoder_mse: 0.0539\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1686 - classifier_loss: 0.1095 - decoder_loss: 0.0591 - classifier_accuracy: 0.9630 - decoder_mse: 0.0591 - val_loss: 0.3000 - val_classifier_loss: 0.2457 - val_decoder_loss: 0.0539 - val_classifier_accuracy: 0.9420 - val_decoder_mse: 0.0539\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1694 - classifier_loss: 0.1102 - decoder_loss: 0.0593 - classifier_accuracy: 0.9657 - decoder_mse: 0.0593 - val_loss: 0.2783 - val_classifier_loss: 0.2244 - val_decoder_loss: 0.0538 - val_classifier_accuracy: 0.9384 - val_decoder_mse: 0.0539\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.1586 - classifier_loss: 0.0997 - decoder_loss: 0.0590 - classifier_accuracy: 0.9679 - decoder_mse: 0.0590 - val_loss: 0.2805 - val_classifier_loss: 0.2249 - val_decoder_loss: 0.0537 - val_classifier_accuracy: 0.9384 - val_decoder_mse: 0.0538\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.1570 - classifier_loss: 0.0983 - decoder_loss: 0.0588 - classifier_accuracy: 0.9679 - decoder_mse: 0.0588 - val_loss: 0.2850 - val_classifier_loss: 0.2320 - val_decoder_loss: 0.0533 - val_classifier_accuracy: 0.9357 - val_decoder_mse: 0.0533\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1489 - classifier_loss: 0.0901 - decoder_loss: 0.0588 - classifier_accuracy: 0.9722 - decoder_mse: 0.0588 - val_loss: 0.3166 - val_classifier_loss: 0.2616 - val_decoder_loss: 0.0530 - val_classifier_accuracy: 0.9384 - val_decoder_mse: 0.0531\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1635 - classifier_loss: 0.1049 - decoder_loss: 0.0586 - classifier_accuracy: 0.9670 - decoder_mse: 0.0586 - val_loss: 0.2985 - val_classifier_loss: 0.2444 - val_decoder_loss: 0.0523 - val_classifier_accuracy: 0.9357 - val_decoder_mse: 0.0523\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1484 - classifier_loss: 0.0901 - decoder_loss: 0.0582 - classifier_accuracy: 0.9692 - decoder_mse: 0.0582 - val_loss: 0.3103 - val_classifier_loss: 0.2560 - val_decoder_loss: 0.0524 - val_classifier_accuracy: 0.9339 - val_decoder_mse: 0.0525\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.1370 - classifier_loss: 0.0784 - decoder_loss: 0.0585 - classifier_accuracy: 0.9740 - decoder_mse: 0.0585 - val_loss: 0.2877 - val_classifier_loss: 0.2330 - val_decoder_loss: 0.0533 - val_classifier_accuracy: 0.9455 - val_decoder_mse: 0.0533\n",
            "Epoch 25/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.1352 - classifier_loss: 0.0773 - decoder_loss: 0.0582 - classifier_accuracy: 0.9767 - decoder_mse: 0.0582 - val_loss: 0.2802 - val_classifier_loss: 0.2264 - val_decoder_loss: 0.0524 - val_classifier_accuracy: 0.9339 - val_decoder_mse: 0.0524\n",
            "Epoch 26/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.1337 - classifier_loss: 0.0758 - decoder_loss: 0.0577 - classifier_accuracy: 0.9761 - decoder_mse: 0.0577 - val_loss: 0.3163 - val_classifier_loss: 0.2643 - val_decoder_loss: 0.0516 - val_classifier_accuracy: 0.9330 - val_decoder_mse: 0.0517\n",
            "Performance on the validation set\n",
            "loss: 0.32\n",
            "classifier_loss: 0.27\n",
            "decoder_loss: 0.05\n",
            "classifier_accuracy: 0.94\n",
            "decoder_mse: 0.05\n",
            "\n",
            "FOLD:  5\n",
            "Model: \"model_46\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_23 (InputLayer)           (None, 784)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_213 (Dense)               (None, 512)          401920      input_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 512)          0           dense_213[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_214 (Dense)               (None, 256)          131328      dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 256)          0           dense_214[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_215 (Dense)               (None, 128)          32896       dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 128)          0           dense_215[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_216 (Dense)               (None, 64)           8256        dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Dense)                 (None, 32)           2080        dense_216[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_217 (Dense)               (None, 32)           1056        encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_218 (Dense)               (None, 64)           2112        dense_217[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_219 (Dense)               (None, 128)          8320        dense_218[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_220 (Dense)               (None, 256)          33024       dense_219[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_221 (Dense)               (None, 512)          131584      dense_220[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           363         encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          402192      dense_221[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,131\n",
            "Trainable params: 1,155,131\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 326us/step - loss: 1.5313 - classifier_loss: 1.4383 - decoder_loss: 0.0916 - classifier_accuracy: 0.5127 - decoder_mse: 0.0917 - val_loss: 0.7929 - val_classifier_loss: 0.7198 - val_decoder_loss: 0.0699 - val_classifier_accuracy: 0.7732 - val_decoder_mse: 0.0698\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.7729 - classifier_loss: 0.7042 - decoder_loss: 0.0684 - classifier_accuracy: 0.7789 - decoder_mse: 0.0684 - val_loss: 0.5216 - val_classifier_loss: 0.4576 - val_decoder_loss: 0.0629 - val_classifier_accuracy: 0.8661 - val_decoder_mse: 0.0628\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.5593 - classifier_loss: 0.4943 - decoder_loss: 0.0648 - classifier_accuracy: 0.8531 - decoder_mse: 0.0648 - val_loss: 0.4306 - val_classifier_loss: 0.3672 - val_decoder_loss: 0.0610 - val_classifier_accuracy: 0.8875 - val_decoder_mse: 0.0609\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.4477 - classifier_loss: 0.3834 - decoder_loss: 0.0638 - classifier_accuracy: 0.8823 - decoder_mse: 0.0638 - val_loss: 0.3698 - val_classifier_loss: 0.3094 - val_decoder_loss: 0.0594 - val_classifier_accuracy: 0.9107 - val_decoder_mse: 0.0594\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.3793 - classifier_loss: 0.3163 - decoder_loss: 0.0627 - classifier_accuracy: 0.9066 - decoder_mse: 0.0627 - val_loss: 0.3479 - val_classifier_loss: 0.2870 - val_decoder_loss: 0.0589 - val_classifier_accuracy: 0.9161 - val_decoder_mse: 0.0589\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.3353 - classifier_loss: 0.2734 - decoder_loss: 0.0619 - classifier_accuracy: 0.9156 - decoder_mse: 0.0619 - val_loss: 0.3514 - val_classifier_loss: 0.2930 - val_decoder_loss: 0.0573 - val_classifier_accuracy: 0.9143 - val_decoder_mse: 0.0573\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.3073 - classifier_loss: 0.2454 - decoder_loss: 0.0619 - classifier_accuracy: 0.9251 - decoder_mse: 0.0619 - val_loss: 0.3047 - val_classifier_loss: 0.2448 - val_decoder_loss: 0.0574 - val_classifier_accuracy: 0.9286 - val_decoder_mse: 0.0574\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.2674 - classifier_loss: 0.2061 - decoder_loss: 0.0613 - classifier_accuracy: 0.9370 - decoder_mse: 0.0613 - val_loss: 0.3327 - val_classifier_loss: 0.2738 - val_decoder_loss: 0.0563 - val_classifier_accuracy: 0.9250 - val_decoder_mse: 0.0563\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.2589 - classifier_loss: 0.1975 - decoder_loss: 0.0612 - classifier_accuracy: 0.9388 - decoder_mse: 0.0612 - val_loss: 0.3253 - val_classifier_loss: 0.2659 - val_decoder_loss: 0.0566 - val_classifier_accuracy: 0.9295 - val_decoder_mse: 0.0565\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 117us/step - loss: 0.2349 - classifier_loss: 0.1744 - decoder_loss: 0.0609 - classifier_accuracy: 0.9471 - decoder_mse: 0.0609 - val_loss: 0.3075 - val_classifier_loss: 0.2478 - val_decoder_loss: 0.0567 - val_classifier_accuracy: 0.9330 - val_decoder_mse: 0.0566\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.2225 - classifier_loss: 0.1621 - decoder_loss: 0.0600 - classifier_accuracy: 0.9499 - decoder_mse: 0.0600 - val_loss: 0.3214 - val_classifier_loss: 0.2623 - val_decoder_loss: 0.0560 - val_classifier_accuracy: 0.9295 - val_decoder_mse: 0.0560\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.2077 - classifier_loss: 0.1472 - decoder_loss: 0.0603 - classifier_accuracy: 0.9539 - decoder_mse: 0.0603 - val_loss: 0.3188 - val_classifier_loss: 0.2600 - val_decoder_loss: 0.0548 - val_classifier_accuracy: 0.9339 - val_decoder_mse: 0.0547\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.2121 - classifier_loss: 0.1521 - decoder_loss: 0.0602 - classifier_accuracy: 0.9519 - decoder_mse: 0.0602 - val_loss: 0.3012 - val_classifier_loss: 0.2433 - val_decoder_loss: 0.0548 - val_classifier_accuracy: 0.9330 - val_decoder_mse: 0.0548\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.1905 - classifier_loss: 0.1308 - decoder_loss: 0.0597 - classifier_accuracy: 0.9599 - decoder_mse: 0.0598 - val_loss: 0.3101 - val_classifier_loss: 0.2549 - val_decoder_loss: 0.0550 - val_classifier_accuracy: 0.9348 - val_decoder_mse: 0.0550\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.1757 - classifier_loss: 0.1160 - decoder_loss: 0.0596 - classifier_accuracy: 0.9623 - decoder_mse: 0.0596 - val_loss: 0.3032 - val_classifier_loss: 0.2446 - val_decoder_loss: 0.0549 - val_classifier_accuracy: 0.9420 - val_decoder_mse: 0.0549\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.1797 - classifier_loss: 0.1202 - decoder_loss: 0.0594 - classifier_accuracy: 0.9639 - decoder_mse: 0.0595 - val_loss: 0.3205 - val_classifier_loss: 0.2624 - val_decoder_loss: 0.0546 - val_classifier_accuracy: 0.9348 - val_decoder_mse: 0.0546\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 118us/step - loss: 0.1769 - classifier_loss: 0.1172 - decoder_loss: 0.0596 - classifier_accuracy: 0.9634 - decoder_mse: 0.0596 - val_loss: 0.3157 - val_classifier_loss: 0.2587 - val_decoder_loss: 0.0551 - val_classifier_accuracy: 0.9357 - val_decoder_mse: 0.0551\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.1615 - classifier_loss: 0.1026 - decoder_loss: 0.0591 - classifier_accuracy: 0.9668 - decoder_mse: 0.0591 - val_loss: 0.3129 - val_classifier_loss: 0.2557 - val_decoder_loss: 0.0542 - val_classifier_accuracy: 0.9357 - val_decoder_mse: 0.0542\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.1532 - classifier_loss: 0.0947 - decoder_loss: 0.0590 - classifier_accuracy: 0.9701 - decoder_mse: 0.0590 - val_loss: 0.3243 - val_classifier_loss: 0.2687 - val_decoder_loss: 0.0538 - val_classifier_accuracy: 0.9295 - val_decoder_mse: 0.0538\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.1445 - classifier_loss: 0.0859 - decoder_loss: 0.0589 - classifier_accuracy: 0.9750 - decoder_mse: 0.0589 - val_loss: 0.3155 - val_classifier_loss: 0.2580 - val_decoder_loss: 0.0540 - val_classifier_accuracy: 0.9402 - val_decoder_mse: 0.0540\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.1562 - classifier_loss: 0.0974 - decoder_loss: 0.0587 - classifier_accuracy: 0.9684 - decoder_mse: 0.0587 - val_loss: 0.3120 - val_classifier_loss: 0.2561 - val_decoder_loss: 0.0529 - val_classifier_accuracy: 0.9384 - val_decoder_mse: 0.0529\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1452 - classifier_loss: 0.0866 - decoder_loss: 0.0585 - classifier_accuracy: 0.9733 - decoder_mse: 0.0585 - val_loss: 0.3359 - val_classifier_loss: 0.2798 - val_decoder_loss: 0.0530 - val_classifier_accuracy: 0.9384 - val_decoder_mse: 0.0530\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1418 - classifier_loss: 0.0836 - decoder_loss: 0.0582 - classifier_accuracy: 0.9714 - decoder_mse: 0.0582 - val_loss: 0.3158 - val_classifier_loss: 0.2615 - val_decoder_loss: 0.0523 - val_classifier_accuracy: 0.9375 - val_decoder_mse: 0.0523\n",
            "Performance on the validation set\n",
            "loss: 0.33\n",
            "classifier_loss: 0.28\n",
            "decoder_loss: 0.06\n",
            "classifier_accuracy: 0.93\n",
            "decoder_mse: 0.06\n",
            "\n",
            "FOLD:  6\n",
            "Model: \"model_48\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_24 (InputLayer)           (None, 784)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_222 (Dense)               (None, 512)          401920      input_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 512)          0           dense_222[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_223 (Dense)               (None, 256)          131328      dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 256)          0           dense_223[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_224 (Dense)               (None, 128)          32896       dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 128)          0           dense_224[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_225 (Dense)               (None, 64)           8256        dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Dense)                 (None, 32)           2080        dense_225[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_226 (Dense)               (None, 32)           1056        encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_227 (Dense)               (None, 64)           2112        dense_226[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_228 (Dense)               (None, 128)          8320        dense_227[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_229 (Dense)               (None, 256)          33024       dense_228[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_230 (Dense)               (None, 512)          131584      dense_229[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           363         encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          402192      dense_230[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,131\n",
            "Trainable params: 1,155,131\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 333us/step - loss: 1.7268 - classifier_loss: 1.6293 - decoder_loss: 0.0955 - classifier_accuracy: 0.4564 - decoder_mse: 0.0956 - val_loss: 0.8215 - val_classifier_loss: 0.7517 - val_decoder_loss: 0.0733 - val_classifier_accuracy: 0.7661 - val_decoder_mse: 0.0733\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.8509 - classifier_loss: 0.7801 - decoder_loss: 0.0698 - classifier_accuracy: 0.7533 - decoder_mse: 0.0698 - val_loss: 0.5086 - val_classifier_loss: 0.4465 - val_decoder_loss: 0.0636 - val_classifier_accuracy: 0.8652 - val_decoder_mse: 0.0636\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.5853 - classifier_loss: 0.5199 - decoder_loss: 0.0651 - classifier_accuracy: 0.8407 - decoder_mse: 0.0651 - val_loss: 0.3993 - val_classifier_loss: 0.3400 - val_decoder_loss: 0.0620 - val_classifier_accuracy: 0.8920 - val_decoder_mse: 0.0619\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.4795 - classifier_loss: 0.4150 - decoder_loss: 0.0643 - classifier_accuracy: 0.8763 - decoder_mse: 0.0643 - val_loss: 0.3351 - val_classifier_loss: 0.2746 - val_decoder_loss: 0.0614 - val_classifier_accuracy: 0.9152 - val_decoder_mse: 0.0613\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.3946 - classifier_loss: 0.3310 - decoder_loss: 0.0636 - classifier_accuracy: 0.9042 - decoder_mse: 0.0636 - val_loss: 0.3002 - val_classifier_loss: 0.2399 - val_decoder_loss: 0.0605 - val_classifier_accuracy: 0.9268 - val_decoder_mse: 0.0604\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.3517 - classifier_loss: 0.2883 - decoder_loss: 0.0631 - classifier_accuracy: 0.9165 - decoder_mse: 0.0631 - val_loss: 0.2897 - val_classifier_loss: 0.2297 - val_decoder_loss: 0.0601 - val_classifier_accuracy: 0.9312 - val_decoder_mse: 0.0600\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.3054 - classifier_loss: 0.2424 - decoder_loss: 0.0627 - classifier_accuracy: 0.9264 - decoder_mse: 0.0627 - val_loss: 0.2742 - val_classifier_loss: 0.2161 - val_decoder_loss: 0.0596 - val_classifier_accuracy: 0.9286 - val_decoder_mse: 0.0595\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.2815 - classifier_loss: 0.2196 - decoder_loss: 0.0619 - classifier_accuracy: 0.9319 - decoder_mse: 0.0619 - val_loss: 0.2823 - val_classifier_loss: 0.2237 - val_decoder_loss: 0.0589 - val_classifier_accuracy: 0.9339 - val_decoder_mse: 0.0588\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.2493 - classifier_loss: 0.1881 - decoder_loss: 0.0615 - classifier_accuracy: 0.9422 - decoder_mse: 0.0615 - val_loss: 0.2821 - val_classifier_loss: 0.2231 - val_decoder_loss: 0.0579 - val_classifier_accuracy: 0.9339 - val_decoder_mse: 0.0578\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.2306 - classifier_loss: 0.1693 - decoder_loss: 0.0612 - classifier_accuracy: 0.9481 - decoder_mse: 0.0612 - val_loss: 0.2711 - val_classifier_loss: 0.2131 - val_decoder_loss: 0.0581 - val_classifier_accuracy: 0.9384 - val_decoder_mse: 0.0580\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.2240 - classifier_loss: 0.1630 - decoder_loss: 0.0609 - classifier_accuracy: 0.9497 - decoder_mse: 0.0609 - val_loss: 0.2437 - val_classifier_loss: 0.1868 - val_decoder_loss: 0.0574 - val_classifier_accuracy: 0.9491 - val_decoder_mse: 0.0574\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.2126 - classifier_loss: 0.1516 - decoder_loss: 0.0608 - classifier_accuracy: 0.9515 - decoder_mse: 0.0608 - val_loss: 0.2830 - val_classifier_loss: 0.2255 - val_decoder_loss: 0.0570 - val_classifier_accuracy: 0.9304 - val_decoder_mse: 0.0570\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.2061 - classifier_loss: 0.1454 - decoder_loss: 0.0605 - classifier_accuracy: 0.9548 - decoder_mse: 0.0605 - val_loss: 0.3024 - val_classifier_loss: 0.2449 - val_decoder_loss: 0.0565 - val_classifier_accuracy: 0.9250 - val_decoder_mse: 0.0564\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.1940 - classifier_loss: 0.1334 - decoder_loss: 0.0605 - classifier_accuracy: 0.9576 - decoder_mse: 0.0605 - val_loss: 0.2843 - val_classifier_loss: 0.2283 - val_decoder_loss: 0.0562 - val_classifier_accuracy: 0.9339 - val_decoder_mse: 0.0561\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.1696 - classifier_loss: 0.1097 - decoder_loss: 0.0599 - classifier_accuracy: 0.9656 - decoder_mse: 0.0599 - val_loss: 0.2663 - val_classifier_loss: 0.2081 - val_decoder_loss: 0.0562 - val_classifier_accuracy: 0.9429 - val_decoder_mse: 0.0562\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.1730 - classifier_loss: 0.1132 - decoder_loss: 0.0598 - classifier_accuracy: 0.9649 - decoder_mse: 0.0598 - val_loss: 0.2762 - val_classifier_loss: 0.2192 - val_decoder_loss: 0.0558 - val_classifier_accuracy: 0.9384 - val_decoder_mse: 0.0557\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1761 - classifier_loss: 0.1163 - decoder_loss: 0.0597 - classifier_accuracy: 0.9636 - decoder_mse: 0.0597 - val_loss: 0.2925 - val_classifier_loss: 0.2356 - val_decoder_loss: 0.0559 - val_classifier_accuracy: 0.9366 - val_decoder_mse: 0.0558\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1652 - classifier_loss: 0.1054 - decoder_loss: 0.0597 - classifier_accuracy: 0.9647 - decoder_mse: 0.0597 - val_loss: 0.2882 - val_classifier_loss: 0.2318 - val_decoder_loss: 0.0558 - val_classifier_accuracy: 0.9366 - val_decoder_mse: 0.0557\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1513 - classifier_loss: 0.0922 - decoder_loss: 0.0593 - classifier_accuracy: 0.9688 - decoder_mse: 0.0593 - val_loss: 0.2753 - val_classifier_loss: 0.2173 - val_decoder_loss: 0.0557 - val_classifier_accuracy: 0.9402 - val_decoder_mse: 0.0557\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.1522 - classifier_loss: 0.0932 - decoder_loss: 0.0589 - classifier_accuracy: 0.9698 - decoder_mse: 0.0589 - val_loss: 0.2761 - val_classifier_loss: 0.2203 - val_decoder_loss: 0.0548 - val_classifier_accuracy: 0.9402 - val_decoder_mse: 0.0547\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.1507 - classifier_loss: 0.0915 - decoder_loss: 0.0590 - classifier_accuracy: 0.9714 - decoder_mse: 0.0590 - val_loss: 0.2904 - val_classifier_loss: 0.2353 - val_decoder_loss: 0.0547 - val_classifier_accuracy: 0.9482 - val_decoder_mse: 0.0547\n",
            "Performance on the validation set\n",
            "loss: 0.32\n",
            "classifier_loss: 0.26\n",
            "decoder_loss: 0.06\n",
            "classifier_accuracy: 0.93\n",
            "decoder_mse: 0.06\n",
            "\n",
            "FOLD:  7\n",
            "Model: \"model_50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_25 (InputLayer)           (None, 784)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_231 (Dense)               (None, 512)          401920      input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 512)          0           dense_231[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_232 (Dense)               (None, 256)          131328      dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 256)          0           dense_232[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_233 (Dense)               (None, 128)          32896       dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 128)          0           dense_233[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_234 (Dense)               (None, 64)           8256        dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Dense)                 (None, 32)           2080        dense_234[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_235 (Dense)               (None, 32)           1056        encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_236 (Dense)               (None, 64)           2112        dense_235[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_237 (Dense)               (None, 128)          8320        dense_236[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_238 (Dense)               (None, 256)          33024       dense_237[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_239 (Dense)               (None, 512)          131584      dense_238[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           363         encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          402192      dense_239[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,131\n",
            "Trainable params: 1,155,131\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 3s 339us/step - loss: 1.6421 - classifier_loss: 1.5471 - decoder_loss: 0.0928 - classifier_accuracy: 0.4825 - decoder_mse: 0.0928 - val_loss: 0.8494 - val_classifier_loss: 0.7786 - val_decoder_loss: 0.0704 - val_classifier_accuracy: 0.7589 - val_decoder_mse: 0.0704\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.8413 - classifier_loss: 0.7709 - decoder_loss: 0.0698 - classifier_accuracy: 0.7551 - decoder_mse: 0.0698 - val_loss: 0.5531 - val_classifier_loss: 0.4906 - val_decoder_loss: 0.0626 - val_classifier_accuracy: 0.8509 - val_decoder_mse: 0.0626\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.6081 - classifier_loss: 0.5423 - decoder_loss: 0.0653 - classifier_accuracy: 0.8277 - decoder_mse: 0.0653 - val_loss: 0.4563 - val_classifier_loss: 0.3950 - val_decoder_loss: 0.0606 - val_classifier_accuracy: 0.8893 - val_decoder_mse: 0.0606\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 125us/step - loss: 0.4869 - classifier_loss: 0.4230 - decoder_loss: 0.0640 - classifier_accuracy: 0.8689 - decoder_mse: 0.0640 - val_loss: 0.3798 - val_classifier_loss: 0.3196 - val_decoder_loss: 0.0590 - val_classifier_accuracy: 0.9089 - val_decoder_mse: 0.0591\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.4008 - classifier_loss: 0.3379 - decoder_loss: 0.0627 - classifier_accuracy: 0.8980 - decoder_mse: 0.0627 - val_loss: 0.3497 - val_classifier_loss: 0.2898 - val_decoder_loss: 0.0583 - val_classifier_accuracy: 0.9250 - val_decoder_mse: 0.0583\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 125us/step - loss: 0.3494 - classifier_loss: 0.2873 - decoder_loss: 0.0621 - classifier_accuracy: 0.9139 - decoder_mse: 0.0621 - val_loss: 0.3508 - val_classifier_loss: 0.2907 - val_decoder_loss: 0.0581 - val_classifier_accuracy: 0.9241 - val_decoder_mse: 0.0581\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 125us/step - loss: 0.3119 - classifier_loss: 0.2503 - decoder_loss: 0.0617 - classifier_accuracy: 0.9221 - decoder_mse: 0.0617 - val_loss: 0.3288 - val_classifier_loss: 0.2686 - val_decoder_loss: 0.0576 - val_classifier_accuracy: 0.9241 - val_decoder_mse: 0.0577\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 125us/step - loss: 0.2872 - classifier_loss: 0.2256 - decoder_loss: 0.0616 - classifier_accuracy: 0.9314 - decoder_mse: 0.0616 - val_loss: 0.3450 - val_classifier_loss: 0.2866 - val_decoder_loss: 0.0564 - val_classifier_accuracy: 0.9330 - val_decoder_mse: 0.0565\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 125us/step - loss: 0.2598 - classifier_loss: 0.1983 - decoder_loss: 0.0613 - classifier_accuracy: 0.9400 - decoder_mse: 0.0613 - val_loss: 0.3251 - val_classifier_loss: 0.2664 - val_decoder_loss: 0.0559 - val_classifier_accuracy: 0.9366 - val_decoder_mse: 0.0559\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 124us/step - loss: 0.2444 - classifier_loss: 0.1834 - decoder_loss: 0.0608 - classifier_accuracy: 0.9439 - decoder_mse: 0.0608 - val_loss: 0.3235 - val_classifier_loss: 0.2651 - val_decoder_loss: 0.0563 - val_classifier_accuracy: 0.9357 - val_decoder_mse: 0.0563\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.2205 - classifier_loss: 0.1602 - decoder_loss: 0.0603 - classifier_accuracy: 0.9505 - decoder_mse: 0.0604 - val_loss: 0.3148 - val_classifier_loss: 0.2564 - val_decoder_loss: 0.0559 - val_classifier_accuracy: 0.9366 - val_decoder_mse: 0.0560\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.2070 - classifier_loss: 0.1469 - decoder_loss: 0.0604 - classifier_accuracy: 0.9524 - decoder_mse: 0.0604 - val_loss: 0.3438 - val_classifier_loss: 0.2852 - val_decoder_loss: 0.0555 - val_classifier_accuracy: 0.9312 - val_decoder_mse: 0.0555\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.2067 - classifier_loss: 0.1467 - decoder_loss: 0.0601 - classifier_accuracy: 0.9535 - decoder_mse: 0.0601 - val_loss: 0.3408 - val_classifier_loss: 0.2830 - val_decoder_loss: 0.0547 - val_classifier_accuracy: 0.9330 - val_decoder_mse: 0.0547\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1976 - classifier_loss: 0.1377 - decoder_loss: 0.0601 - classifier_accuracy: 0.9570 - decoder_mse: 0.0601 - val_loss: 0.3551 - val_classifier_loss: 0.2978 - val_decoder_loss: 0.0549 - val_classifier_accuracy: 0.9348 - val_decoder_mse: 0.0549\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.1931 - classifier_loss: 0.1333 - decoder_loss: 0.0598 - classifier_accuracy: 0.9600 - decoder_mse: 0.0598 - val_loss: 0.3221 - val_classifier_loss: 0.2652 - val_decoder_loss: 0.0545 - val_classifier_accuracy: 0.9402 - val_decoder_mse: 0.0545\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1887 - classifier_loss: 0.1290 - decoder_loss: 0.0595 - classifier_accuracy: 0.9605 - decoder_mse: 0.0595 - val_loss: 0.3416 - val_classifier_loss: 0.2846 - val_decoder_loss: 0.0537 - val_classifier_accuracy: 0.9375 - val_decoder_mse: 0.0537\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.1657 - classifier_loss: 0.1070 - decoder_loss: 0.0589 - classifier_accuracy: 0.9663 - decoder_mse: 0.0589 - val_loss: 0.3340 - val_classifier_loss: 0.2761 - val_decoder_loss: 0.0543 - val_classifier_accuracy: 0.9384 - val_decoder_mse: 0.0543\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 119us/step - loss: 0.1594 - classifier_loss: 0.1009 - decoder_loss: 0.0590 - classifier_accuracy: 0.9676 - decoder_mse: 0.0590 - val_loss: 0.3554 - val_classifier_loss: 0.2994 - val_decoder_loss: 0.0538 - val_classifier_accuracy: 0.9357 - val_decoder_mse: 0.0538\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1659 - classifier_loss: 0.1069 - decoder_loss: 0.0590 - classifier_accuracy: 0.9654 - decoder_mse: 0.0590 - val_loss: 0.3692 - val_classifier_loss: 0.3115 - val_decoder_loss: 0.0539 - val_classifier_accuracy: 0.9339 - val_decoder_mse: 0.0539\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1567 - classifier_loss: 0.0980 - decoder_loss: 0.0588 - classifier_accuracy: 0.9682 - decoder_mse: 0.0588 - val_loss: 0.3559 - val_classifier_loss: 0.2981 - val_decoder_loss: 0.0537 - val_classifier_accuracy: 0.9402 - val_decoder_mse: 0.0537\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.1468 - classifier_loss: 0.0879 - decoder_loss: 0.0587 - classifier_accuracy: 0.9718 - decoder_mse: 0.0587 - val_loss: 0.3648 - val_classifier_loss: 0.3090 - val_decoder_loss: 0.0531 - val_classifier_accuracy: 0.9357 - val_decoder_mse: 0.0531\n",
            "Performance on the validation set\n",
            "loss: 0.32\n",
            "classifier_loss: 0.26\n",
            "decoder_loss: 0.06\n",
            "classifier_accuracy: 0.93\n",
            "decoder_mse: 0.06\n",
            "\n",
            "FOLD:  8\n",
            "Model: \"model_52\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_26 (InputLayer)           (None, 784)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_240 (Dense)               (None, 512)          401920      input_26[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 512)          0           dense_240[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_241 (Dense)               (None, 256)          131328      dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 256)          0           dense_241[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_242 (Dense)               (None, 128)          32896       dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 128)          0           dense_242[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_243 (Dense)               (None, 64)           8256        dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Dense)                 (None, 32)           2080        dense_243[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_244 (Dense)               (None, 32)           1056        encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_245 (Dense)               (None, 64)           2112        dense_244[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_246 (Dense)               (None, 128)          8320        dense_245[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_247 (Dense)               (None, 256)          33024       dense_246[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_248 (Dense)               (None, 512)          131584      dense_247[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           363         encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          402192      dense_248[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,131\n",
            "Trainable params: 1,155,131\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 4s 353us/step - loss: 1.6466 - classifier_loss: 1.5513 - decoder_loss: 0.0936 - classifier_accuracy: 0.4760 - decoder_mse: 0.0937 - val_loss: 0.8606 - val_classifier_loss: 0.7927 - val_decoder_loss: 0.0715 - val_classifier_accuracy: 0.7536 - val_decoder_mse: 0.0716\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.8090 - classifier_loss: 0.7390 - decoder_loss: 0.0694 - classifier_accuracy: 0.7703 - decoder_mse: 0.0694 - val_loss: 0.5379 - val_classifier_loss: 0.4766 - val_decoder_loss: 0.0626 - val_classifier_accuracy: 0.8580 - val_decoder_mse: 0.0626\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.5706 - classifier_loss: 0.5059 - decoder_loss: 0.0645 - classifier_accuracy: 0.8464 - decoder_mse: 0.0645 - val_loss: 0.4288 - val_classifier_loss: 0.3703 - val_decoder_loss: 0.0608 - val_classifier_accuracy: 0.8920 - val_decoder_mse: 0.0609\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.4552 - classifier_loss: 0.3922 - decoder_loss: 0.0631 - classifier_accuracy: 0.8800 - decoder_mse: 0.0631 - val_loss: 0.3850 - val_classifier_loss: 0.3306 - val_decoder_loss: 0.0584 - val_classifier_accuracy: 0.8946 - val_decoder_mse: 0.0585\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.3852 - classifier_loss: 0.3230 - decoder_loss: 0.0621 - classifier_accuracy: 0.9030 - decoder_mse: 0.0621 - val_loss: 0.3437 - val_classifier_loss: 0.2877 - val_decoder_loss: 0.0574 - val_classifier_accuracy: 0.9161 - val_decoder_mse: 0.0575\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.3347 - classifier_loss: 0.2732 - decoder_loss: 0.0615 - classifier_accuracy: 0.9174 - decoder_mse: 0.0615 - val_loss: 0.3325 - val_classifier_loss: 0.2788 - val_decoder_loss: 0.0573 - val_classifier_accuracy: 0.9259 - val_decoder_mse: 0.0574\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.3009 - classifier_loss: 0.2398 - decoder_loss: 0.0611 - classifier_accuracy: 0.9268 - decoder_mse: 0.0611 - val_loss: 0.3333 - val_classifier_loss: 0.2790 - val_decoder_loss: 0.0565 - val_classifier_accuracy: 0.9161 - val_decoder_mse: 0.0566\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.2748 - classifier_loss: 0.2137 - decoder_loss: 0.0608 - classifier_accuracy: 0.9358 - decoder_mse: 0.0608 - val_loss: 0.3341 - val_classifier_loss: 0.2781 - val_decoder_loss: 0.0573 - val_classifier_accuracy: 0.9187 - val_decoder_mse: 0.0574\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.2531 - classifier_loss: 0.1925 - decoder_loss: 0.0609 - classifier_accuracy: 0.9412 - decoder_mse: 0.0609 - val_loss: 0.3332 - val_classifier_loss: 0.2786 - val_decoder_loss: 0.0568 - val_classifier_accuracy: 0.9187 - val_decoder_mse: 0.0569\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.2356 - classifier_loss: 0.1748 - decoder_loss: 0.0605 - classifier_accuracy: 0.9446 - decoder_mse: 0.0605 - val_loss: 0.3169 - val_classifier_loss: 0.2621 - val_decoder_loss: 0.0560 - val_classifier_accuracy: 0.9268 - val_decoder_mse: 0.0561\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.2204 - classifier_loss: 0.1603 - decoder_loss: 0.0601 - classifier_accuracy: 0.9500 - decoder_mse: 0.0601 - val_loss: 0.3419 - val_classifier_loss: 0.2874 - val_decoder_loss: 0.0559 - val_classifier_accuracy: 0.9152 - val_decoder_mse: 0.0560\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.2195 - classifier_loss: 0.1592 - decoder_loss: 0.0601 - classifier_accuracy: 0.9495 - decoder_mse: 0.0601 - val_loss: 0.3191 - val_classifier_loss: 0.2667 - val_decoder_loss: 0.0554 - val_classifier_accuracy: 0.9259 - val_decoder_mse: 0.0555\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.1960 - classifier_loss: 0.1363 - decoder_loss: 0.0597 - classifier_accuracy: 0.9571 - decoder_mse: 0.0597 - val_loss: 0.3268 - val_classifier_loss: 0.2733 - val_decoder_loss: 0.0550 - val_classifier_accuracy: 0.9312 - val_decoder_mse: 0.0552\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.1884 - classifier_loss: 0.1288 - decoder_loss: 0.0596 - classifier_accuracy: 0.9604 - decoder_mse: 0.0596 - val_loss: 0.3201 - val_classifier_loss: 0.2686 - val_decoder_loss: 0.0551 - val_classifier_accuracy: 0.9277 - val_decoder_mse: 0.0552\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.1759 - classifier_loss: 0.1166 - decoder_loss: 0.0595 - classifier_accuracy: 0.9653 - decoder_mse: 0.0595 - val_loss: 0.3209 - val_classifier_loss: 0.2703 - val_decoder_loss: 0.0547 - val_classifier_accuracy: 0.9393 - val_decoder_mse: 0.0548\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1762 - classifier_loss: 0.1170 - decoder_loss: 0.0591 - classifier_accuracy: 0.9635 - decoder_mse: 0.0591 - val_loss: 0.3231 - val_classifier_loss: 0.2701 - val_decoder_loss: 0.0550 - val_classifier_accuracy: 0.9312 - val_decoder_mse: 0.0551\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.1650 - classifier_loss: 0.1062 - decoder_loss: 0.0589 - classifier_accuracy: 0.9667 - decoder_mse: 0.0589 - val_loss: 0.3493 - val_classifier_loss: 0.2948 - val_decoder_loss: 0.0549 - val_classifier_accuracy: 0.9223 - val_decoder_mse: 0.0550\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1673 - classifier_loss: 0.1082 - decoder_loss: 0.0588 - classifier_accuracy: 0.9636 - decoder_mse: 0.0588 - val_loss: 0.3374 - val_classifier_loss: 0.2847 - val_decoder_loss: 0.0541 - val_classifier_accuracy: 0.9277 - val_decoder_mse: 0.0542\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.1547 - classifier_loss: 0.0957 - decoder_loss: 0.0589 - classifier_accuracy: 0.9698 - decoder_mse: 0.0589 - val_loss: 0.3374 - val_classifier_loss: 0.2848 - val_decoder_loss: 0.0541 - val_classifier_accuracy: 0.9357 - val_decoder_mse: 0.0542\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.1458 - classifier_loss: 0.0871 - decoder_loss: 0.0587 - classifier_accuracy: 0.9731 - decoder_mse: 0.0587 - val_loss: 0.3436 - val_classifier_loss: 0.2927 - val_decoder_loss: 0.0539 - val_classifier_accuracy: 0.9250 - val_decoder_mse: 0.0540\n",
            "Performance on the validation set\n",
            "loss: 0.32\n",
            "classifier_loss: 0.26\n",
            "decoder_loss: 0.06\n",
            "classifier_accuracy: 0.93\n",
            "decoder_mse: 0.06\n",
            "\n",
            "FOLD:  9\n",
            "Model: \"model_54\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_27 (InputLayer)           (None, 784)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_249 (Dense)               (None, 512)          401920      input_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 512)          0           dense_249[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_250 (Dense)               (None, 256)          131328      dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 256)          0           dense_250[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_251 (Dense)               (None, 128)          32896       dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 128)          0           dense_251[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_252 (Dense)               (None, 64)           8256        dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Dense)                 (None, 32)           2080        dense_252[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_253 (Dense)               (None, 32)           1056        encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_254 (Dense)               (None, 64)           2112        dense_253[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_255 (Dense)               (None, 128)          8320        dense_254[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_256 (Dense)               (None, 256)          33024       dense_255[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_257 (Dense)               (None, 512)          131584      dense_256[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           363         encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          402192      dense_257[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,131\n",
            "Trainable params: 1,155,131\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 4s 361us/step - loss: 1.8708 - classifier_loss: 1.7749 - decoder_loss: 0.0947 - classifier_accuracy: 0.3909 - decoder_mse: 0.0947 - val_loss: 0.9486 - val_classifier_loss: 0.8816 - val_decoder_loss: 0.0706 - val_classifier_accuracy: 0.7241 - val_decoder_mse: 0.0705\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.9075 - classifier_loss: 0.8373 - decoder_loss: 0.0695 - classifier_accuracy: 0.7408 - decoder_mse: 0.0695 - val_loss: 0.5584 - val_classifier_loss: 0.4978 - val_decoder_loss: 0.0628 - val_classifier_accuracy: 0.8402 - val_decoder_mse: 0.0628\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.6007 - classifier_loss: 0.5354 - decoder_loss: 0.0652 - classifier_accuracy: 0.8303 - decoder_mse: 0.0652 - val_loss: 0.4512 - val_classifier_loss: 0.3930 - val_decoder_loss: 0.0613 - val_classifier_accuracy: 0.8696 - val_decoder_mse: 0.0613\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.4789 - classifier_loss: 0.4145 - decoder_loss: 0.0645 - classifier_accuracy: 0.8767 - decoder_mse: 0.0645 - val_loss: 0.3801 - val_classifier_loss: 0.3229 - val_decoder_loss: 0.0605 - val_classifier_accuracy: 0.8964 - val_decoder_mse: 0.0604\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 125us/step - loss: 0.3974 - classifier_loss: 0.3336 - decoder_loss: 0.0636 - classifier_accuracy: 0.8991 - decoder_mse: 0.0636 - val_loss: 0.3422 - val_classifier_loss: 0.2855 - val_decoder_loss: 0.0597 - val_classifier_accuracy: 0.9170 - val_decoder_mse: 0.0597\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.3467 - classifier_loss: 0.2837 - decoder_loss: 0.0629 - classifier_accuracy: 0.9147 - decoder_mse: 0.0629 - val_loss: 0.3366 - val_classifier_loss: 0.2804 - val_decoder_loss: 0.0592 - val_classifier_accuracy: 0.9170 - val_decoder_mse: 0.0592\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.3084 - classifier_loss: 0.2461 - decoder_loss: 0.0625 - classifier_accuracy: 0.9237 - decoder_mse: 0.0625 - val_loss: 0.3141 - val_classifier_loss: 0.2580 - val_decoder_loss: 0.0585 - val_classifier_accuracy: 0.9232 - val_decoder_mse: 0.0585\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.2918 - classifier_loss: 0.2295 - decoder_loss: 0.0622 - classifier_accuracy: 0.9287 - decoder_mse: 0.0622 - val_loss: 0.3150 - val_classifier_loss: 0.2595 - val_decoder_loss: 0.0576 - val_classifier_accuracy: 0.9241 - val_decoder_mse: 0.0575\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.2715 - classifier_loss: 0.2096 - decoder_loss: 0.0619 - classifier_accuracy: 0.9375 - decoder_mse: 0.0619 - val_loss: 0.2979 - val_classifier_loss: 0.2451 - val_decoder_loss: 0.0572 - val_classifier_accuracy: 0.9223 - val_decoder_mse: 0.0572\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 124us/step - loss: 0.2461 - classifier_loss: 0.1848 - decoder_loss: 0.0615 - classifier_accuracy: 0.9448 - decoder_mse: 0.0615 - val_loss: 0.3091 - val_classifier_loss: 0.2557 - val_decoder_loss: 0.0571 - val_classifier_accuracy: 0.9241 - val_decoder_mse: 0.0570\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 121us/step - loss: 0.2359 - classifier_loss: 0.1748 - decoder_loss: 0.0610 - classifier_accuracy: 0.9449 - decoder_mse: 0.0609 - val_loss: 0.3013 - val_classifier_loss: 0.2484 - val_decoder_loss: 0.0571 - val_classifier_accuracy: 0.9250 - val_decoder_mse: 0.0570\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.2119 - classifier_loss: 0.1507 - decoder_loss: 0.0611 - classifier_accuracy: 0.9534 - decoder_mse: 0.0611 - val_loss: 0.3029 - val_classifier_loss: 0.2517 - val_decoder_loss: 0.0560 - val_classifier_accuracy: 0.9348 - val_decoder_mse: 0.0559\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 124us/step - loss: 0.1974 - classifier_loss: 0.1361 - decoder_loss: 0.0610 - classifier_accuracy: 0.9582 - decoder_mse: 0.0610 - val_loss: 0.2769 - val_classifier_loss: 0.2230 - val_decoder_loss: 0.0559 - val_classifier_accuracy: 0.9348 - val_decoder_mse: 0.0558\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.2063 - classifier_loss: 0.1456 - decoder_loss: 0.0607 - classifier_accuracy: 0.9541 - decoder_mse: 0.0607 - val_loss: 0.3058 - val_classifier_loss: 0.2515 - val_decoder_loss: 0.0571 - val_classifier_accuracy: 0.9286 - val_decoder_mse: 0.0570\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.1894 - classifier_loss: 0.1289 - decoder_loss: 0.0605 - classifier_accuracy: 0.9605 - decoder_mse: 0.0605 - val_loss: 0.2811 - val_classifier_loss: 0.2271 - val_decoder_loss: 0.0555 - val_classifier_accuracy: 0.9330 - val_decoder_mse: 0.0555\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.1869 - classifier_loss: 0.1265 - decoder_loss: 0.0604 - classifier_accuracy: 0.9622 - decoder_mse: 0.0603 - val_loss: 0.2880 - val_classifier_loss: 0.2371 - val_decoder_loss: 0.0555 - val_classifier_accuracy: 0.9330 - val_decoder_mse: 0.0554\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.1734 - classifier_loss: 0.1134 - decoder_loss: 0.0601 - classifier_accuracy: 0.9652 - decoder_mse: 0.0601 - val_loss: 0.3003 - val_classifier_loss: 0.2483 - val_decoder_loss: 0.0547 - val_classifier_accuracy: 0.9304 - val_decoder_mse: 0.0546\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.1671 - classifier_loss: 0.1073 - decoder_loss: 0.0600 - classifier_accuracy: 0.9666 - decoder_mse: 0.0600 - val_loss: 0.2897 - val_classifier_loss: 0.2381 - val_decoder_loss: 0.0545 - val_classifier_accuracy: 0.9348 - val_decoder_mse: 0.0545\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.1640 - classifier_loss: 0.1052 - decoder_loss: 0.0596 - classifier_accuracy: 0.9634 - decoder_mse: 0.0596 - val_loss: 0.2927 - val_classifier_loss: 0.2404 - val_decoder_loss: 0.0541 - val_classifier_accuracy: 0.9304 - val_decoder_mse: 0.0540\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.1595 - classifier_loss: 0.0998 - decoder_loss: 0.0596 - classifier_accuracy: 0.9689 - decoder_mse: 0.0596 - val_loss: 0.2788 - val_classifier_loss: 0.2277 - val_decoder_loss: 0.0548 - val_classifier_accuracy: 0.9339 - val_decoder_mse: 0.0548\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 120us/step - loss: 0.1581 - classifier_loss: 0.0991 - decoder_loss: 0.0593 - classifier_accuracy: 0.9699 - decoder_mse: 0.0593 - val_loss: 0.2981 - val_classifier_loss: 0.2461 - val_decoder_loss: 0.0546 - val_classifier_accuracy: 0.9348 - val_decoder_mse: 0.0546\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 124us/step - loss: 0.1456 - classifier_loss: 0.0865 - decoder_loss: 0.0593 - classifier_accuracy: 0.9735 - decoder_mse: 0.0593 - val_loss: 0.3035 - val_classifier_loss: 0.2535 - val_decoder_loss: 0.0539 - val_classifier_accuracy: 0.9357 - val_decoder_mse: 0.0539\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 124us/step - loss: 0.1425 - classifier_loss: 0.0832 - decoder_loss: 0.0592 - classifier_accuracy: 0.9737 - decoder_mse: 0.0591 - val_loss: 0.2927 - val_classifier_loss: 0.2420 - val_decoder_loss: 0.0539 - val_classifier_accuracy: 0.9366 - val_decoder_mse: 0.0539\n",
            "Performance on the validation set\n",
            "loss: 0.32\n",
            "classifier_loss: 0.27\n",
            "decoder_loss: 0.06\n",
            "classifier_accuracy: 0.93\n",
            "decoder_mse: 0.06\n",
            "\n",
            "FOLD:  10\n",
            "Model: \"model_56\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_28 (InputLayer)           (None, 784)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_258 (Dense)               (None, 512)          401920      input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 512)          0           dense_258[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_259 (Dense)               (None, 256)          131328      dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 256)          0           dense_259[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_260 (Dense)               (None, 128)          32896       dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 128)          0           dense_260[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_261 (Dense)               (None, 64)           8256        dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Dense)                 (None, 32)           2080        dense_261[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_262 (Dense)               (None, 32)           1056        encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_263 (Dense)               (None, 64)           2112        dense_262[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_264 (Dense)               (None, 128)          8320        dense_263[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_265 (Dense)               (None, 256)          33024       dense_264[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_266 (Dense)               (None, 512)          131584      dense_265[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "classifier (Dense)              (None, 11)           363         encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Dense)                 (None, 784)          402192      dense_266[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,155,131\n",
            "Trainable params: 1,155,131\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 10080 samples, validate on 1120 samples\n",
            "Epoch 1/300\n",
            "10080/10080 [==============================] - 4s 391us/step - loss: 1.7376 - classifier_loss: 1.6398 - decoder_loss: 0.0963 - classifier_accuracy: 0.4379 - decoder_mse: 0.0963 - val_loss: 0.8722 - val_classifier_loss: 0.7970 - val_decoder_loss: 0.0711 - val_classifier_accuracy: 0.7589 - val_decoder_mse: 0.0712\n",
            "Epoch 2/300\n",
            "10080/10080 [==============================] - 1s 124us/step - loss: 0.8592 - classifier_loss: 0.7897 - decoder_loss: 0.0686 - classifier_accuracy: 0.7485 - decoder_mse: 0.0686 - val_loss: 0.5336 - val_classifier_loss: 0.4674 - val_decoder_loss: 0.0635 - val_classifier_accuracy: 0.8518 - val_decoder_mse: 0.0636\n",
            "Epoch 3/300\n",
            "10080/10080 [==============================] - 1s 125us/step - loss: 0.6181 - classifier_loss: 0.5526 - decoder_loss: 0.0651 - classifier_accuracy: 0.8310 - decoder_mse: 0.0651 - val_loss: 0.4489 - val_classifier_loss: 0.3843 - val_decoder_loss: 0.0622 - val_classifier_accuracy: 0.8821 - val_decoder_mse: 0.0622\n",
            "Epoch 4/300\n",
            "10080/10080 [==============================] - 1s 126us/step - loss: 0.4953 - classifier_loss: 0.4315 - decoder_loss: 0.0640 - classifier_accuracy: 0.8704 - decoder_mse: 0.0640 - val_loss: 0.3540 - val_classifier_loss: 0.2925 - val_decoder_loss: 0.0596 - val_classifier_accuracy: 0.9098 - val_decoder_mse: 0.0597\n",
            "Epoch 5/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.4103 - classifier_loss: 0.3473 - decoder_loss: 0.0629 - classifier_accuracy: 0.8949 - decoder_mse: 0.0629 - val_loss: 0.3104 - val_classifier_loss: 0.2489 - val_decoder_loss: 0.0594 - val_classifier_accuracy: 0.9295 - val_decoder_mse: 0.0594\n",
            "Epoch 6/300\n",
            "10080/10080 [==============================] - 1s 124us/step - loss: 0.3702 - classifier_loss: 0.3076 - decoder_loss: 0.0625 - classifier_accuracy: 0.9045 - decoder_mse: 0.0625 - val_loss: 0.3121 - val_classifier_loss: 0.2520 - val_decoder_loss: 0.0587 - val_classifier_accuracy: 0.9366 - val_decoder_mse: 0.0588\n",
            "Epoch 7/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.3221 - classifier_loss: 0.2603 - decoder_loss: 0.0620 - classifier_accuracy: 0.9181 - decoder_mse: 0.0620 - val_loss: 0.3115 - val_classifier_loss: 0.2518 - val_decoder_loss: 0.0574 - val_classifier_accuracy: 0.9277 - val_decoder_mse: 0.0574\n",
            "Epoch 8/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.2928 - classifier_loss: 0.2313 - decoder_loss: 0.0615 - classifier_accuracy: 0.9286 - decoder_mse: 0.0615 - val_loss: 0.3097 - val_classifier_loss: 0.2497 - val_decoder_loss: 0.0569 - val_classifier_accuracy: 0.9295 - val_decoder_mse: 0.0570\n",
            "Epoch 9/300\n",
            "10080/10080 [==============================] - 1s 125us/step - loss: 0.2629 - classifier_loss: 0.2015 - decoder_loss: 0.0614 - classifier_accuracy: 0.9382 - decoder_mse: 0.0614 - val_loss: 0.3239 - val_classifier_loss: 0.2642 - val_decoder_loss: 0.0568 - val_classifier_accuracy: 0.9339 - val_decoder_mse: 0.0569\n",
            "Epoch 10/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.2399 - classifier_loss: 0.1792 - decoder_loss: 0.0607 - classifier_accuracy: 0.9459 - decoder_mse: 0.0607 - val_loss: 0.3034 - val_classifier_loss: 0.2447 - val_decoder_loss: 0.0563 - val_classifier_accuracy: 0.9411 - val_decoder_mse: 0.0564\n",
            "Epoch 11/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.2392 - classifier_loss: 0.1786 - decoder_loss: 0.0604 - classifier_accuracy: 0.9438 - decoder_mse: 0.0604 - val_loss: 0.2913 - val_classifier_loss: 0.2327 - val_decoder_loss: 0.0570 - val_classifier_accuracy: 0.9420 - val_decoder_mse: 0.0570\n",
            "Epoch 12/300\n",
            "10080/10080 [==============================] - 1s 126us/step - loss: 0.2277 - classifier_loss: 0.1673 - decoder_loss: 0.0603 - classifier_accuracy: 0.9502 - decoder_mse: 0.0603 - val_loss: 0.3062 - val_classifier_loss: 0.2503 - val_decoder_loss: 0.0563 - val_classifier_accuracy: 0.9429 - val_decoder_mse: 0.0564\n",
            "Epoch 13/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.2202 - classifier_loss: 0.1603 - decoder_loss: 0.0599 - classifier_accuracy: 0.9492 - decoder_mse: 0.0599 - val_loss: 0.2982 - val_classifier_loss: 0.2400 - val_decoder_loss: 0.0559 - val_classifier_accuracy: 0.9438 - val_decoder_mse: 0.0560\n",
            "Epoch 14/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.1920 - classifier_loss: 0.1319 - decoder_loss: 0.0599 - classifier_accuracy: 0.9583 - decoder_mse: 0.0599 - val_loss: 0.2775 - val_classifier_loss: 0.2208 - val_decoder_loss: 0.0554 - val_classifier_accuracy: 0.9518 - val_decoder_mse: 0.0555\n",
            "Epoch 15/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.1870 - classifier_loss: 0.1275 - decoder_loss: 0.0593 - classifier_accuracy: 0.9592 - decoder_mse: 0.0593 - val_loss: 0.2903 - val_classifier_loss: 0.2337 - val_decoder_loss: 0.0557 - val_classifier_accuracy: 0.9464 - val_decoder_mse: 0.0557\n",
            "Epoch 16/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.1861 - classifier_loss: 0.1266 - decoder_loss: 0.0594 - classifier_accuracy: 0.9608 - decoder_mse: 0.0593 - val_loss: 0.2855 - val_classifier_loss: 0.2284 - val_decoder_loss: 0.0550 - val_classifier_accuracy: 0.9473 - val_decoder_mse: 0.0551\n",
            "Epoch 17/300\n",
            "10080/10080 [==============================] - 1s 124us/step - loss: 0.1788 - classifier_loss: 0.1195 - decoder_loss: 0.0591 - classifier_accuracy: 0.9616 - decoder_mse: 0.0591 - val_loss: 0.3127 - val_classifier_loss: 0.2568 - val_decoder_loss: 0.0546 - val_classifier_accuracy: 0.9455 - val_decoder_mse: 0.0546\n",
            "Epoch 18/300\n",
            "10080/10080 [==============================] - 1s 123us/step - loss: 0.1730 - classifier_loss: 0.1139 - decoder_loss: 0.0590 - classifier_accuracy: 0.9637 - decoder_mse: 0.0590 - val_loss: 0.3288 - val_classifier_loss: 0.2727 - val_decoder_loss: 0.0548 - val_classifier_accuracy: 0.9429 - val_decoder_mse: 0.0549\n",
            "Epoch 19/300\n",
            "10080/10080 [==============================] - 1s 124us/step - loss: 0.1742 - classifier_loss: 0.1154 - decoder_loss: 0.0588 - classifier_accuracy: 0.9620 - decoder_mse: 0.0588 - val_loss: 0.2866 - val_classifier_loss: 0.2309 - val_decoder_loss: 0.0543 - val_classifier_accuracy: 0.9473 - val_decoder_mse: 0.0543\n",
            "Epoch 20/300\n",
            "10080/10080 [==============================] - 1s 126us/step - loss: 0.1621 - classifier_loss: 0.1035 - decoder_loss: 0.0587 - classifier_accuracy: 0.9681 - decoder_mse: 0.0587 - val_loss: 0.3029 - val_classifier_loss: 0.2486 - val_decoder_loss: 0.0540 - val_classifier_accuracy: 0.9455 - val_decoder_mse: 0.0541\n",
            "Epoch 21/300\n",
            "10080/10080 [==============================] - 1s 125us/step - loss: 0.1534 - classifier_loss: 0.0955 - decoder_loss: 0.0580 - classifier_accuracy: 0.9691 - decoder_mse: 0.0580 - val_loss: 0.3321 - val_classifier_loss: 0.2765 - val_decoder_loss: 0.0540 - val_classifier_accuracy: 0.9446 - val_decoder_mse: 0.0540\n",
            "Epoch 22/300\n",
            "10080/10080 [==============================] - 1s 124us/step - loss: 0.1476 - classifier_loss: 0.0892 - decoder_loss: 0.0581 - classifier_accuracy: 0.9692 - decoder_mse: 0.0581 - val_loss: 0.3038 - val_classifier_loss: 0.2494 - val_decoder_loss: 0.0530 - val_classifier_accuracy: 0.9473 - val_decoder_mse: 0.0531\n",
            "Epoch 23/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.1392 - classifier_loss: 0.0816 - decoder_loss: 0.0578 - classifier_accuracy: 0.9725 - decoder_mse: 0.0578 - val_loss: 0.3214 - val_classifier_loss: 0.2662 - val_decoder_loss: 0.0533 - val_classifier_accuracy: 0.9446 - val_decoder_mse: 0.0534\n",
            "Epoch 24/300\n",
            "10080/10080 [==============================] - 1s 122us/step - loss: 0.1381 - classifier_loss: 0.0803 - decoder_loss: 0.0578 - classifier_accuracy: 0.9745 - decoder_mse: 0.0578 - val_loss: 0.3311 - val_classifier_loss: 0.2778 - val_decoder_loss: 0.0534 - val_classifier_accuracy: 0.9473 - val_decoder_mse: 0.0535\n",
            "Performance on the validation set\n",
            "loss: 0.32\n",
            "classifier_loss: 0.26\n",
            "decoder_loss: 0.05\n",
            "classifier_accuracy: 0.94\n",
            "decoder_mse: 0.05\n",
            "\n",
            "Overall  loss :  0.32 +- 0.01\n",
            "Overall  classifier_loss :  0.26 +- 0.01\n",
            "Overall  decoder_loss :  0.06 +- 0.00\n",
            "Overall  classifier_accuracy :  0.93 +- 0.00\n",
            "Overall  decoder_mse :  0.06 +- 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEPuhED3n88F",
        "colab_type": "text"
      },
      "source": [
        "## Plot decoder reconstructed images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL6SfF95oLXm",
        "colab_type": "code",
        "outputId": "63903d10-e3c6-47c3-d9de-182d21acb77b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "_, decoded_imgs = autoencoder_nn.predict(x_test)\n",
        "\n",
        "n=10\n",
        "plt.figure(figsize=(40, 4))\n",
        "for i in range(n):\n",
        "    # display original images\n",
        "    ax = plt.subplot(3, 20, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "    # display reconstructed images\n",
        "    ax = plt.subplot(3, 20, 2*20 +i+ 1)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "  \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABEYAAADrCAYAAABkdZM2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzTISAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 2880x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzcSXKmYoNCl",
        "colab_type": "text"
      },
      "source": [
        "## Plot performances plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlkzqJw-d2FL",
        "colab_type": "code",
        "outputId": "ae6da20d-eafd-4090-e41d-793c110e180f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "x_plot = list(range(1, len(best_history.history['val_classifier_accuracy']) + 1))\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Decoder loss')\n",
        "    plt.plot(x_plot, history.history['decoder_loss'])\n",
        "    plt.plot(x_plot, history.history['val_decoder_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Classifier loss')\n",
        "    plt.plot(x_plot, history.history['classifier_loss'])\n",
        "    plt.plot(x_plot, history.history['val_classifier_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Decoder MSE')\n",
        "    plt.plot(x_plot, history.history['decoder_mse'])\n",
        "    plt.plot(x_plot, history.history['val_decoder_mse'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Classifier accuracy')\n",
        "    plt.plot(x_plot, history.history['classifier_accuracy'])\n",
        "    plt.plot(x_plot, history.history['val_classifier_accuracy'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(best_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAEYCAYAAABBfQDEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE+b/A/fpCXssFGzDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAEYCAYAAABBfQDEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeXzU5bX48c/JJDNZJhuTsIZNdhAQ\niKjFDakKXpVbiwt1qbTW1mq9te1tuf1ZW723t7b19mqrtW64VaVWr5UqSGvF3SpBZQdBDBDWJITs\n2yTn98f3mxBilgnMkuW8X695zXd9cib6+nLyzHmeR1QVY4wxxhhjjCMu1gEYY4wxxhjTnViCbIwx\nxhhjTAuWIBtjjDHGGNOCJcjGGGOMMca0YAmyMcYYY4wxLViCbIwxxhhjTAuWIBtjTB8kIktE5KCI\nbGjnvIjIb0Vku4isE5Hp0Y7RGGNixRJkY4zpmx4D5nZwfh4wxn1dD9wfhZiMMaZbsATZGGP6IFV9\nEzjUwSXzgSfU8U8gQ0QGRSc6Y4yJrfhYBxANWVlZOmLEiFiHYYwxnVqzZk2RqmbHOg5gCLC7xX6B\ne2xf6wtF5HqcXmZSUlJmjB8/PioBGmPM8WrvmdsnEuQRI0aQl5cX6zCMMaZTIrIz1jF0lao+CDwI\nkJubq/a8Ncb0FO09c63EwhhjTFv2AENb7Oe4x4wxptezBNkYY0xblgHXuLNZnAqUqurnyiuMMaY3\n6hMlFsYYY44mIs8AZwNZIlIA/BRIAFDVPwDLgQuA7UAVsCg2kRpjTPRZgmyM6VR9fT0FBQXU1NTE\nOpReIzExkZycHBISEmLy81V1YSfnFbgxSuEYY0y3YgmyMaZTBQUFpKamMmLECEQk1uH0eKpKcXEx\nBQUFjBw5MtbhGGOMacVqkI0xnaqpqSEQCFhyHCYiQiAQsB55Y4zppixBNsaExJLj8LLfpzHGdF9W\nYtGGrfvLefOTQq4+bTiJCZ5Yh2OMMcYY06upKiVV9Rwoq2F/WQ0Hy2qormsgIT6OBE8cXo/znuAR\nEuKP7Md7BK8njvSkBIb2Sw5bPJYgt2FdwWF+vnwz508ayLBA+H7ZxphjU1xczJw5cwDYv38/Ho+H\n7Gxn4aMPPvgAr9fbaRuLFi1i8eLFjBs3rt1r7rvvPjIyMrjyyivDE7gxxhgAqusa2HqgnJ3Flewv\nreFAWS0HympaJMS11DU0HnP7504cwEPX5IYtXkuQ25Dl9wFQVFlrCbIx3UAgEODjjz8G4Gc/+xl+\nv58f/OAHR12jqqgqcXFtV449+uijnf6cG2+0SRuMMb2HqlJUUce2A+VsPVDO4ap6JgxKY0pOOoPS\nEyNS6qWqHCirZfO+MjbtK2t+zy+qpFGPXJfs9TAwLZEBaYnkDs9kQHoiA1ITGZjuHBuQ5iPZG0+w\noZG6hkbqG5T6hkbqgo3Ut9xvaKQ+2EjAzd3CxRLkNgT8Tm9UcUVdjCMxpvu5/a8b2bS3LKxtThyc\nxk8vmtTl+7Zv387FF1/MtGnT+Oijj/j73//O7bffzocffkh1dTWXX345t912GwCnn3469957Lyee\neCJZWVl861vfYsWKFSQnJ/Piiy/Sv39/br31VrKysvjud7/L6aefzumnn85rr71GaWkpjz76KF/4\nwheorKzkmmuuYfPmzUycOJH8/HwefvhhTjrppLD+TowxpitKKuv45EC5+6po3i6pqm/z+iy/jyk5\n6Uweks6UnHSm5GSQnRpakqmqVNQGKaqoo6iill3FVWzeV8bm/WVs3lfOocoj+VNOZhITB6Vx0ZTB\nTBiUxuj+KQxISyQ1MTZTXIbKEuQ2NP0VUlxRG+NIjDGd2bJlC0888QS5uc5Xa3feeSf9+vUjGAwy\ne/ZsFixYwMSJE4+6p7S0lLPOOos777yT733veyxZsoTFixd/rm1V5YMPPmDZsmXccccdvPLKK/zu\nd79j4MCBPP/886xdu5bp06dH5XMaY3qPmvoGiivrKK6odd/rOFRZS3FFHUUVdZRW1wPaaTsAlbUN\nbDtYQVGLnCXVF8+YAX7OnzSQsQNS3ZeftKQENu0rY31BKesKSlm/5zCrth5E3R81KD2xOWEekpnE\nocp6iipqKa6obU6GiyvqKKyopS54dDmELz6OcQNTOXfCACYOTmPCoDTGD0olrZsnwu2xBLkNgRS3\nB7nSepCNae1YenojadSoUc3JMcAzzzzDI488QjAYZO/evWzatOlzCXJSUhLz5s0DYMaMGbz11ltt\ntn3JJZc0X5Ofnw/A22+/zY9+9CMApk6dyqRJ3ev3YYyJjaZeVaeuttapsy2v4UCpW2Nb7iSXxRW1\nVNY1tNmGNz6OrBQvaUkJeOJCK3/wxscxe1w2YwekMmaAn7EDUjssn5g+LJPpwzKb9ytrg2zcW8a6\ngsOs31PK+oJS/rbpQPP5+Dgh4PeS5fcR8PsY3d9Plt9HVotjQzISGRFIId7TeyZHswS5DYkJHlJ9\n8RSWWw+yMd1dSkpK8/a2bdu45557+OCDD8jIyOCqq65qc67hloP6PB4PwWCwzbZ9Pl+n1xhjeh9V\npbKugdLqekqr6imrqXe2q+spc1+l1fUcrnZmXThYVsv+shqq2kh8UxPjm2tqhw3LoF+Km1imeOmX\n4iXgJpv9Urz4ffFRnwIyxRfPzJH9mDmyX/Oxspp6DpbVkuX3kpaYQFyIyXpvYglyOwJ+r/UgG9PD\nlJWVkZqaSlpaGvv27WPlypXMnTs3rD9j1qxZPPvss5xxxhmsX7+eTZs2hbV9Y0x4BBsa2Xmoim0H\nKvi0sIKy6nqq6xuoqW+gur6R6rqm7RbvdQ1U1TdQXhOkobH9EgcRp4whI9lL/1QfEwalcfa4/gxI\n8zEwPZH+zYPNnIFmPU1aYkKPLY0Il573Xy1KAn6f1SAb08NMnz6diRMnMn78eIYPH86sWbPC/jO+\n853vcM011zBx4sTmV3p6eth/jjEmNPUNjeQXVbLtYAXbDlSw7WA52w5U8FlR5VHThvni40jyekhK\n8JDovpISnGOZyQnuvockr4fUxHjSkxJIT3ISxfSkBNKSjryn+uL7ZK9qXyKqoRWB92S5ubmal5fX\npXuufyKPncVVrLzlzAhFZUzPsXnzZiZMmBDrMLqFYDBIMBgkMTGRbdu2cd5557Ft2zbi47ve39DW\n71VE1qhq+CbzjLJjed4aE4q6YCOfFVWy7aAzS8P2Folw0O3tFYGhmcmM6e9n9AA/Y/s7dbmjsv2k\n+KxP0Hxee8/ciP7fIiJzgXsAD/Cwqt7Z6rwPeAKYARQDl6tqvoh4gQeAXKAR+DdVfd29ZwbwGJAE\nLHfPhT3LD/h9fLirJNzNGmN6uIqKCubMmUMwGERVeeCBB44pOTbGtK022MCOQqdHeLs7Zdm2g+Xk\nF1c1lz2IwLB+yYzpn8q5EwcwZoCfMf1TGZXtJ8lrK+Ca4xexp7qIeID7gHOBAmC1iCxT1ZYFe18H\nSlR1tIhcAfwSuBz4BoCqThaR/sAKETlZVRuB+93z7+MkyHOBFeGOP9vv5VBlHQ2NGvJIUmNM75eR\nkcGaNWtiHYYxPZqqcqiyjk8LK9lRWMGOIuf908JKdhYfWVAiTmBEIIUxA/zMO3FQcyJ8QnYKiQmW\nCJvIiWS3x0xgu6ruABCRpcB8oGWCPB/4mbv9HHCvOMM3JwKvAajqQRE5DOSKyG4gTVX/6bb5BPCv\nRCBBDvh9NCqUVNU1r6xnjDHGmNA1Nir5xZV8cqCCHUUV7Cis5NNC592Z69fhjY9jZCCF8QNTuWjK\nIEa78/aOzErBF2+JsIm+SCbIQ4DdLfYLgFPau0ZVgyJSCgSAtcDFIvIMMBSnBGMoTrlFQas2h0Qi\n+Jar6VmCbIwxprfbVVzF/rIaBmckMjAtsctz2qoqBSXVrCsoZd2ew6zbXcqGPaWU1x6ZIrF/qo9R\n2X4unDKIE7L9jMpOYVS2n8EZSXgEKNoGqQMg0Qa+9kqVxdBQB2mDYh1Jp7pr4dwSYAKQB+wE3gXa\nnlW7HSJyPXA9wLBhw7ocQCCl5Wp6qV2+3xhjjOnuauobWLFhH0s/2M37nx1qPh4nMDAtkSGZSQzO\nSGJIRlLzdk6G815WU+8kwwWH3VXZSjnsLmvs9cQxYVAq86cNZsqQDMYNdMoiPre8cEUh7HgZ3vgH\nfPoaVBwAXxrM/Aac+m1IyQrTBy2DhCTw9O2py2Lm4GZ4715Y9yw01MOEC+G078Cw1v2m3UckE+Q9\nOL2+TXLcY21dUyAi8UA6UOwOurul6SIReRf4BChx2+moTQBU9UHgQXBGVXc1+Cy3B7nI5kI2xhjT\ny2zYU8qfVu/mLx/vobwmyPBAMv9+/jgmDU5jX2kNew9Xs6ekmj2Hq1mzs4SX1+1rnimiNU+cMG5A\nKnMnDWRyTjpTczIYOyAVb3wbPdDBOij4ALb/Az79B+xb6xxP6gejZsOIM2DH6/DWb+Cf90Pu1+AL\n34HUgV3/kMFa2LoCPvqj87MA0odCv5HQ7wTIHNliewR4U9pupyHoJO7l+5xX2b4j21XF4B/w+Tat\nBxxU4bM34N17YfvfIT4Jpl3t/G7ylsDmv0LOyXDaTTDhIojrXqU0kUyQVwNjRGQkThJ7BfCVVtcs\nA74KvAcsAF5TVRWRZJwp6CpF5Fwg2DS4T0TKRORUnEF61wC/i0TwTWUVRbaanjExN3v2bBYvXsz5\n55/ffOzuu+9m69at3H///W3e4/f7qaioYO/evdx8880899xzn7vm7LPP5q677jpqqerW7r77bq6/\n/nqSk5MBuOCCC3j66afJyMg4zk9lTHSVVtXz4to9LP1gN5v2leGLj2PeiQO5/ORhnDKyX/vz+tZV\n0VC2j8MH8ik9uJuqot0ES/eRKEH6paXSLyOdeF+SkwDFJcLBJChJdPbjfRCfCAc3OUlx/ltQVwFx\n8ZAzE865FUbNgUEnQZybUOcugsKtR5LkDx6C6VfDrH+DjBC+Ed63zkmK1z8L1SWQNsS5Ny4BDu2A\nks9g4wvOuZb8A53kNn0o1JYfSYIrDgKt/jiIi4fUQZCUCXs/hsqDR59PDrjJ8glHkuf0oeDxEpK4\nOCfx9g8IT693XSWU73c+V+pASMmOXEIarIMNz8N798GB9ZDSH2bf6vyxkxJwrjnzB/Dx006v8p+/\nChnDnW8Mpl0FPn9k4uqiiCXIbk3xTcBKnGnelqjqRhG5A8hT1WXAI8CTIrIdOISTRAP0B1aKSCNO\ncn11i6a/zZFp3lYQgQF6AOnuOujFlZYgGxNrCxcuZOnSpUclyEuXLuVXv/pVp/cOHjy4zeQ4VHff\nfTdXXXVVc4K8fPnyY27LmHYd2AS73oXAGOg/EfzZYWm2sVH552fFPLt6Nys27Kc22MikwWncMX8S\n86cOIT05AWorYO+HcHAjlOQf3UNavg9qSvHgDBAKNDWckOwkvvtroL6azyWQbckcAVMuh1HnwMgz\nITGt/Wuzx8ElD8DZP4K374Y1j8Oax2DqFXD69yAw6ujrqw45X99//EfYvx48Phj/LzDtSjhhdtvJ\nYPVhJ1k+tAMOfeZufwa7/unEljoIBk1x3pteae57ctaRhB6c32HT/U1JeFNbG54Dbfz8zw+JOMls\n2qC240gd5MRaUQjle50kuMx9L9/r/rfcD7WlrZr1OMl36kBIG+y8pw5qsT0Y/P2d3t5QE+nqEue/\n0fsPOP/fZI+Hi++FyZdCQuLR13pTnDKa3K/BlpedRPmVH8Hr/+0cm/nNmNcpR7QGWVWX40zF1vLY\nbS22a4BL27gvHxjXTpt5wIlhDbQNcXFCvxQvxRVWYmHMUVYsdv4BCqeBk2Hene2eXrBgAbfeeit1\ndXV4vV7y8/PZu3cv06ZNY86cOZSUlFBfX89//dd/MX/+/KPuzc/P58ILL2TDhg1UV1ezaNEi1q5d\ny/jx46murm6+7oYbbmD16tVUV1ezYMECbr/9dn7729+yd+9eZs+eTVZWFqtWrWLEiBHk5eWRlZXF\nb37zG5YsWQLAddddx3e/+13y8/OZN28ep59+Ou+++y5DhgzhxRdfJCkpKby/M9M7lO2DVT+Hj586\nOolKzoIBE51kuf8E6D8J+o8HX+djYmqDDbz3aTGvbj7Aq5sOsr+shtTEeBZOH8iVY+oZwy44uApe\n3AwHNsLhnUdubkqc0gZBYLRT8tBWcuZLcyYjBuer9IY6J1EO1jiv+hoIVh95zxj++aQ2FP1OgIt/\nC2f9EN75LXz4uNPzeOKXYdZ3nUTsoyedUoqGOqcn+oK7nPPJ/TpuOykDkqbB4Gldj6s1n995jg2c\n/PlzwVo4vAtKd0NjiIlyY72b5O4/kviW7oGCPKgq6vjeuHinNzxtEGSPhRPOPpIIe1OccpGWfwQV\nfwr5b0PN4TYaEydJTsp0f1+ZziuxxXZShvNvwodPQn0ljDzLSYxHzzny/0i7sXpg4sXOa/dqeO93\n8M49TlnG5AUw7gLnj7EE9xuJhETnD7OEJOe96RXXtQGloeiug/S6hUCKlyJLkI2JuX79+jFz5kxW\nrFjB/PnzWbp0KZdddhlJSUm88MILpKWlUVRUxKmnnsrFF1+MtPNQvv/++0lOTmbz5s2sW7eO6dOn\nN5/7+c9/Tr9+/WhoaGDOnDmsW7eOm2++md/85jesWrWKrKyjBwutWbOGRx99lPfffx9V5ZRTTuGs\ns84iMzOTbdu28cwzz/DQQw9x2WWX8fzzz3PVVVdF9HdkepjaCnj3t/Du76Ax6Hy9nPs1J5E6uNkp\nSTi46UjS0SR9mJM4pw8FOZIU1AYbKSipYvehKvYcrqauQRnvEc7JSGJsdi2D63YQt2E7rHNnlBAP\nZI2BIdOdutD+E5xX5oiuf/Uu4pZSRHDGp/QcuOBXcMb3nd7G1Y/A+j8755IDcPJ1cNKVMDDi/Wdd\nF+9zftdZY8LTXrD26CS3pvTo3uDWvduhqqtyk+b9bmnJAaeXveaw0zvc9CrZ6bzXHD7yR11cPJy4\nAE670el1PxZDT4ahTzg97/+83ymTWftMaPd6fM7AvwVLju1nt8ES5A5k+X1WYmFMax309EZSU5lF\nU4L8yCOPoKr8+Mc/5s033yQuLo49e/Zw4MABBg5se0DPm2++yc033wzAlClTmDLlyIP82Wef5cEH\nHyQYDLJv3z42bdp01PnW3n77bb70pS+RkuIM7Lnkkkt46623uPjiixk5ciQnnXQSADNmzCA/Pz9M\nvwXT4zUEnR7PVf/t1K1OugTm3ObUqYLTyzpq9pHrGxuhdJdTgnFw05Hkedc/aVAINjRS39BIsFEJ\nAFkizIwXEnxxxMcJUgtImtMDPe6CIz3SWWMim9BGSuoAOO8/4fRbYO1SJ3EeOxfiQ6zt7Q3ifU4t\ndij12F3hTXb+/wu1p7+xEerKnWTZm3qkvvh49Rvp/DF0zv9zSn6CtUe+nejoW4qsseH5+S5LkDuQ\n5feyc1dl5xcaYyJu/vz53HLLLXz44YdUVVUxY8YMHnvsMQoLC1mzZg0JCQmMGDGCmpqaLrf92Wef\ncdddd7F69WoyMzO59tprj6mdJj7fkcTD4/EcVcph+ihV2PZ3+PtPoHALDDsNFj4DOe0PEAVoQNgv\nA9jjS6PAP4U99dUUUM3ausNs2V8OwNgBfr44YQDnThzA1JyM9gfb9SbJ/eC0b8c6ir4tLs4pv4jU\njB2J6TBoamTaDoElyB0I+H1Wg2xMN+H3+5k9ezZf+9rXWLhwIQClpaX079+fhIQEVq1axc6dOzts\n48wzz+Tpp5/mnHPOYcOGDaxbtw6AsrIyUlJSSE9P58CBA6xYsYKzzz4bgNTUVMrLyz9XYnHGGWdw\n7bXXsnjxYlSVF154gSeffDL8H9z0fPvWwt9uhc/edGpqL/8jjL/wqPrMg2U1vPFJIQUl1RSUVLPn\ncBUFJdXsL6353PRqWX4fY/r7ufVfJnDuxAEMD7QzPZkx5phZgtyBgN9LVV0DVXVBkr32qzIm1hYu\nXMiXvvQlli5dCsCVV17JRRddxOTJk8nNzWX8+PEd3n/DDTewaNEiJkyYwIQJE5gxYwYAU6dOZdq0\naYwfP56hQ4cya9as5nuuv/565s6dy+DBg1m1alXz8enTp3Pttdcyc+ZMwBmkN23aNCunMEcUbYe3\n7nJKAZIyYd6vYMaio8oB9pVW88AbO3j6g13UBRuRpgU6MpLIHZ7JkMwkhmQkk5OZ5G4nkZjQveaL\nNaY3EmdNjt4tNzdX8/Lyunzfs6t388Pn1/HWD2cztF9yBCIzpmfYvHkzEyZMiHUYvU5bv1cRWaOq\nHX/v3o0d6/O212gIwievwOqHYccqZ/DQqTfAGd876qvoPYeruf/17Ty7uoBGVb48PYdFp4/ghCx/\n2wtsGGMior1nrnWLdiDgrqZXXFlnCbIxxpj2VRx0piDLewzKCpzFKWbfCtOvcQaWuXYfquL3r2/n\nuTUFAFyaO5Qbzhpl/8YY081YgtwBW03PGGNMu1Rh13tOb/GmZc7ctSec7cz0MnYeeI78E/tZUSX3\nrdrOCx/twSPCwpnD+NZZoxicYfNjG9MdWYLcgSM9yJYgG6Oq7c4vbLquL5S39Vq15c6qbasfcVaf\n86UfWRWs1Vy32w9WcN+q7bz48R4SPHFcc9pwvnnmKAamJ7bTuDGmO7AEuQOBFLcH2WayMH1cYmIi\nxcXFBAIBS5LDQFUpLi4mMdGSpB6lZCe8/wdn8Y66cmfVtIt+66z45T16JonGRuXuVz/hd6u2kxjv\n4bozTuC6M0bSP9X+mxvTE1iC3IEkr4cUr8emejN9Xk5ODgUFBRQWFsY6lF4jMTGRnJycWIdhQrFn\njbP07aYXnanZJn0JZn7TmcO4jT8Ya+ob+MGf1/LSun18eXoOP75gPAF/D1yUw5g+zBLkTgRsNT1j\nSEhIYOTIkbEOw5joaWyET1Y4ifGud8GX5iyje8q3IH1Iu7cVVdRy/RN5fLjrMIvnjeebZ55g37oY\n0wNZgtyJLL/XepCNMb2SiMwF7gE8wMOqemer88OBJUA2cAi4SlULoh5oNNVVwdqn4b3fw6FPIX0Y\nnP8LmH41+FI7vHXbgXK+9vhqDpbVcv+V05k3eVCUgjbGhJslyJ0I+H3sPlQV6zCMMSasRMQD3Aec\nCxQAq0VkmapuanHZXcATqvq4iJwD/AK4OvrRRkHFQfjgIWdGiupDMHg6LFgCE+YfNRtFe97eVsQN\nT63BF+/hT988jZOGZkQhaGNMpFiC3Iksv5ePdh2OdRjGGBNuM4HtqroDQESWAvOBlgnyROB77vYq\n4C9RjTDS6qpg20pY/xxs+xs01MO4C+ALN8Gw09qsL27LMx/s4ta/bGB0tp9Hrs0lJ9PmNDamp7ME\nuROBFB+HKmtpbFTi4qyOzBjTawwBdrfYLwBOaXXNWuASnDKMLwGpIhJQ1eKWF4nI9cD1AMOGDYtY\nwGERrINPX4MNz8PW5VBXAf4BkPt1OPk6yBodclONjcovX9nCA2/u4Kyx2dz7lWmkJiZEMHhjTLRY\ngtyJgN9Lo8Lh6nr6pXhjHY4xxkTTD4B7ReRa4E1gD9DQ+iJVfRB4EJylpqMZYEgaGyD/bScp3rwM\nqksgMQNO/LLzGnE6xHm61GR1XQPf/dNHrNx4gKtOHcbPLppEvMeWiDamt4hoghzCABAf8AQwAygG\nLlfVfBFJAB4GprsxPqGqv3DvyQfKcR7SwbbWzw6npql5iitqLUE2xvQme4ChLfZz3GPNVHUvTg8y\nIuIHvqyqPafmrGANrH8WNr4AFQcgIQXG/4szb/EJsyH+2J7pB8tquO6JPNbvKeUnF07ka7NG2EwV\nxvQyEUuQQxwA8nWgRFVHi8gVwC+By4FLAZ+qThaRZGCTiDyjqvnufbNVtShSsbeU5a6mV1hRy5gB\nHY9gNsaYHmQ1MEZERuIkxlcAX2l5gYhkAYdUtRH4D5wZLbo/VXj1Z/DO3eDxwdjznJ7iMeeD9/jq\ngzfsKeX6J/I4XF3PQ1fn8sWJA8ITszGmW4lkD3IoA0DmAz9zt5/D+SpPAAVSRCQeSALqgLIIxtqu\nrOYeZJvqzRjTe6hqUERuAlbifMu3RFU3isgdQJ6qLgPOBn4hIopTYnFjzAIOVUM9LPsOrH0GZiyC\nc2+HxPSwNP2n1bv4yYsb6Zfs5dlvnsaJQ8LTrjGm+4lkghzKAJDma9yHdSkQwEmW5wP7gGTgFlU9\n5N6jwN/cB/YDbu1bxATcsoriClssxBjTu6jqcmB5q2O3tdh+Dud53DPUVsCfvwrbX4XZ/w/O/PeQ\nZ6LoSE19A7e9uIFn8wo4fXQW91xxkq2MZ0wv110H6c3EqTEeDGQCb4nIq25v9OmqukdE+gN/F5Et\nqvpm6wbCNao6I9lLnEBxpfUgG2NMt1VZBE9dCvs+hot+CzO+GpZmdxVX8a0/rmHTvjK+c85ovvvF\nsXhsRiNjer1IDrntdABIy2vccop0nMF6XwFeUdV6VT0IvAPkAqjqHvf9IPACTjL9Oar6oKrmqmpu\ndnb2MX8IT5zQL8VLkZVYGGNM93ToM3jkPDi4CS5/KmzJ8aubDnDh796ioKSKJdfm8v3zxllybEwf\nEckEuXkAiIh4cQaALGt1zTKg6Um2AHhNVRXYBZwDICIpwKnAFhFJEZHUFsfPAzZE8DMATh1ykZVY\nGGNM97NvrZMcVxXDNctg/AXH3WSwoZFfvbKF657IY1ggmZdvPoNzxttgPGP6koiVWIQ4AOQR4EkR\n2Q4cwkmiwZn94lER2QgI8KiqrhORE4AX3Ol04oGnVfWVSH2GJgG/12qQjTGmu9nxOiy9yhmEd+1L\nkD3uuJssqqjl5mc+4t1Pi1k4cyg/vWgSiQldmyPZGNPzRbQGOYQBIDU4U7q1vq+ineM7gKnhj7Rj\ngRQfa0t6ztSfxhjT661/Dl74FmSNgaueh7TBx93kmp2HuPGpjyipquNXC6ZwWe7Qzm8yxvRK3XWQ\nXrfi9CBbDbIxxnQL7/0eVv4HDJ8FVzwNSRnH1Zyq8ti7+fz85c0Mzkji/779BSYNtincjOnLLEEO\nQZbfR0VtkJr6BvuqzRhjYqXlAiATLoJLHoaExONu9u5Xt3HPP7bxxQkD+J/LppKelHD8sRpjejRb\nOD4ETavp2UA9Y4yJoV3/dJqqM9UAACAASURBVJLjGdfCpY+HJTl+bk0B9/xjG1+ensODV8+w5NgY\nA1iCHJJAiq2mZ4wxMbflJfB44dz/hLjj/zbvne1FLH5+HbNGB/jFJZOJsyncjDEuS5BDEHB7kIsr\nrQfZGGNiQhU2/xVGngWJacfd3Nb95XzryTWMyvZz/1Uz8MbbP4fGmCPsiRCCLHdJUVssxBhjYuTA\nRji8EyZcePxNldWw6NEPSPJ6WLLoZNISrazCGHM0G6QXguYeZEuQjTEmNra8BAiMO76FQCprg3zt\nsdUcrq7n2W+expCMpPDEZ4zpVSxBDkGyN55kr8cG6RljTKxseQmGngL+/sfcRLChkZue/pAt+8t5\n+JpcThxiU7kZY9pmJRYhstX0jDEmRkryYf96GP8vx9yEqvLTZRtZtbWQO+ZPYvb4Y0+0jTG9nyXI\nIQqk+CiutBILY4yJui3ugqzHkSA/8OYOnnp/F986axRXnjI8TIEZY3orS5BDlOX32iA9Y4yJhS0v\nQ/+JEBh1TLf/de1e7lyxhYumDuaH548Lc3DGmN7IEuQQBVJ8VmJhjDHRVlkEu96F8cc2e8Xq/EN8\n/9m1nDwik18vmGJzHRtjQmIJcoiyUr0cqqyjsVFjHYoxxvQdn7wC2nhM5RU7Civ4xhN55GQm8eDV\nuSQmHP/iIsaYvsES5BAFUnwEG5XS6vpYh2KMMX3H5pcgfSgMmtql28pq6ln02Go8Ijy66GQyU7wR\nCtAY0xtZghwiW03PGGOirLYCPn3N6T2WrpVG/HXtXnYWV3HfldMZHkiJUIDGmN7KEuQQ2Wp6xhgT\nZZ/+Axpqj6n+eMX6/ZyQlcIpI/tFIDBjTG9nCXKIbDU9Y4yJsi0vQ1I/GHZal247VFnHezuKmTd5\nINLFnmdjjIEIJ8giMldEtorIdhFZ3MZ5n4j8yT3/voiMcI8niMjjIrJeRDaLyH+E2makNPUgW4mF\nMaY7EZE1InKjiGTGOpawaqh3BuiNmweeri36+vdN+2loVOadOChCwRljeruIJcgi4gHuA+YBE4GF\nIjKx1WVfB0pUdTTwv8Av3eOXAj5VnQzMAL4pIiNCbDMiMpO9iEBRuSXIxphu5XJgMLBaRJaKyPnS\nG7pN89+GmtJjmr1i+fr9DOuXzKTBaREIzBjTF0SyB3kmsF1Vd6hqHbAUmN/qmvnA4+72c8Ac98Gu\nQIqIxANJQB1QFmKbEeGJE/oleymy1fSMMd2Iqm5X1f8HjAWeBpYAO0XkdhHpuQW4W16G+CQ4YXaX\nbiutqued7UVWXmGMOS6RTJCHALtb7Be4x9q8RlWDQCkQwEmWK4F9wC7gLlU9FGKbAIjI9SKSJyJ5\nhYWFx/9pcOqQbbEQY0x3IyJTgP8Bfg08j/MtXBnwWizjOmaNjU6CPHoOeJO7dOvfNx8g2KhcYOUV\nxpjj0LXCruiZCTTgfG2YCbwlIq92pQFVfRB4ECA3Nzcsq3s4q+lZD7IxpvsQkTXAYeARYLGqNv0V\n/76IzIpdZMdh30dQvhfG39blW1es38eQjCSm5KRHIDBjTF8RyQR5DzC0xX6Oe6ytawrccop0oBj4\nCvCKqtYDB0XkHSAXp/e4szYjJivVx4Y9pdH6ccYYE4pLVXVHWydU9ZJoBxMWm18C8cDY87t0W1lN\nPW9tK+Ka04ZbeYUx5rhEssRiNTBGREaKiBe4AljW6pplwFfd7QXAa6qqOGUV5wCISApwKrAlxDYj\nJpDitUF6xpju5joRyWjaEZFMEfmvWAZ03La8DCNmQXLXSqhf23yQuoZG5k228gpjzPGJWILs1hTf\nBKwENgPPqupGEblDRC52L3sECIjIduB7QNO0bfcBfhHZiJMUP6qq69prM1KfobUsv5fy2iA19Q3R\n+pHGGNOZeap6uGlHVUuAC2IYz/Ep2gZFW2H8RV2+dfn6fQxMS2Ta0IzOLzbGmA5EtAZZVZcDy1sd\nu63Fdg3OYJLW91W0dby9NqMl4M6FfKiyjsEZSbEIwRhjWvOIiK+p9lhEkgBfjGM6dltect7Hdy3H\nr6gN8vonhXxl5jDi4qy8whhzfLrrIL1uKZByZDU9S5CNMd3EU8A/RORRd38RR6bP7Hk2vwSDp0F6\nTpduW7XlIHXBRi6w8gpjTBhYgtwFWalOp0yRraZnjOkmVPWXIrIOmOMe+k9VXRnLmI5Z2T7Ykwfn\n3NrlW1ds2Ed2qo8Zw3vXgoLGmNiwBLkLslLcBNkG6hljuhFVXQGs6Op9IjIXuAfwAA+r6p2tzg/D\n6Y3OcK9Z7Ja5RcbWl533LtYfV9UFWbWlkAUzcvBYeYUxJgwiOYtFrxPwuyUWtpqeMaabEJFTRWS1\niFSISJ2INIhIWQj3eXAGRM8DJgILRWRiq8tuxRkMPQ1n1qDfhzv+o2x5GfqNguxxXbrtja2FVNc3\nMG/ywAgFZozpayxB7oJkr4fEhDhbTc8Y053cCywEtgFJwHU4iW9nZgLbVXWHqtYBS4H5ra5RIM3d\nTgf2hiXitlQfhs/ehAkXQhfnMF6+YT+BFC8zR/TclbWNMd1LuwmyiPywxfalrc79dySD6q5ExFbT\nM8Z0O6q6HfCoaoOqPgrMDeG2ITiLLzUpcI+19DPgKhEpwJk96DttNSQi14tInojkFRYWdjl+ALb9\nHRqDMP7CLt1WU9/Aa5sPcN6kgcR7rM/HGBMeHT1Nrmix/R+tzoXy8O2VsvxeiqzEwhjTfVS5Cyd9\nLCK/EpFbCN+3gwuBx1Q1B2du5SdF5HNtq+qDqpqrqrnZ2dnH9pO2vAT+ATAkt0u3vflJIZV1DVxg\n5RXGmDDq6CEq7Wy3td9nZPl9NkjPGNOdXI3zLL8JqASGAl8O4b497rVNctxjLX0deBZAVd8DEoGs\n44z38+prYPurMO4CiOtabr9iw34ykhM49YRA2MMyxvRdHT2JtJ3ttvb7jIDfS7FN82aM6QbcgXb/\nrao1qlqmqrer6vfckovOrAbGiMhItwf6CmBZq2t24U4fJyITcBLkY6yh6MBnb0BdhVN/3AW1wQZe\n3XSA8yYOIMHKK4wxYdTRNG9T3ZHQAiS1GBUtOA/JPingd2qQVRXp4kASY4wJJ1VtEJHhIuJ1B9p1\n5d6giNwErMSZwm2Jqm4UkTuAPFVdBnwfeMgt21DgWlUNfwfJ5r+CLw1GnNml297ZXkR5bZB5tjiI\nMSbM2k2QVdUTzUB6ikCKl2CjUlYdJD05IdbhGGPMDuAdEVmGU2IBgKr+prMb3TmNl7c6dluL7U3A\nrPCF2obGBti6AsacC/HeLt26fP1+UhPjmTUq/FUfxpi+rd0EWUSSgXpVrXf3x+EM0shX1ReiFF+3\nk+U/spqeJcjGmG7gU/cVB6TGOJauK8l33rs4e0VdsJG/bdzPuRMH4I238gpjTHh1VGLxCs4AjW0i\nMhp4D3gKuFBETlHVxdEIsLtpSpCLK+oYdYyDtY0xJlxU9fZYx3BcAqPgB5+ANnbptvd2FFNWE+SC\nE628whgTfh0lyJmqus3d/irwjKp+xx3MsQbokwly02p6RbZYiDGmGxCRVbQxcFpVz4lBOMcmzoNT\nBh26Fev34ffFc/oYK68wxoRfRwlyywfuOcCvAVS1TkS69qd+L9K83LQlyMaY7uEHLbYTcaZ4C8Yo\nlqgINjSycuN+5kzoT2KCDZcxxoRfRwnyOhG5C2dezNHA3wBEJCMagXVX/ZKbepBtsRBjTOyp6ppW\nh94RkQ9iEkyUvP/ZIUqq6pln5RXGmAjpaGTDN4AiYARwnqpWuccnAndFOK5uK94TR2Zygs2FbIzp\nFkSkX4tXloicD6THOq5IWr5+H8leD2ePs4EgxpjI6Giat2rgzjaOvwu8G0rjIjIXuAenuOxhVb2z\n1Xkf8AQwAygGLlfVfBG5Evj3FpdOAaar6sci8jowCKh2z52nqgdDiSdcsty5kI0xphtYg1MSJzil\nFZ/hDLDulRoalZUb9zN7vJVXGGMip6Np3tZ1dKOqTunovLvC033AuUABsFpElrnzajb5OlCiqqNF\n5ArglzhJ8lM4M2YgIpOBv6jqxy3uu1JV8zr6+ZEU8HttkJ4xpltQ1ZGxjiGa8vIPUVRRZ7NXGGMi\nqqMSi0agAXgSuAy4qNWrMzOB7aq6w13haSkwv9U184HH3e3ngDny+eXpFrr3dhsB60E2xnQTInJj\ny7EhIpIpIt+OZUyRtGLDfhIT4qy8whgTUe0myKp6Ek5y6geeBn4OTAL2qOrOENoeAuxusV/gHmvz\nGlUNAqVAoNU1lwPPtDr2qIh8LCI/aSOhBkBErheRPBHJKywsDCHc0GWlWA+yMabb+IaqHm7aUdUS\nnDEkvU5jo7Jiwz7OHtufFF9HY8yNMeb4dLj8kKpuUdWfqup04K849cK3RCUyQEROAapUdUOLw1eq\n6mTgDPd1dVv3quqDqpqrqrnZ2eHtaQj4fZTVBKkL9tnZ7owx3YenZUeBW97WtTWbe4jthRUcKKvl\nixMHxDoUY0wv12GCLCJDROT7IvI2cBVOcnx/iG3vAYa22M9xj7V5jYjE44y8Lm5x/gpa9R6r6h73\nvRynZ3tmiPGETdNqeocqrczCGBNzrwB/EpE5IjIH55n5SoxjiogDZTUADOuXHONIjDG9XUeD9N4A\nUoFngUUcSVy9ItJPVQ910vZqYIyIjMRJhK8AvtLqmmU4q/S9BywAXlNVdX9+HE7t8xktYooHMlS1\nSEQSgAuBV0P5oOHUcjW9gemJ0f7xxhjT0o+A64Eb3P2/Aw/HLpzIKSx3StuyU30xjsQY09t1VMQ1\nHGfqoG/iPHybiHv8hI4aVtWgiNwErMSZ5m2Jqm4UkTuAPFVdBjwCPCki24FDOEl0kzOB3aq6o8Ux\nH7DSTY49OMnxQ51/zPDKsuWmjTHdRxLwkKr+AZpLLHxAVYd39UBNz9ymZ7AxxkRKR/MgjzjexlV1\nObC81bHbWmzXAJe2c+/rwKmtjlXizJkcU4EUp/fCZrIwxnQD/wC+CFS4+0k4K59+IWYRRUhRRR2+\n+Dj8NkDPGBNhHdYgm7Y1lVjYanrGmG4gUVWbkmPc7V5ZpFtYXkt2qo92Ji8yxpiwsQT5GPh98fji\n46wH2RjTHVSKyPSmHRGZwZGVRnuVoora5kHSxhgTSfY91TEQEbL8PgqtBtkYE3vfBf4sIntxxogM\nxJk/vtcpLK9lqM1gYYyJgg4TZHewx0ZVHR+leHqMgN9rPcjGmJhT1dUiMh4Y5x7aqqr1sYwpUooq\napk2LDPWYRhj+oAOE2RVbRCRrSIyTFV3RSuoniCQ4rUeZGNMdzEOmAgkAtNFBFV9IsYxhVVDo3Ko\nso5sm8HCGBMFoZRYZAIbReQDoLLpoKpeHLGoeoCA38eW/eWxDsMY08eJyE+Bs3ES5OXAPOBtnJVP\ne41DlXU0qs2BbIyJjlAS5J9EPIoeKMvvo7iiDlW1EdXGmFhaAEwFPlLVRSIyAPhjjGMKu6ZFQmyQ\nnjEmGjpNkFX1DREZDoxR1VdFJBlnkY4+Lcvvpa6hkbKaIOlJCbEOxxjTd1WraqOIBEUkDTgIDI11\nUOHWvEiI9SAbY6Kg02neROQbwHPAA+6hIcBfIhlUT9A8F7LVIRtjYitPRDJwVhVdA3wIvBfbkMKv\neZlp60E2xkRBKCUWNwIzgfcBVHWbiPSPaFQ9QPNqepV1nJAd42CMMX2Wqn7b3fyDiLwCpKnquljG\nFAnWg2yMiaZQEuRaVa1rqrMVkXhAIxpVD2A9yMaY7kZV82MdQ6QUVdSSmBBHirfPV/gZY6IglJX0\n3hCRHwNJInIu8Gfgr5ENq/tr+pqvyOZCNsaYiLNlpo0x0RRKgrwYKATWA9/EmUbo1kgG1RNkpjT1\nIFuCbIwxkVZUUWczWBhjoiaUWSwacQZ/PBT5cHqOBE8cGckJzXVxxhgTbX1ptdOiCltm2hgTPe0m\nyCKyng5qjVV1SkQi6kECKV6KKy1BNsbERl9a7bSwvJbpw22ZaWNMdHTUg3yh+36j+/6k+34VNkgP\ncFbTsxpkY0yM9frVToMNjRyqshILY0z0tJsgq+pOABE5V1WntTj1IxH5EKc2uU/L9vvYsr8s1mEY\nY/q2Xr/a6aGqOlQh2509yBhjIi2UQXoiIrNa7HwhxPsQkbnu13/bReRzCbWI+ETkT+7590VkhHv8\nShH5uMWrUUROcs/NEJH17j2/lRgOaQ74vRRXWg+yMSZ2VPUNIB9IcLdX4ywW0ms0LxJicyAbY6Ik\nlET368DvRSRfRHYCvwe+1tlN7uCR+4B5wERgoYhMbKPtElUdDfwv8EsAVX1KVU9S1ZOAq4HPVPVj\n9577gW8AY9zX3BA+Q0QEUnwcrqqnvqExViEYY/q441ntNIROjP9t0VHxiYgcDl/koWsqZbMSC2NM\ntHSaIKvqGlWdCkwFpriJayi9EzOB7aq6Q1XrgKXA/FbXzAced7efA+a00SO80L0XERmEs0rUP1VV\ngSeAfw0hlohoWizkkPUiG2Ni50ZgFlAGzmqnQKernYbSiaGqt7TorPgd8H9hjj0kRW4PsiXIxpho\n6TRBFpF0EfkN8A/gHyLyPyKSHkLbQ4DdLfYL3GNtXqOqQaAUCLS65nLgmRbXF3TSZlPc14tInojk\nFRYWhhBu12W5CbJN9WaMiaFatxMC6NJqp6F0YrS0kCPP4qgqrLASC2NMdIVSYrEEKAcuc19lwKOR\nDKqJiJwCVKnqhq7eq6oPqmququZmZ2dHILojvRm2WIgxJoaOdbXTUDoxABCR4cBI4LV2zke0Q6Ko\nvJakBA8pvk6n7jfGmLAIJUEepao/dXsZdqjq7cAJIdy3BxjaYj/HPdbmNW6vRzpQ3OL8FRzdY7HH\nbaejNqMm0JQg21zIxpjYicZqp1cAz6lqQ1snI90hUVhRS1aqzWBhjImeUBLkahE5vWnHndGiOoT7\nVgNjRGSkiHhxHrDLWl2zDPiqu70AeM2tLUZE4nB6rJc2Xayq+4AyETnVrVW+BngxhFgioqkGuajc\nepCNMbGhqo2q+pCqXqqqC9ztUEosQunEaNK6syKqiipqybb6Y2NMFIXyfdUNwOMt6o5LgGs7u0lV\ngyJyE7AS8ABLVHWjiNwB5KnqMuAR4EkR2Q4cwnkINzkT2K2qO1o1/W3gMSAJWOG+YiLVF4/XE0eR\n9SAbY6JMRJ5V1cvaW/U0hNVOmzsxcBLjK4CvtPFzxuMsRvLe8Ud9bIrK6xgesGWmjTHR02mC7E6v\nNlVE0tz9kFfGUNXlOF/3tTx2W4vtGuDSdu59HTi1jeN5wImhxhBJIuLMhWw1yMaY6Puu+35hh1e1\nI8RODHAS56Uh9kpHRGFFLbkjbJlpY0z0dJogi8h/A79S1cPufibwfVUNd41bj+QkyNaDbIyJupeA\n6cB/qerVx9JAZ50Y7v7PjjXAcAg2NFJiy0wbY6IslBKLear646YdVS0RkQsI/yCQHinL77PV9Iwx\nseAVka8AXxCRS1qfVNWYzFkcbocqnWWms2yKN2NMFIWSIHtExKeqtQAikgT0/idVYyPEdT6GMZDi\n45P95VEIyBhjjvIt4EogA7io1TklRot6hNvBpmWmrQfZGBNFoSTIT+EsENI09/Eijqx+1zutfw5W\n/Ahu/hASO14TJcvvpaiyDlXl84sAGmNMZKjq28DbIpKnqo/EOp5IKWpeJMSmeTPGRE8og/R+KSJr\ngS+6h/5TVVdGNqwYSxsCVUXw6Wsw6UsdXpqTmURdsJEPd5UwY3i/KAVojOnrROQcVX0NKOnNJRZF\n7iBoq0E2xkRTKPMgA2wGXlHVHwBviUhqBGOKvZyTISkTPun874BLpufQP9XHf760mcbGmA3yNsb0\nPWe57xe18TqmmS26o0K3xMISZGNMNIUyi8U3gOuBfsAonKVI/wDMiWxoMeSJh9Hnwra/QWMDxHna\nvTTFF88P547nB39ey7K1e/nXaW2u1GqMMWGlqj913xfFOpZIKqqoJdlry0wbY6IrlB7kG4FZQBmA\nqm4D+kcyqG5h7PlQVQx71nR66SXThjB5SDq/fGUL1XVtrsRqjDERISL/JiJp4nhYRD4UkfNiHVe4\nFFXUWu+xMSbqQkmQa1W1eR4zEYmnjVWbep3Rc0A88MkrnV4aFyf85MKJ7Cut4cE3Wy/8Z4wxEfU1\ndwGn84AAcDVwZ2xDCp/C8lqybYo3Y0yUhZIgvyEiPwaSRORc4M/AXyMbVjeQlAnDTgupDhlg5sh+\n/MvkQfzhjU/ZV1od4eCMMaZZ0/Q5FwBPqOrGFsd6PKcH2WawMMZEVygJ8mKgEFgPfBNn1aW+sUjI\n2PPhwAY4vDukyxfPG0+DKr9+ZWuEAzPGmGZrRORvOAnySncQdWOMYwqbwnIrsTDGRF+nCbKqNgJ/\nAb6tqgtU9SFV7f0lFgBj5zrv20LrRR7aL5nrTh/J/320h493H45gYMYY0+zrOB0ZJ6tqFZCAM199\nj1ff0EhJVb2VWBhjoq7dBNkd8PEzESkCtgJbRaRQRG6LXngxljUGMkeGXGYB8O3Zo8ny+7jjrxvp\nK39HGGNi6jRgq6oeFpGrcL7hK41xTGFxqNLmQDbGxEZHPci34MxecbKq9lPVfsApwCwRuSUq0cWa\niNOLvOMNqKsM6Ra/L54fnj+OD3cd5q/r9kU4QGOM4X6gSkSmAt8HPgWeiG1I4WFzIBtjYqWjBPlq\nYKGqftZ0QFV3AFcB10Q6sG5j7PnQUAufvRnyLV+ekcPEQWncuXwzNfU27ZsxJqKCbtnbfOBeVb0P\n6BWLORU2LzNtCbIxJro6SpATVLWo9UFVLcSpcesbhs8Crz+k6d6aeNxp3/aW1vDwWzbtmzEmospF\n5D9wOi9eFpE4eskzusjtQc62HmRjTJR1lCDXHeO53iXeC6POceqQu1BTfNqoAHMnDeT3r3/KgbKa\nCAZojOnjLgdqga+r6n4gB/h1bEMKj6Ye5KxUm+bNGBNdHSXIU0WkrI1XOTA5lMZFZK6IbBWR7SKy\nuI3zPhH5k3v+fREZ0eLcFBF5T0Q2ish6EUl0j7/utvmx+4r8qn5j50L5Pti/rku3/ccF4wk2KL9e\nadO+GWMiQ1X3q+pvVPUtd3+XqvaKGuSi8jpSvB6SvbbMtDEmutpNkFXVo6ppbbxSVbXTr+9ExAPc\nB8wDJgILRWRiq8u+DpSo6mjgf4FfuvfGA38EvqWqk4CzgfoW912pqie5r4Ohf9xjNOZcQLo0mwXA\n8EAKi04fwXNrClhf0CsGlRtjuhkROVVEVotIhYjUiUiDiPSKB05RRS1ZVn9sjImBUBYKOVYzge2q\nusNdqnopziCSluYDj7vbzwFzRERwlkxdp6prAVS1WFVjN9rN3x+GzOhSHXKTm2aPJpDi5Y6XbNo3\nY0xE3AssBLYBScB1wO9jGlGYFJbXWv2xMSYmIpkgDwFaLkFX4B5r8xpVDeLM3RkAxgIqIitF5EMR\n+WGr+x51yyt+4ibUnyMi14tInojkFRYWHv+nGTsX9qyBiq51WKcmJvD988axOr+E5ev3H38cxhjT\niqpuBzyq2qCqjwJzYx1TODjLTFuCbIyJvkgmyMcjHjgduNJ9/5KIzHHPXamqk4Ez3NfVbTWgqg+q\naq6q5mZnZx9/RGPPd963/a3Lt15+8lDGD0zlFyts2jdjTNhViYgX+FhEfuXOU99dn+1d4pRY2AA9\nY0z0RfIhugcY2mI/xz3W5jVu3XE6UIzT2/ymqha5S6cuB6YDqOoe970ceBqnlCPyBk6G1MHHVGbh\niRNuu3AiBSXVLHnns85vMMaY0F0NeICbgEqcZ+qXYxpRGDQvM+1PjHUoxpg+KJIJ8mpgjIiMdHs3\nrgCWtbpmGfBVd3sB8Jo74f1KYLKIJLuJ81nAJhGJF5EsABFJAC4ENkTwMxwh4vQif7oKgrVdvv0L\no7M4d+IA7lq5lZuf+YhNe8siEKQxpq9R1Z2qWq2qZap6u6p+zy256NGKK9xlpq0H2RgTAxGbO0dV\ngyJyE06y6wGWqOpGEbkDyFPVZcAjwJMish04hJNEo6olIvIbnCRbgeWq+rKIpAAr3eTYA7wKPBSp\nz/A5Y+fCmkdh5zvO3MhddNeCqdz3+nae+udOlq3dy+xx2dxw9mhOHpFJO6XUxhjTJhFZj/N8bJOq\nToliOGFny0wbY2IpopNLqupynPKIlsdua7FdA1zazr1/xJnqreWxSmBG+CMN0cgzIT7Rme7tGBLk\n9OQEfnzBBG48ezRP/jOfR9/J57IH3mPG8ExuOGsU54zvT1ycJcrGmJBcGOsAIqnIlpk2xsRQrxjI\nETXeZBh5Fmxd0aVV9VpLT07gpnPG8PaPzuGO+ZPYX1rDdU/kMfeeN/m/Dwuob2gMY9DGmF4qAchx\nSyyaXzjjPXr8yhpNq+jZNG/GmFiwBLmrxp4Ph3dC0SfH3VSS18M1p43g9X8/m7svPwlB+N6zazn7\n16/z+Lv5VNYGwxCwMaaXuhtoazBDmXuuR7MSC2NMLPX4XoaoG3s+vIwzm0X2uLA0meCJ41+nDWH+\nSYN5bctBfv/6p/x02UZ+umwjCR4h2RtPitdDktdDii+eZK+HFG+8s++NJ9nn4YSsFOZPG0JaYqeL\nHBpjeocBqrq+9UFVXS8iI6IfTngVVdTi9znPOWOMiTZLkLsqPQcGTHbqkGf9W1ibFhHmTBjAnAkD\nWJ1/iPd3FFNZ10BVbZCqugaq6hqorAtSVdvAgfKa/9/enYdXVV6LH/+uk3meIQkhJEwyyBCIaAUH\npCJ2kOqlznWo1moHa/21v9pen171Z59re63X8Xqd61ClVuvQW5E60KpXZR6UQUEJEAiQBDKSOev3\nx7uTHCDMOZwh6/M8+zn77L3PPu8+B14W71l7vexpcc8bWzpoaGnn3+et41slg7jiK0MYlZvap20z\nxoSc9IPsSzhurQiQqoZWspOtgoUxJjgsQD4aI8+BD/4T9uyCxMyAvMVJRZmcVHT4515VXsMzH23i\n5aXlPL9wM1OKMvnOudheBAAAIABJREFUV4ZwzthcYqMtk8aYCLRERL6nqntV8hGRa4GlQWpTn6ms\nb7b0CmNM0FiAfDRGzoL374Yv3oVxc4LdGgDGF6Rz97fT+devjebPS7fw3Meb+fELy8lJieOSKYVc\nOqWQ3DQruG9MBLkJeEVELqMnIC4FYoHzg9aqPlLV0MqIAcnBboYxpp+yocWjMWgSJGYf1ax6gZaR\nFMt1pw/jHz87k6euOokT81N54N31TP3tu/zgj0v56Itq9BgqcBhjQoOq7lDVU4HbgTJvuV1Vv6Kq\n24PZtr5Q1dBiI8jGmKCxEeSj4YuCETPhszegox2iQu9j9PmE6aMGMH3UADZVN/LHhZt5cckW3vhk\nO3lp8UwqzKCkMJ2SwnTG5qcRH2M3whgTjlR1AbAg2O3oS63tndTsabMA2RgTNKEX2YWLkefAyueh\nfBEMOTXYrTmoIVlJ/Opro7n57JH8z6oK/vl5Jcs37+Zvn1QAEBMljM5LpWRwOiVe4FyYmWiz+xlj\ngqK60SYJMcYElwXIR2vYWeCLdmkWIR4gd4mPiWLO5ALmTC4AYGd9Mys217B8Sw3LN+/mz0vLefqj\nTQBkJsVSMjidcQVpjM5LZXRuKgUZCTbTnzERRERmAfcBUcDjqnpXL8dcCNyGm9Z6papeGuh2VdW3\nAlgVC2NM0FiAfLTiU2HIVFfu7ew7gt2aozIgJZ6ZY3OZOTYXgPaOTtbvbGD5ZhcwL99Sw7uf7eye\nNDA5LpoTclMYnZfC6LxURuWmMio3haQ4+2NkTLgRkSjgIeBsoBxYLCKvq+oav2NGAL8EpqrqbhEZ\ncDzaVtnQDEC2jSAbY4LEIptjMXIWzP8l7NoImcXBbs0xi47yudHivFQuPbkQgKbWDj7bUc+6ijrW\nVtSxtqKe11Zs47mPNwMgAkMyExmVm8qY/FTG5KUydlAquanxlqJhTGibAmxQ1S8BRGQuMBtY43fM\n94CHVHU3gKruPB4N6xpBtmmmjTHBYgHysRh5jguQ1/8dTv5+sFsTEAmxUUwcnM7EwT1zEqgqW2ua\nWFvhBc7bXeA8f8327tHmzKRYxua7oHlsfhpj81MpzkqyFA1jQscgYIvf83Lg5H2OGQkgIv+LS8O4\nTVX3K98jItcB1wEUFhYec8MqGywH2RgTXBYgH4usYZA1wuUhR2iA3BsRoSAjkYKMRM4eM7B7e2NL\nO2sr6li9rY7V22pZva2OJz/YSFuHi5oTY6MYlZvC2Pw08tLjSYmLJjk+muS4GJLjokmJjya5e1s0\ncdE+G4U2JriigRHAmUAB8J6IjFPVGv+DVPVR4FGA0tLSY64jWVnfQkpctFXXMcYEjQXIx2rkObDw\nEfj0L3DiBcFuTVAlxUVTWpRJqd8MgK3tnazfWc/qbXWs8ZZXlm+loaX9kOeL9gkp8dEMy0lmgjeK\nPXFwOgUZCRY4G3PstgKD/Z4XeNv8lQMLVbUN2Cgin+MC5sWBbFhVQ4vlHxtjgsoC5GM19SewZSG8\ndDVs/hhm3gnRdud1l9hon5dikda9TVVpae+kvrmdhpZ2GprbqW9po6HreUvP9pqmNj7bXs9zH2/i\niQ82Au7O9gkF6d1B84SCdNISY4J1icaEq8XACBEpxgXGFwP7Vqh4FbgEeEpEsnEpF18GumGV9S1W\nwcIYE1QWIB+r5AFw1Rvw9r/Bx/8FW5fAt/8A6ceehxepRIT4mCjiY6IOO8ewraOTdRX1rCivYcXm\nGlaW1/DOup77hYZmJzFxcDpj8r3qGnkpNsmAMQehqu0i8iNgPi6/+ElVXS0idwBLVPV1b99MEVkD\ndAA/V9XqQLetqqGFE3JTAv02xhhzQAENkA9VY1NE4oBngMlANXCRqpZ5+8YDjwCpQCdwkqo2i8hk\n4A9AAvAG8BMN9tzJ0bEw69+h8BR47Ufw36fBBY+69AvTJ2KifIwrSGNcQRrfOWUIAHXNbXxSXsuK\nLTWs2FLDBxuq+Mvynl+Is5PjGJWb4pY8V5Ju+IBky2s0xqOqb+D6Uf9tv/ZbV+BmbzluqhpamWr/\nwTXGBFHAAuTDqbEJXAPsVtXhInIx8FvgIhGJBp4DvqOqK0UkC2jzXvMwrvTQQlzHPguYF6jrOCJj\nZsPAE+HPV8LzF8K0n8L0W0NyKupIkBofw9Th2Uwdnt29raqhhc+217Nuu6uwsW57Pc9+vImW9k4A\nonxCcXYSJ+SmkJ8WT05KnFuSe9bTE2Ks2oYxQdLS3kFtk00zbYwJrkBGbodTY3M2boYmgJeAB8Xd\nfTUTWKWqKwG6ftITkTwgVVU/9p4/A3yLUAmQwVW2uOYtmPcL+OA/YctimPMEpOQGu2X9QnZyHNnD\n4/YKmjs6lbLqRtZV1LPOK0n36dZa3l6zoztw9hftE7KT47oD5sykWGKiBBFBcLWfBcEndN8s6BNB\nxL02NSGG9MQYMhJjux+71m302piDq27waiDbTXrGmCAKZIB8ODU2u4/x8uFqgSzcjSAqIvOBHGCu\nqv7OO758n3MO6u3N+7ou5xGJSYDz7ndTUP/PT13KxZwnoPj049sOA7hR42E5yQzLSebr4/O6t6sq\nDS3tVNa3uKWhpWfde76jrpm1FXW0dyqq7jXqvbbT7zkKnaq0d2qvQXeXxNio7mA5JyWOiYPTmVKU\nSUlhBgmxFjwbU+XVQLYRZGNMMIXqb//RwDTgJGAP8I6ILAVqD/cEfV2X86hMuBjyJsCLV8Azs+HM\nX8Fp/wd8vqA0x+xNREiJjyElPoahOcl9dt7mtg5q9rSxe0+rWxrdes2eVnbv6VpvY1tNE/d9vh5V\nN/J84qA0phRnclJRJicVZZCeaHfxm/6nsr4rQLY//8aY4AlkgHw4NTa7jin38o7TcDfrlQPvqWoV\ngIi8AUzC5SUXHOKcoWXAaPjeAvifm2DBnW5SkcJTIOcEyB7plsTMQ5/HhI34mChy06LITYs/5LG1\nTW0s27SbRWW7WLxxF3/43zIefc9V0TphYAonFWdwUlEmkwozGJgaT2y0/efKRLYqm0XPGBMCAhkg\nH06NzdeBK4GPgDnAu6ralVrxf0UkEWgFzgD+U1UrRKRORE7B3aR3BfBAAK+hb8QlwwWPwZCpsOgx\nt3S09OxPzPYC5hGQ7QXOOSMhtcBGmyNcWkIM00cNYPqoAYAbfV65pYZFG3exqGwXryzbynMfb+4+\nPj0xhhy//Oi91r0lKTaaXY2t7GpspbqxlV2NLVQ3dK33bNvV0EqHKkMykyjKTqQoK4mi7CTvMZGB\nKfF2s6I57qq8HGRLsTDGBFPAAuTDrLH5BPCsiGwAduGCaFR1t4jcgwuyFXhDVf/mnfoH9JR5m0co\n3aB3MCJQerVbOjugZjNUfe6Wys+gaj2seQ2adve8JirW3dyXkg+peX6PeZCa7x5T8iDm0COVJjzE\nx0Rx8tAsTh6aBUB7RydrK+pZva12vzzpFVtq2FnXQlNbxyHPGxvlIzMplsykWLKSYynOSiQzKQ4R\n2FS9hy8qG1mwrpLWjp786fgYH0VZSQzJSqQoO4nCzETy0uLJTU0gLy2e9MQYm9HQ9DmbZtoYEwoC\nmoN8GDU2m4FvH+C1z+FSKvbdvgQ4sW9bepz5oiCz2C3+tZJVYU+1FzB/Drs3Ql0F1FdAxSr4fD60\n7dn/fAmZUHwafPN+SEg/ftdhAi7ar/7zgTR23WjoBc8NLe1kJsaSmRxLlhcUJ8dFHzKY7ehUKmqb\nKKvaw8bqRjZVNVJW3ciGnQ37Bc8AcdE+8tLiGZga7wLntATvMZ6U+GhQvBsaQXE3OXb63dTYtS0h\nJooBqXHkpMSTGn/odprIVtnQYukVxpigC9Wb9PonEUjKdkvR1P33q0JzrQuY6yu84HmbG41e8QLs\nWAOX/smVmjP9RlJcNElx0RRlJx3TeaJ8QkFGIgUZiUwbkb3Xvo5OZWd9M9tr3VJR28z2up7nSzfv\nZkft9v2C6CMVF+0jJyWOASlxDEiJd4FzchwDUt3zwiyXChJlqR8Ry00zbQGyMSa4LEAOJyJuhDgh\n3d3852/8xfCny+Gxs+CiZ62knOlTUT4hLy2BvLSEAx7T2ans2tPK9tpm6pvbvXrRrlqICLiYVvbe\nDjS2uhHwnXVuFHxnXTM761v4orKBj76sprapba/3iY/xMXKgN0OiN634qNxUMpMOXfWgsaWdzbv2\nuKXaPZbv3kNyfAzF2UkUZydSnJ1McVYSaYkxx/KRmaNU1dDCKJtm2hgTZBYgR4qiqfC9d+D5i+HZ\n8+Hrv4fJVwW7VaYf8XkTrPT16F9zWwdVDS3sqGthY1Vj9wyJ76zdyYtLesqiD0yN8wuYU2jv0J5g\neNcetuza030DWJfU+GgKMhLZUNnA31Zto9OvIGRmUizF3k2LQ3PcY3F2EsMHJFs1kQCqqm8hZ3j2\noQ80xpgAsgA5kmQOhWvfgj9fDX/9ictlnnmny3k2JkzFx0R1p35MHpKx177K+hbWba9jXUU9a73H\nj76o7k718AnkpydQmJnI2WMGMjgzkcLMRIZkupsO/UeJW9o72LJrD19WutzrjVVu+WBDJS8v6wnE\nY6N9jBuURsngdEoKMygpTCcvLd5yp/tAc1sHdc3tlmJhjAk6C5AjTXwaXPoi/P1W+Pi/XHWMOU9C\nfGqwW2ZMn3Ol7XI4bURO97a2jk7KqhqJifIxKCOBmKjDG+2Ni45i+IAUhg/Y/+f9xpZ2yqob+aKy\nkU/Ka1i+uYZnP97E4x9sBNzo9cSugHlwOuMK0kiMte71SFU3eiXe7CY9Y0yQWQ8eiaKi4dy7XC3l\nN34OT5wNl8x1VTOMiXAxUT5GDOzbHNakuGjG5qcxNj+N8ybkA9Da3sm67XUs31zD8s27Wb6lhvmr\ndwAuZ3tUbgo/mTGCmWNz+7QtkazKm0Uvx0aQjTFBZgFyJCv9LmQOc1NdPz4DLnoOhpwa7FYZExFi\no32ML0hnfEE6V55aBEB1QwsrvRHm5ZtrLFf5CHVPM20jyMaYILMAOdINPQOufQdeuAiePg++eR+U\nXBbsVhkTkbKS4zhr1EDOGjUw2E0JS13TTGcnH7oiiTHGBJIFyP1B9nC49m148Up47Qew8gWISQTx\neYt4iw8Qv+0+N5tfTIK3JO7zmLD3vqwRkJQV7Ks1xoSpngDZRpCNMcFlAXJ/kZABl78M7/4/+PKf\n0FIH2ulNddbpFrRnXRW0Azra3ex9bU3Q3nSINxHIL4HhX3XLoMkuH9oYYw5DZX0LKfE2zbQxJvgs\neulPomLg7DuO/vWdndDe7ILlrqC567G1EbYtgw1vw/t3w3u/cxU1hp7pguVhMyBtUF9diTEmAlU1\ntNo006bfa2tro7y8nObm5mA3JaLEx8dTUFBATMzhTQJlAbI5fD4fxCa6hV5SKUZ8Fc74v9C0G778\nhwuWN7wLa15z+weMgeEzXLA85FSItn8IjTE9KhtsmmljysvLSUlJoaioyOqr9xFVpbq6mvLycoqL\nD6+ilwXIpu8lZMDY892iCjvXesHy27DwEfjwAYhOcLP/DZ0Ow85yU2dbR2BMv1ZV38LoPKvZbvq3\n5uZmC477mIiQlZVFZWXlYb/GAmQTWCIwcIxbpt7oUjE2vg9fLoAv3oW//6s7LjkXhnnB8tAzIXlA\nMFttjAmCyoYWTrcUC2MsOA6AI/1MLUA2x1dsEpwwyy0AteXwhRcsfz7fVdgAGDjOC5inQ95ESMwM\nXpuNMQHX3NZBfXO7lXgzxoQEC5BNcKUVwKTvuKWzE7av7AmYP34YPrzfHZeY5crIZQ/3HkdA1nDI\nKIZo+wfVmHBnJd6MCQ3V1dXMmDEDgO3btxMVFUVOTg4AixYtIjb20P/mXn311dxyyy2ccMIJBzzm\noYceIj09ncsuC825GSxANqHD53Nl4vJL4LSbXTrG5o9gxxqoXg/VX8Dnf4fG53peI1GQMaQnaM4e\nAdknQPZIq8lsTBipamgFsCoWxgRZVlYWK1asAOC2224jOTmZn/3sZ3sdo6qoKj5f77OFPvXUU4d8\nnx/+8IfH3tgACmiALCKzgPuAKOBxVb1rn/1xwDPAZKAauEhVy0SkCFgLfOYd+rGqXu+95h9AHtBV\nlHemqu4M5HWYIIlN6qmp7K+pxgXL1euhar33uAE2/tOVoeuSmOUFyyMg54Se9bTBLhg3xoSM7mmm\nbQTZmG63/3U1a7bV9ek5x+Sn8m/fHHvEr9uwYQPnnXceJSUlLF++nLfeeovbb7+dZcuW0dTUxEUX\nXcSvf/1rAKZNm8aDDz7IiSeeSHZ2Ntdffz3z5s0jMTGR1157jQEDBnDrrbeSnZ3NTTfdxLRp05g2\nbRrvvvsutbW1PPXUU5x66qk0NjZyxRVXsHbtWsaMGUNZWRmPP/44EydO7NPPpDcBC5BFJAp4CDgb\nKAcWi8jrqrrG77BrgN2qOlxELgZ+C1zk7ftCVQ/0CVymqksC1XYT4hLSoWCyW/x1dkLtZqj8HKo+\nh6rP3Pra12HZ7p7jYhJdesaA0d4y1j2mFVglDWOCpDvFwkaQjQlZ69at45lnnqG0tBSAu+66i8zM\nTNrb25k+fTpz5sxhzJgxe72mtraWM844g7vuuoubb76ZJ598kltuuWW/c6sqixYt4vXXX+eOO+7g\nzTff5IEHHiA3N5eXX36ZlStXMmnSpONynRDYEeQpwAZV/RJAROYCswH/AHk2cJu3/hLwoNitm+Zo\n+XyQUeSWkTP33tdY5YLmys96Hss+gFV/6jkmNsUvaB7T85icczyvwph+qap7BNnuKTCmy9GM9AbS\nsGHDuoNjgBdeeIEnnniC9vZ2tm3bxpo1a/YLkBMSEjj33HMBmDx5Mu+//36v577gggu6jykrKwPg\ngw8+4Be/+AUAEyZMYOzY4/d5BDJAHgRs8XteDpx8oGNUtV1EaumZgaJYRJYDdcCtqur/iT4lIh3A\ny8Cdqqr7vrmIXAdcB1BYWNgHl2PCWlK2W4acuvf2phpXp3nnGu9xrTfi/LTfa3PctNmDp8DgkyF/\nkjdZylFo2g0VK2HbCqjfDif+CxSU2si16fcqG1pIjY8mLtqmmTYmVCUlJXWvr1+/nvvuu49FixaR\nnp7O5Zdf3uvsf/439UVFRdHe3t7ruePi4g55zPEUqjfpVQCFqlotIpOBV0VkrKrW4dIrtopICi5A\n/g4uj3kvqvoo8ChAaWnpfgG0MYBL1xjyFbd0UYWGnT1B845PoXwxfP6m2++LhoEnumB58BS3pA3e\nP8htqnHBcMUKFxBvWw67N/bsj4qFhQ+7gPvk62Hst2x2QdNvVTW0WHqFMWGkrq6OlJQUUlNTqaio\nYP78+cyaNatP32Pq1Km8+OKLnHbaaXzyySesWbPm0C/qI4EMkLcCg/2eF3jbejumXESigTSg2hsR\nbgFQ1aUi8gUwEliiqlu97fUi8jwulWO/ANmYoyYCKQPdMmx6z/Y9u1ygvGUhbFkEy5+FRY+4fSl5\nLlDOGe1uGty2HHZ92fPatELIn+jK2eWXuNrOUbGu7vPCR+CV6+Dvt8JJ18Dkq917G9OPVNW3kmM3\n6BkTNiZNmsSYMWMYNWoUQ4YMYerUqX3+Hj/+8Y+54oorGDNmTPeSlpbW5+/TG+klO6FvTuwC3s+B\nGbhAeDFwqaqu9jvmh8A4Vb3eu0nvAlW9UERygF2q2iEiQ4H3gXG4dIt0Va0SkRjgBeBtVf3vg7Wl\ntLRUlyyxe/pMH+tod6PLWxZB+SIXONdsdqPJeRO8knUTIa/k4CXnOjvdzIILH4H188EXAydeACd/\n36V2mH5FRJaqaumhjwxNR9vfTr/7H4zJT+WhS4/fTTjGhKK1a9cyevToYDcjJLS3t9Pe3k58fDzr\n169n5syZrF+/nujooxvf7e2zPVCfG7ARZC+n+EfAfFyZtydVdbWI3IEbCX4deAJ4VkQ2ALuAi72X\nnw7cISJtQCdwvaruEpEkYL4XHEcBbwOPBeoajDmoqGgXAOdPhJOvc9vamiAm4cjO4/PB8Bluqf4C\nFj0Ky//obiAsmOIC5TGzISqm76/BmBBRVd9iI8jGmL00NDQwY8YM2tvbUVUeeeSRow6Oj1RA30VV\n3wDe2Gfbr/3Wm4Fv9/K6l3H5xftub8TVTDYmNB1pcLyvrGFw7m9h+r/2pF+8fA28fqObMVA7XY70\nXo/egvccICrO5TNHx+293v08FqLjXZpHdJx77Fqiu9bjXFDetT8xCwZNgvQhdlNhhDiMWvVXAf9B\nT3rcg6r6eF+3o7mtg/qWdpskxBizl/T0dJYuXRqU9w7Vm/SM6d/iU93I8Unfgy/egfVvAQri61lg\n7+ci7lEVOlqgvdVNnNLhPfo/b93jcqo7Wr393mNHC3S0ufXOA9xFnJjtKm8MmuwtkyAh47h9NKZv\nHGateoA/qeqPAtmWSivxZowJMRYgGxPKfD4YcbZbjrfODi9Y9oLm2nLYugS2LoPyJfD5fMC7hyFr\nuBcse4HzwLEQE3/822yOxOHUqj8uuicJsRQLY0yIsADZGNM7X5RbugLdpGyXb32St7+51lXrKPeC\n5i//sffEK8kD3Q2L6YWQ7j2mFfY8j03a9x2Pr8YqWDkXPn0ZMotdqb2Ck/pT+sjh1KoH+BcROR13\n0/VPVXXLvgcca935qoZWAEuxMMaEDAuQjTFHJz4Nhp7pFnCpHXVbXcBc9TnUbIKaLS6IXvtX6Gzb\n+/UJmS5YHjQJxl0IhacEPjjtqhiy7BlY9zfXprwJsP5tFyjnTXSB8okXWE1q56/AC6raIiLfB54G\nztr3oGOtO9+TYmGfuTEmNFiAbIzpGyKQVuCWfXV2QsN2FzDXbIbazd76JjeKu+RJFyyPv8gt2SP6\ntm21W2HFH2HZs+69EzJgyveg5DswcAy0NMCqubDwUXj1eleTevJVri51an7ftiV0HLJWvapW+z19\nHPhdIBrSlWKRZTnIxgTd9OnTueWWWzjnnHO6t91777189tlnPPzww72+Jjk5mYaGBrZt28aNN97I\nSy+9tN8xZ555JnffffdeU1Xv69577+W6664jMdHNVvu1r32N559/nvT09GO8qiNnAbIxJvB8Phdo\npuZD4T6/4rfUu9HcVX+C938P7/2HqyE9/mI3FXdyztG9Z0eby5Ne9jRseNtV+Bh6Jpx9G4z6xt4j\nxHHJcNK1UHoNbPynqx7y/u/hf++F0d90o8qDT4609IvFwAgRKcYFxhcDl/ofICJ5qlrhPT0PWBuI\nhlTWt5CWEGPTTBsTAi655BLmzp27V4A8d+5cfve7Q///OD8/v9fg+HDde++9XH755d0B8htvvHGI\nVwSOBcjGmOCKS4EJF7ulrsKlOqz6E7z5C5j/Kxh2ltt3wtcg1nWaqLoc6KZdrhrHnl3eerVbb9wJ\nn73pHlPyYNrNUHK5yzU+GJGetJHdZbD4cZeOsfoVyB3vKouMPT/4+dN94DBr1d8oIucB7bha9VcF\noi1VDS1WwcKY3sy7BbZ/0rfnzB0H5951wN1z5szh1ltvpbW1ldjYWMrKyti2bRslJSXMmDGD3bt3\n09bWxp133sns2bP3em1ZWRnf+MY3+PTTT2lqauLqq69m5cqVjBo1iqampu7jbrjhBhYvXkxTUxNz\n5szh9ttv5/7772fbtm1Mnz6d7OxsFixYQFFREUuWLCE7O5t77rmHJ598EoBrr72Wm266ibKyMs49\n91ymTZvGhx9+yKBBg3jttddISDjGkqtYgGyMCSWpeXDqj9yyc60LlFf92dWCjk2G1EE9QbF29H4O\n8bkUisEnw6QrYfhX3aQuRyqjCGbeCWf+Ela96CZwee2H8MbPYeQsl6c8/OywrtZxGLXqfwn8MtDt\nqGposRv0jAkRmZmZTJkyhXnz5jF79mzmzp3LhRdeSEJCAq+88gqpqalUVVVxyimncN555yEH+GXt\n4YcfJjExkbVr17Jq1SomTeqZJfM3v/kNmZmZdHR0MGPGDFatWsWNN97IPffcw4IFC8jOzt7rXEuX\nLuWpp55i4cKFqConn3wyZ5xxBhkZGaxfv54XXniBxx57jAsvvJCXX36Zyy+//Jg/BwuQjTGhacBo\n+OptcNavYfOH8MmfXWCcmOkmLUnI7GU9E+LSXEpHX4lNgtKrXU7ypg/h05dgzWuw+i8QmwKjvu6C\n5aHT3SQr5ohV1rdw4qC0YDfDmNBzkJHeQOpKs+gKkJ944glUlV/96le89957+Hw+tm7dyo4dO8jN\nze31HO+99x433ngjAOPHj2f8+PHd+1588UUeffRR2tvbqaioYM2aNXvt39cHH3zA+eefT1KS+/Xu\nggsu4P333+e8886juLiYiRMnAjB58mTKysr65DOwANkYE9p8Piia5pZgEoGiqW459z+g7D349C+u\nQsequRCfDqO/AWMvgOIzjm7Uup+qami1ChbGhJDZs2fz05/+lGXLlrFnzx4mT57MH/7wByorK1m6\ndCkxMTEUFRXR3Nx8xOfeuHEjd999N4sXLyYjI4OrrrrqqM7TJS6up++IioraK5XjWPThMIsxxvQT\nUdEuN3r2g/Cz9XDpn13axerX4LkL4Pcj4a839X3uYARqau2gwaaZNiakJCcnM336dL773e9yySWX\nAFBbW8uAAQOIiYlhwYIFbNq06aDnOP3003n++ecB+PTTT1m1ahUAdXV1JCUlkZaWxo4dO5g3b173\na1JSUqivr9/vXKeddhqvvvoqe/bsobGxkVdeeYXTTjutry63VzbEYYwxxyI6FkbOdEtbs6uYsfov\nLn96yKnuhhhzQLv2tBLlE3JsBNmYkHLJJZdw/vnnM3fuXAAuu+wyvvnNbzJu3DhKS0sZNWrUQV9/\nww03cPXVVzN69GhGjx7N5MmTAZgwYQIlJSWMGjWKwYMHM3Xq1O7XXHfddcyaNYv8/HwWLFjQvX3S\npElcddVVTJkyBXA36ZWUlPRZOkVvRPWIa7qHndLSUl2yZEmwm2GM6U9a97gbBo/wJj4RWaqqBy4U\nGuKOpr/t7FQ5oBviAAAIgUlEQVQ6VImJsh81jVm7di2jR48OdjMiUm+f7YH6XBtBNsaYQOgqSWcO\nyecTfERUjWljTJiz/64bY4wxxhjjxwJkY4wxxpgQ0h/SX4+3I/1MLUA2xhhjjAkR8fHxVFdXW5Dc\nh1SV6upq4uMP/56QgOYgi8gs4D7cNKaPq+pd++yPA54BJgPVwEWqWiYiRcBa4DPv0I9V9XrvNZOB\nPwAJuBmgfqL2p8gYY4wxEaCgoIDy8nIqKyuD3ZSIEh8fT0FBwWEfH7AAWUSigIeAs4FyYLGIvK6q\na/wOuwbYrarDReRi4LfARd6+L1R1Yi+nfhj4HrAQFyDPAub1cpwxxhhjTFiJiYmhuLg42M3o9wKZ\nYjEF2KCqX6pqKzAXmL3PMbOBp731l4AZcqBJvQERyQNSVfVjb9T4GeBbfd90Y4wxxhjTXwUyQB4E\nbPF7Xu5t6/UYVW0HaoEsb1+xiCwXkX+KyGl+x5cf4pwAiMh1IrJERJbYzxTGGGOMMeZwhepNehVA\noaqWADcDz4tI6pGcQFUfVdVSVS3NyckJSCONMcYYY0zkCeRNeluBwX7PC7xtvR1TLiLRQBpQ7aVP\ntACo6lIR+QIY6R3vn2Hd2zn3s3Tp0ioR2QRkA1VHdzkhz64t/ETqdYFd27EYEsBzB5xffwuR++cg\nUq8L7NrCUaReFxyfa+u1zw1kgLwYGCEixbgg9mLg0n2OeR24EvgImAO8q6oqIjnALlXtEJGhwAjg\nS1XdJSJ1InIK7ia9K4AHDtUQVc0BEJEl4TyF68HYtYWfSL0usGvrz7r6W4jczypSrwvs2sJRpF4X\nBPfaAhYgq2q7iPwImI8r8/akqq4WkTuAJar6OvAE8KyIbAB24YJogNOBO0SkDegErlfVXd6+H9BT\n5m0eVsHCGGOMMcb0oYDWQVbVN3Cl2Py3/dpvvRn4di+vexl4+QDnXAKc2LctNcYYY4wxxgnVm/QC\n5dFgNyCA7NrCT6ReF9i1GSdSP6tIvS6wawtHkXpdEMRrE5uEzhhjjDHGmB79bQTZGGOMMcaYg7IA\n2RhjjDHGGD/9JkAWkVki8pmIbBCRW4Ldnr4kImUi8omIrBCRJcFuz7EQkSdFZKeIfOq3LVNE3hKR\n9d5jRjDbeDQOcF23ichW73tbISJfC2Ybj4aIDBaRBSKyRkRWi8hPvO2R8J0d6NrC/nsLNOtvw4P1\nt+EnUvvcUOxv+0UOsohEAZ8DZ+Omp14MXKKqa4LasD4iImVAqaqGfaFwETkdaACeUdUTvW2/w9XF\nvsv7xzZDVX8RzHYeqQNc121Ag6reHcy2HQsRyQPyVHWZiKQAS4FvAVcR/t/Zga7tQsL8ewsk62/D\nh/W34SdS+9xQ7G/7ywjyFGCDqn6pqq3AXGB2kNtkeqGq7+FqYvubDTztrT+N+0sTVg5wXWFPVStU\ndZm3Xg+sBQYRGd/Zga7NHJz1t2HC+tvwE6l9bij2t/0lQB4EbPF7Xk5k/UOnwN9FZKmIXBfsxgTA\nQFWt8Na3AwOD2Zg+9iMRWeX9JBhWP4ntS0SKgBLcLJcR9Z3tc20QQd9bAFh/G94i6u/uPiLq722k\n9rmh0t/2lwA50k1T1UnAucAPvZ+XIpK6nKBIyQt6GBgGTAQqgN8HtzlHT0SScZP73KSqdf77wv07\n6+XaIuZ7M0fF+tvwFFF/byO1zw2l/ra/BMhbgcF+zwu8bRFBVbd6jzuBV3A/cUaSHV5+Ulee0s4g\nt6dPqOoOVe1Q1U7gMcL0exORGFyH9kdV/Yu3OSK+s96uLVK+twCy/ja8RcTf3X1F0t/bSO1zQ62/\n7S8B8mJghIgUi0gscDHwepDb1CdEJMlLaEdEkoCZwKcHf1XYeR240lu/EngtiG3pM12dmed8wvB7\nExEBngDWquo9frvC/js70LVFwvcWYNbfhrew/7vbm0j5exupfW4o9rf9oooFgFca5F4gCnhSVX8T\n5Cb1CREZihvFAIgGng/naxORF4AzgWxgB/BvwKvAi0AhsAm4UFXD6gaMA1zXmbifjRQoA77vl0MW\nFkRkGvA+8AnQ6W3+FS53LNy/swNd2yWE+fcWaNbfhgfrb8Pv722k9rmh2N/2mwDZGGOMMcaYw9Ff\nUiyMMcYYY4w5LBYgG2OMMcYY48cCZGOMMcYYY/xYgGyMMcYYY4wfC5CNMcYYY4zxYwGy6ZdEpENE\nVvgtt/ThuYtEJCxrbBpjTF+z/taEo+hgN8CYIGlS1YnBboQxxvQD1t+asGMjyMb4EZEyEfmdiHwi\nIotEZLi3vUhE3hWRVSLyjogUetsHisgrIrLSW071ThUlIo+JyGoR+buIJHjH3ygia7zzzA3SZRpj\nTNBZf2tCmQXIpr9K2Ocnv4v89tWq6jjgQdxsYAAPAE+r6njgj8D93vb7gX+q6gRgErDa2z4CeEhV\nxwI1wL94228BSrzzXB+oizPGmBBi/a0JOzaTnumXRKRBVZN72V4GnKWqX4pIDLBdVbNEpArIU9U2\nb3uFqmaLSCVQoKotfucoAt5S1RHe818AMap6p4i8CTTgpnN9VVUbAnypxhgTVNbfmnBkI8jG7E8P\nsH4kWvzWO+jJ9/868BBu9GOxiNh9AMaY/sz6WxOSLEA2Zn8X+T1+5K1/CFzsrV8GvO+tvwPcACAi\nUSKSdqCTiogPGKyqC4BfAGnAfqMqxhjTj1h/a0KS/W/K9FcJIrLC7/mbqtpVeihDRFbhRiUu8bb9\nGHhKRH4OVAJXe9t/AjwqItfgRi5uACoO8J5RwHNepy7A/apa02dXZIwxocn6WxN2LAfZGD9eTlyp\nqlYFuy3GGBPJrL81ocxSLIwxxhhjjPFjI8jGGGOMMcb4sRFkY4wxxhhj/FiAbIwxxhhjjB8LkI0x\nxhhjjPFjAbIxxhhjjDF+LEA2xhhjjDHGz/8Heh1z1BWQVf8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J29MjpFPktgI",
        "colab_type": "code",
        "outputId": "8ea4b05b-8efc-4894-9691-a9c0ca308606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "labels, _ = autoencoder_nn.predict(x_val)\n",
        "print(classification_report(labels_val[0], np.argmax(labels, axis=1)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.94      0.95       271\n",
            "           1       0.90      0.93      0.92       262\n",
            "           2       0.95      0.92      0.94       244\n",
            "           3       0.99      0.99      0.99       260\n",
            "           4       0.93      0.92      0.93       273\n",
            "           5       0.92      0.94      0.93       251\n",
            "           6       0.89      0.89      0.89       235\n",
            "           7       0.95      0.98      0.97       273\n",
            "           8       0.96      0.94      0.95       254\n",
            "           9       0.90      0.89      0.89       289\n",
            "          10       0.96      0.95      0.96       188\n",
            "\n",
            "    accuracy                           0.94      2800\n",
            "   macro avg       0.94      0.94      0.94      2800\n",
            "weighted avg       0.94      0.94      0.94      2800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBJHphr-w3UA",
        "colab_type": "text"
      },
      "source": [
        "# Tied autoencoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uulr90Inw6h0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import losses\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "  return K.sqrt(losses.mse(y_true, y_pred))\n",
        "\n",
        "def tanh_crossentropy(y_true, y_pred):\n",
        "  return -0.5 * ((1 - y_true) * K.log(1 - y_pred) + (1 + y_true) * K.log(1 + y_pred))\n",
        "\n",
        "def get_tied_model(\n",
        "    model_type='autoencoder_nn',\n",
        "    divide_by=2,\n",
        "    hidden_first=256,\n",
        "    hidden_last=64,\n",
        "    dropout=True, \n",
        "    dropout_rate=0.3,\n",
        "    batch_norm=False, \n",
        "    noise_type=None,\n",
        "    salt_pepper_noise_ratio=0.3, \n",
        "    gaussian_noise_std=0.5,\n",
        "    standard=False,\n",
        "    verbose=True\n",
        "):\n",
        "  \n",
        "  input_layer = layers.Input(shape=(IMG_SHAPE, ))\n",
        "  \n",
        "  if model_type == 'autoencoder' or model_type == 'autoencoder_nn':\n",
        "    if noise_type is not None:\n",
        "      # noisy layer\n",
        "      if noise_type == 'gaussian':  # Gaussian\n",
        "        noise_layer = layers.GaussianNoise(gaussian_noise_std)(input_layer)\n",
        "      elif noise_type == 'saltpepper':  # Salt and pepper\n",
        "        noise_layer = SaltPepperNoise(ratio=salt_pepper_noise_ratio)(input_layer)\n",
        "      if standard:\n",
        "        noise_layer = StandardizeLayer(mean, std)(noise_layer)\n",
        "      if isinstance(divide_by, list) and divide_by != []:\n",
        "        dim = int(IMG_SHAPE / divide_by[0])\n",
        "        dims = [dim]\n",
        "        encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(noise_layer)\n",
        "      else:\n",
        "        encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(noise_layer)\n",
        "    elif standard:\n",
        "      encode = StandardizeLayer(mean, std)(input_layer)\n",
        "      if dropout:\n",
        "          encode = layers.Dropout(dropout_rate)(encode) \n",
        "    else:\n",
        "      if dropout:\n",
        "        encode = layers.Dropout(dropout_rate)(input_layer)\n",
        "        if isinstance(divide_by, list) and divide_by != []:\n",
        "          dim = int(IMG_SHAPE / divide_by[0])\n",
        "          dims = [dim]\n",
        "          encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "        else:\n",
        "          encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "      else:\n",
        "        if isinstance(divide_by, list) and divide_by != []:\n",
        "          dim = int(IMG_SHAPE / divide_by[0])\n",
        "          dims = [dim]\n",
        "          encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(input_layer)\n",
        "        else:\n",
        "          encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(input_layer)\n",
        "\n",
        "  # encoder\n",
        "  if isinstance(divide_by, list) and divide_by != []:\n",
        "    # saved_hidden_first = hidden_first\n",
        "    # if noise_type or standard:\n",
        "    #  encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "    i = 1\n",
        "    while i < len(divide_by) and int(dim / divide_by[i]) > hidden_last:\n",
        "      dim = int(dim / divide_by[i])\n",
        "      dims.insert(0, dim)\n",
        "      encode = layers.Dense(dim, activation=\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "      if batch_norm:\n",
        "        encode = layers.BatchNormalization()(encode)\n",
        "      i += 1\n",
        "  else:\n",
        "    saved_hidden_first = hidden_first\n",
        "    # if noise_type or standard:\n",
        "    #  encode = layers.Dense(hidden_first, activation =\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "    while hidden_first > hidden_last * divide_by:\n",
        "      hidden_first = int(hidden_first / divide_by)\n",
        "      encode = layers.Dense(hidden_first, activation=\"relu\", kernel_initializer=\"he_normal\")(encode)\n",
        "      if batch_norm:\n",
        "        encode = layers.BatchNormalization()(encode)\n",
        "  encode = layers.Dense(hidden_last, activation =\"relu\", kernel_initializer=\"he_normal\", name='encoder')(encode)\n",
        "  encoder = Model(inputs=input_layer, outputs=encode)\n",
        "\n",
        "  # decoder\n",
        "  i = len(encoder.layers) - 2\n",
        "  end = 1 if not dropout else 2\n",
        "  decode = DenseTranspose(encoder.layers[-1], activation='relu')(encode) \n",
        "  while i > end:\n",
        "    print(encoder.layers[i].name)\n",
        "    if 'dense' in encoder.layers[i].name:\n",
        "      decode = DenseTranspose(encoder.layers[i], activation='relu')(decode) \n",
        "    i -= 1\n",
        "  decode = DenseTranspose(encoder.layers[i], activation='sigmoid', name='decoder')(decode)\n",
        "\n",
        "  outputs = [decode]\n",
        "  if model_type == 'autoencoder_nn':\n",
        "    # classifier\n",
        "    classifier = layers.Dense(N_CLASSES, activation=\"softmax\", name='classifier')(encode)\n",
        "    outputs.insert(0, classifier)\n",
        "\n",
        "  # model\n",
        "  model = Model(inputs=input_layer, outputs=outputs)\n",
        "\n",
        "  if len(outputs) > 1:\n",
        "    loss = [\"sparse_categorical_crossentropy\", 'mse']\n",
        "    metrics = {'classifier': 'accuracy', 'decoder': 'mse'}\n",
        "  else:\n",
        "    loss = 'mse'\n",
        "    metrics = {'decoder': 'mse'}\n",
        "  \n",
        "  # print(metrics)\n",
        "  model.compile(optimizer=\"adam\", loss=loss, metrics=metrics)\n",
        "  if verbose:\n",
        "    model.summary()\n",
        "  # print(model.metrics_names)\n",
        "\n",
        "  return model, encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWaO4R0TzPzt",
        "colab_type": "code",
        "outputId": "b0c48923-1f2f-498b-a2f7-82deb0eca3b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "monitor = 'val_loss'\n",
        "early_stopping = EarlyStopping(monitor=monitor, patience=10, min_delta=0.001, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=10, verbose=1)\n",
        "callbacks = [early_stopping]\n",
        "\n",
        "model_type = 'autoencoder'\n",
        "if model_type == 'autoencoder_nn':\n",
        "  labels_train = [y_train, x_train]\n",
        "  labels_val = [y_val, x_val]\n",
        "elif model_type == 'nn':\n",
        "  labels_train = y_train\n",
        "  labels_val = y_val\n",
        "elif model_type == 'autoencoder':\n",
        "  labels_train = x_train\n",
        "  labels_val = x_val\n",
        "\n",
        "autoencoder, encoder = get_tied_model(\n",
        "    model_type=model_type, \n",
        "    divide_by=[1.5, 2, 2.5],\n",
        "    hidden_first=512, \n",
        "    hidden_last=32, \n",
        "    noise_type=None, \n",
        "    dropout=False, \n",
        "    batch_norm=False, \n",
        "    standard=False, \n",
        "    verbose=True\n",
        ")\n",
        "history_autoencoder = autoencoder.fit(\n",
        "    x_train, \n",
        "    labels_train,\n",
        "    validation_data=(x_val, labels_val), \n",
        "    epochs=300, \n",
        "    batch_size=128, \n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_46\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_59 (InputLayer)        [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_113 (Dense)            (None, 522)               409770    \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 261)               136503    \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            (None, 104)               27248     \n",
            "_________________________________________________________________\n",
            "encoder (Dense)              (None, 32)                3360      \n",
            "_________________________________________________________________\n",
            "dense_transpose_78 (DenseTra (None, 104)               3464      \n",
            "_________________________________________________________________\n",
            "dense_transpose_79 (DenseTra (None, 261)               27509     \n",
            "_________________________________________________________________\n",
            "dense_transpose_80 (DenseTra (None, 522)               137025    \n",
            "_________________________________________________________________\n",
            "decoder (DenseTranspose)     (None, 784)               410554    \n",
            "=================================================================\n",
            "Total params: 578,552\n",
            "Trainable params: 578,552\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 11200 samples, validate on 2800 samples\n",
            "Epoch 1/300\n",
            "11200/11200 [==============================] - 7s 627us/sample - loss: 0.0767 - mean_squared_error: 0.0767 - val_loss: 0.0468 - val_mean_squared_error: 0.0468\n",
            "Epoch 2/300\n",
            "11200/11200 [==============================] - 1s 87us/sample - loss: 0.0368 - mean_squared_error: 0.0368 - val_loss: 0.0298 - val_mean_squared_error: 0.0298\n",
            "Epoch 3/300\n",
            "11200/11200 [==============================] - 1s 87us/sample - loss: 0.0263 - mean_squared_error: 0.0263 - val_loss: 0.0240 - val_mean_squared_error: 0.0240\n",
            "Epoch 4/300\n",
            "11200/11200 [==============================] - 1s 86us/sample - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0208 - val_mean_squared_error: 0.0208\n",
            "Epoch 5/300\n",
            "11200/11200 [==============================] - 1s 91us/sample - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0189 - val_mean_squared_error: 0.0189\n",
            "Epoch 6/300\n",
            "11200/11200 [==============================] - 1s 87us/sample - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0174 - val_mean_squared_error: 0.0174\n",
            "Epoch 7/300\n",
            "11200/11200 [==============================] - 1s 87us/sample - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0164 - val_mean_squared_error: 0.0164\n",
            "Epoch 8/300\n",
            "11200/11200 [==============================] - 1s 86us/sample - loss: 0.0151 - mean_squared_error: 0.0151 - val_loss: 0.0156 - val_mean_squared_error: 0.0156\n",
            "Epoch 9/300\n",
            "11200/11200 [==============================] - 1s 88us/sample - loss: 0.0143 - mean_squared_error: 0.0143 - val_loss: 0.0147 - val_mean_squared_error: 0.0147\n",
            "Epoch 10/300\n",
            "11200/11200 [==============================] - 1s 85us/sample - loss: 0.0136 - mean_squared_error: 0.0136 - val_loss: 0.0147 - val_mean_squared_error: 0.0147\n",
            "Epoch 11/300\n",
            "11200/11200 [==============================] - 1s 85us/sample - loss: 0.0130 - mean_squared_error: 0.0130 - val_loss: 0.0138 - val_mean_squared_error: 0.0138\n",
            "Epoch 12/300\n",
            "11200/11200 [==============================] - 1s 86us/sample - loss: 0.0125 - mean_squared_error: 0.0125 - val_loss: 0.0134 - val_mean_squared_error: 0.0134\n",
            "Epoch 13/300\n",
            "11200/11200 [==============================] - 1s 88us/sample - loss: 0.0121 - mean_squared_error: 0.0121 - val_loss: 0.0130 - val_mean_squared_error: 0.0130\n",
            "Epoch 14/300\n",
            "11200/11200 [==============================] - 1s 85us/sample - loss: 0.0116 - mean_squared_error: 0.0116 - val_loss: 0.0127 - val_mean_squared_error: 0.0127\n",
            "Epoch 15/300\n",
            "11200/11200 [==============================] - 1s 86us/sample - loss: 0.0114 - mean_squared_error: 0.0114 - val_loss: 0.0125 - val_mean_squared_error: 0.0125\n",
            "Epoch 16/300\n",
            "11200/11200 [==============================] - 1s 85us/sample - loss: 0.0110 - mean_squared_error: 0.0110 - val_loss: 0.0121 - val_mean_squared_error: 0.0121\n",
            "Epoch 17/300\n",
            "11200/11200 [==============================] - 1s 85us/sample - loss: 0.0107 - mean_squared_error: 0.0107 - val_loss: 0.0122 - val_mean_squared_error: 0.0122\n",
            "Epoch 18/300\n",
            "11200/11200 [==============================] - 1s 87us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - val_loss: 0.0118 - val_mean_squared_error: 0.0118\n",
            "Epoch 19/300\n",
            "11200/11200 [==============================] - 1s 86us/sample - loss: 0.0101 - mean_squared_error: 0.0101 - val_loss: 0.0114 - val_mean_squared_error: 0.0114\n",
            "Epoch 20/300\n",
            "11200/11200 [==============================] - 1s 86us/sample - loss: 0.0099 - mean_squared_error: 0.0099 - val_loss: 0.0113 - val_mean_squared_error: 0.0113\n",
            "Epoch 21/300\n",
            "11200/11200 [==============================] - 1s 86us/sample - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0111 - val_mean_squared_error: 0.0111\n",
            "Epoch 22/300\n",
            "11200/11200 [==============================] - 1s 86us/sample - loss: 0.0095 - mean_squared_error: 0.0095 - val_loss: 0.0110 - val_mean_squared_error: 0.0110\n",
            "Epoch 23/300\n",
            "11200/11200 [==============================] - 1s 86us/sample - loss: 0.0094 - mean_squared_error: 0.0094 - val_loss: 0.0109 - val_mean_squared_error: 0.0109\n",
            "Epoch 24/300\n",
            "11200/11200 [==============================] - 1s 87us/sample - loss: 0.0092 - mean_squared_error: 0.0092 - val_loss: 0.0106 - val_mean_squared_error: 0.0106\n",
            "Epoch 25/300\n",
            "11200/11200 [==============================] - 1s 84us/sample - loss: 0.0090 - mean_squared_error: 0.0090 - val_loss: 0.0107 - val_mean_squared_error: 0.0107\n",
            "Epoch 26/300\n",
            "11200/11200 [==============================] - 1s 88us/sample - loss: 0.0089 - mean_squared_error: 0.0089 - val_loss: 0.0105 - val_mean_squared_error: 0.0105\n",
            "Epoch 27/300\n",
            "11200/11200 [==============================] - 1s 87us/sample - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0103 - val_mean_squared_error: 0.0103\n",
            "Epoch 28/300\n",
            "11200/11200 [==============================] - 1s 86us/sample - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
            "Epoch 29/300\n",
            "11200/11200 [==============================] - 1s 86us/sample - loss: 0.0084 - mean_squared_error: 0.0084 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
            "Epoch 30/300\n",
            "11200/11200 [==============================] - 1s 87us/sample - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
            "Epoch 31/300\n",
            "11200/11200 [==============================] - 1s 85us/sample - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
            "Epoch 32/300\n",
            "11200/11200 [==============================] - 1s 87us/sample - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
            "Epoch 33/300\n",
            "11200/11200 [==============================] - 1s 86us/sample - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
            "Epoch 34/300\n",
            "11200/11200 [==============================] - 1s 88us/sample - loss: 0.0079 - mean_squared_error: 0.0079 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
            "Epoch 35/300\n",
            "11200/11200 [==============================] - 1s 85us/sample - loss: 0.0079 - mean_squared_error: 0.0079 - val_loss: 0.0099 - val_mean_squared_error: 0.0099\n",
            "Epoch 36/300\n",
            "11200/11200 [==============================] - 1s 84us/sample - loss: 0.0078 - mean_squared_error: 0.0078 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
            "Epoch 37/300\n",
            "11200/11200 [==============================] - 1s 85us/sample - loss: 0.0076 - mean_squared_error: 0.0076 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
            "Epoch 38/300\n",
            "11200/11200 [==============================] - 1s 86us/sample - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
            "Epoch 39/300\n",
            "11200/11200 [==============================] - 1s 89us/sample - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
            "Epoch 40/300\n",
            "11200/11200 [==============================] - 1s 85us/sample - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
            "Epoch 41/300\n",
            "11200/11200 [==============================] - 1s 85us/sample - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
            "Epoch 42/300\n",
            "11200/11200 [==============================] - 4s 392us/sample - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfVizLNjzT2r",
        "colab_type": "code",
        "outputId": "cc592db2-bb1c-4181-92a8-747d6c01f5e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# print accuracy\n",
        "x_plot = list(range(1, len(history_autoencoder.history['val_loss']) + 1))\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(x_plot, history.history['loss'])\n",
        "    plt.plot(x_plot, history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "plot_history(history_autoencoder)\n",
        "\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "\n",
        "n=10\n",
        "plt.figure(figsize=(40, 4))\n",
        "for i in range(n):\n",
        "    # display original images\n",
        "    ax = plt.subplot(3, 20, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "    # display encoded images\n",
        "    ax = plt.subplot(3, 20, i + 1 + 20)\n",
        "    plt.imshow(encoded_imgs[i].reshape(8,4))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "    # display reconstructed images\n",
        "    ax = plt.subplot(3, 20, 2*20 +i+ 1)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "  \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABEYAAADrCAYAAABkdZM2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 2880x288 with 30 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9pEvVhE-MuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}